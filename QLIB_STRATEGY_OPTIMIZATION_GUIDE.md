# RD-Agent Qlib ç­–ç•¥å„ªåŒ–æŒ‡å—ï¼ˆç°¡åŒ–ç‰ˆï¼‰

**æ ¸å¿ƒå„ªå‹¢**ï¼šRD-Agent åŸç”Ÿæ”¯æ´ Qlibï¼Œç„¡éœ€è‡ªè¨‚ Scenarioï¼

**ç•¶å‰ç‹€æ…‹**ï¼šç³»çµ±å·²æœ‰ 5 å€‹ Qlib ç­–ç•¥ï¼Œå¯ç›´æ¥ä½¿ç”¨

**é è¨ˆå·¥ä½œé‡**ï¼š1 é€±ï¼ˆ40 å°æ™‚ï¼‰â† æ¯” Backtrader ç‰ˆæœ¬æ¸›å°‘ 60%

---

## ğŸ¯ ç‚ºä½•åªç”¨ Qlibï¼Ÿ

### RD-Agent åŸç”Ÿæ”¯æ´

RD-Agent å°±æ˜¯ç‚º Qlib è¨­è¨ˆçš„ï¼š

```python
# RD-Agent å®˜æ–¹æä¾›çš„ Qlib å ´æ™¯
from rdagent.scenarios.qlib.experiment.model_experiment import QlibModelScenario

# ç›´æ¥ä½¿ç”¨ï¼Œç„¡éœ€è‡ªè¨‚ï¼
scenario = QlibModelScenario()
```

### ç•¶å‰ç³»çµ±è³‡æº

**å·²æœ‰ Qlib ç­–ç•¥**ï¼ˆ5 å€‹ï¼‰ï¼š
1. Qlib å‡ç·šäº¤å‰ç­–ç•¥ï¼ˆID: 41ï¼‰
2. LightGBM é æ¸¬æ¨¡å‹ï¼ˆID: 45ï¼‰
3. Alpha158 + LightGBMï¼ˆID: 49ï¼‰
4. å…¶ä»– 2 å€‹

**å·²æœ‰ Qlib æ•¸æ“š**ï¼š
- æ—¥ç·šï¼š`/data/qlib/tw_stock_v2/`ï¼ˆ2007-è‡³ä»Šï¼‰
- åˆ†é˜ç·šï¼š`/data/qlib/tw_stock_minute/`ï¼ˆ7 å¹´ï¼‰
- æœŸè²¨ï¼šTXCONT, MTXCONT

**å·²æœ‰ Qlib å› å­**ï¼ˆ17 å€‹ï¼‰ï¼š
- å¾ `generated_factors` è¡¨ä¸­çš„ Qlib è¡¨é”å¼
- å¯ç›´æ¥æ’å…¥ç­–ç•¥å„ªåŒ–

### æŠ€è¡“å„ªå‹¢å°æ¯”

| é …ç›® | Backtrader ç‰ˆæœ¬ | Qlib ç‰ˆæœ¬ |
|------|----------------|-----------|
| **å¯¦ç¾è¤‡é›œåº¦** | é«˜ï¼ˆéœ€è‡ªè¨‚ Scenarioï¼‰ | ä½ï¼ˆå®˜æ–¹æ”¯æ´ï¼‰ |
| **é–‹ç™¼æ™‚é–“** | 2-3 é€± | 1 é€± |
| **æ¸¬è©¦é›£åº¦** | é«˜ï¼ˆå…©å¥—ç³»çµ±ï¼‰ | ä½ï¼ˆå–®ä¸€ç³»çµ±ï¼‰ |
| **ç¶­è­·æˆæœ¬** | é«˜ | ä½ |
| **RD-Agent æ•´åˆ** | éœ€è‡ªè¡Œå¯¦ç¾ | é–‹ç®±å³ç”¨ |

---

## ğŸš€ ç°¡åŒ–å¯¦ç¾æ–¹æ¡ˆ

### éšæ®µ 1ï¼šRD-Agent Qlib æ•´åˆï¼ˆ2-3 å¤©ï¼‰

#### 1.1 å®‰è£ RD-Agent Qlib æ¨¡çµ„

**æª¢æŸ¥ç•¶å‰ç‰ˆæœ¬**ï¼š

```bash
docker compose exec backend pip show rdagent
```

**ç¢ºèª Qlib å ´æ™¯å¯ç”¨**ï¼š

```python
# backend/test_rdagent_qlib.py
from rdagent.scenarios.qlib.experiment.model_experiment import QlibModelScenario

scenario = QlibModelScenario()
print("âœ… RD-Agent Qlib å ´æ™¯å·²å°±ç·’")
```

#### 1.2 å¯¦ç¾ç­–ç•¥å„ªåŒ–æœå‹™

**æ–‡ä»¶**ï¼š`backend/app/services/rdagent_service.py`

```python
def execute_qlib_strategy_optimization(
    self,
    task_id: int,
    strategy_id: int,
    optimization_goal: str,
    max_iterations: int = 5,
    llm_model: str = "gpt-4-turbo"
) -> str:
    """åŸ·è¡Œ Qlib ç­–ç•¥å„ªåŒ–

    Args:
        task_id: RD-Agent ä»»å‹™ ID
        strategy_id: Qlib ç­–ç•¥ ID
        optimization_goal: å„ªåŒ–ç›®æ¨™
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•¸
        llm_model: LLM æ¨¡å‹

    Returns:
        log_dir: æ—¥èªŒç›®éŒ„è·¯å¾‘
    """
    from rdagent.scenarios.qlib.experiment.model_experiment import QlibModelScenario
    from rdagent.core.evolving_framework import EvolvingFramework

    logger.info(f"Starting Qlib strategy optimization for strategy {strategy_id}")

    # Step 1: ç²å–åŸå§‹ç­–ç•¥
    strategy = self.db.query(Strategy).filter(
        Strategy.id == strategy_id,
        Strategy.engine_type == 'qlib'
    ).first()

    if not strategy:
        raise ValueError(f"Qlib strategy {strategy_id} not found")

    # Step 2: å›æ¸¬åŸå§‹ç­–ç•¥ï¼ˆç²å–åŸºæº–ï¼‰
    baseline_metrics = self._backtest_qlib_strategy(strategy)

    logger.info(f"Baseline metrics: {baseline_metrics}")

    # Step 3: æ§‹å»ºå„ªåŒ– Prompt
    prompt = self._build_qlib_optimization_prompt(
        strategy=strategy,
        baseline_metrics=baseline_metrics,
        optimization_goal=optimization_goal
    )

    # Step 4: å‰µå»º RD-Agent é€²åŒ–æ¡†æ¶
    scenario = QlibModelScenario()
    framework = EvolvingFramework(
        scenario=scenario,
        max_iterations=max_iterations
    )

    # Step 5: åŸ·è¡Œå„ªåŒ–å¾ªç’°
    logger.info(f"Starting evolution with {max_iterations} iterations...")

    results = framework.evolve(
        baseline_code=strategy.code,
        baseline_metrics=baseline_metrics,
        optimization_goal=prompt,
        llm_config={
            "model": llm_model,
            "api_key": os.getenv("OPENAI_API_KEY")
        }
    )

    # Step 6: ä¿å­˜æ—¥èªŒ
    log_dir = self._save_qlib_optimization_logs(strategy_id, results)

    logger.info(f"Optimization completed. Log directory: {log_dir}")

    return log_dir


def _backtest_qlib_strategy(self, strategy: Strategy) -> Dict[str, float]:
    """å›æ¸¬ Qlib ç­–ç•¥

    Args:
        strategy: Qlib ç­–ç•¥ç‰©ä»¶

    Returns:
        metrics: ç¸¾æ•ˆæŒ‡æ¨™
    """
    import qlib
    from qlib.data import D
    from qlib.backtest import backtest
    from qlib.contrib.strategy import TopkDropoutStrategy

    # åˆå§‹åŒ– Qlib
    qlib.init(provider_uri='/data/qlib/tw_stock_v2', region='tw')

    # åŸ·è¡Œç­–ç•¥ä»£ç¢¼ç²å–é æ¸¬
    # ï¼ˆé€™è£¡éœ€è¦æ ¹æ“šç­–ç•¥é¡å‹å‹•æ…‹åŸ·è¡Œï¼‰
    exec_globals = {}
    exec(strategy.code, exec_globals)

    # å‡è¨­ç­–ç•¥æœƒç”Ÿæˆä¸€å€‹ predictions DataFrame
    predictions = exec_globals.get('predictions')

    if predictions is None:
        raise ValueError("Strategy code must generate 'predictions' DataFrame")

    # åŸ·è¡Œå›æ¸¬
    strategy_obj = TopkDropoutStrategy(
        model=predictions,
        topk=10,
        n_drop=5
    )

    backtest_result = backtest(
        predictions,
        strategy=strategy_obj,
        executor_config={
            "time_per_step": "day",
            "generate_portfolio_metrics": True
        }
    )

    # æå–æŒ‡æ¨™
    analysis = backtest_result['analysis']

    return {
        "sharpe_ratio": float(analysis.get('sharpe', 0)),
        "annual_return": float(analysis.get('annualized_return', 0)),
        "max_drawdown": float(analysis.get('max_drawdown', 0)),
        "information_ratio": float(analysis.get('information_ratio', 0)),
        "total_return": float(analysis.get('total_return', 0))
    }


def _build_qlib_optimization_prompt(
    self,
    strategy: Strategy,
    baseline_metrics: Dict[str, float],
    optimization_goal: str
) -> str:
    """æ§‹å»º Qlib ç­–ç•¥å„ªåŒ– Prompt

    Args:
        strategy: åŸå§‹ç­–ç•¥
        baseline_metrics: åŸºæº–æŒ‡æ¨™
        optimization_goal: å„ªåŒ–ç›®æ¨™

    Returns:
        prompt: å®Œæ•´ Prompt
    """
    # æŸ¥è©¢å¯ç”¨çš„ç”Ÿæˆå› å­
    available_factors = self.db.query(GeneratedFactor).filter(
        GeneratedFactor.user_id == strategy.user_id
    ).limit(20).all()

    factors_list = "\n".join([
        f"  - {f.name}: {f.formula} (IC: {f.ic or 'N/A'})"
        for f in available_factors
    ])

    prompt = f"""
# Qlib ç­–ç•¥å„ªåŒ–ä»»å‹™

## åŸå§‹ç­–ç•¥

**åç¨±**: {strategy.name}
**æè¿°**: {strategy.description or "ç„¡æè¿°"}

**ä»£ç¢¼**:
```python
{strategy.code}
```

## ç•¶å‰ç¸¾æ•ˆï¼ˆåŸºæº–ï¼‰

- **Sharpe Ratio**: {baseline_metrics.get('sharpe_ratio', 0):.3f}
- **Annual Return**: {baseline_metrics.get('annual_return', 0):.2%}
- **Max Drawdown**: {baseline_metrics.get('max_drawdown', 0):.2%}
- **Information Ratio**: {baseline_metrics.get('information_ratio', 0):.3f}

## å„ªåŒ–ç›®æ¨™

{optimization_goal}

## å¯ç”¨çš„ç”Ÿæˆå› å­

ä»¥ä¸‹æ˜¯ RD-Agent ä¹‹å‰ç”Ÿæˆçš„å› å­ï¼Œå¯ä»¥åŠ å…¥ç­–ç•¥ä¸­ï¼š

{factors_list}

## Qlib ç­–ç•¥å„ªåŒ–æ–¹å‘

### 1. å› å­å„ªåŒ–

**å¢åŠ æ–°å› å­**:
```python
# åŸå§‹
QLIB_FIELDS = ['$close', 'Mean($close, 20)']

# å„ªåŒ–ï¼šå¢åŠ æ³¢å‹•ç‡å’Œæˆäº¤é‡å› å­
QLIB_FIELDS = [
    '$close',
    'Mean($close, 20)',
    'Std($close, 20)',           # æ–°å¢ï¼šæ³¢å‹•ç‡
    'Correlation($close, $volume, 10)',  # æ–°å¢ï¼šåƒ¹é‡ç›¸é—œæ€§
]
```

**å› å­çµ„åˆåŠ æ¬Š**:
```python
# å¤šå› å­çµ„åˆ
combined_factor = (
    0.4 * momentum_factor +
    0.3 * volatility_factor +
    0.3 * volume_factor
)
```

### 2. æ¨¡å‹å„ªåŒ–ï¼ˆå¦‚æœæ˜¯ ML ç­–ç•¥ï¼‰

**LightGBM åƒæ•¸èª¿æ•´**:
```python
# åŸå§‹
model = lgb.LGBMRegressor(n_estimators=100, max_depth=5)

# å„ªåŒ–
model = lgb.LGBMRegressor(
    n_estimators=200,      # å¢åŠ æ¨¹æ•¸é‡
    max_depth=7,           # å¢åŠ æ·±åº¦
    learning_rate=0.05,    # é™ä½å­¸ç¿’ç‡
    num_leaves=63,         # èª¿æ•´è‘‰å­æ•¸
    min_child_samples=30   # é˜²æ­¢éæ“¬åˆ
)
```

**ç‰¹å¾µå·¥ç¨‹**:
```python
# å¢åŠ äº¤äº’é …
df['ma_cross'] = df['ma5'] - df['ma20']
df['price_volume'] = df['close'] * df['volume']
```

### 3. é¢¨éšªæ§åˆ¶

**æŒå€‰é™åˆ¶**:
```python
# å¢åŠ æœ€å¤§æŒå€‰é™åˆ¶
strategy = TopkDropoutStrategy(
    model=predictions,
    topk=10,              # åŸå§‹ï¼š20
    n_drop=5,
    method_sell="bottom",
    method_buy="top"
)
```

**æ›æ‰‹ç‡æ§åˆ¶**:
```python
# é™ä½æ›æ‰‹ç‡
if abs(new_weight - old_weight) < 0.05:
    new_weight = old_weight  # å°æ–¼ 5% è®Šå‹•ä¸èª¿æ•´
```

## è¼¸å‡ºæ ¼å¼

è«‹æä¾›ï¼š

### 1. å•é¡Œåˆ†æ
ï¼ˆæè¿°ç•¶å‰ç­–ç•¥çš„ä¸»è¦å•é¡Œï¼‰

### 2. æ”¹é€²æ–¹æ¡ˆ
ï¼ˆå…·é«”çš„å„ªåŒ–å»ºè­°ï¼Œåˆ†é»åˆ—å‡ºï¼‰

### 3. å„ªåŒ–å¾Œä»£ç¢¼
```python
# å®Œæ•´çš„ Qlib ç­–ç•¥ä»£ç¢¼
```

### 4. é æœŸæ”¹é€²
- Sharpe Ratio: X.XX â†’ X.XX (+XX%)
- Annual Return: XX% â†’ XX% (+XX%)
- Max Drawdown: XX% â†’ XX% (æ”¹å–„ XX%)
"""

    return prompt


def _save_qlib_optimization_logs(
    self,
    strategy_id: int,
    results: Any
) -> str:
    """ä¿å­˜ Qlib å„ªåŒ–æ—¥èªŒ

    Args:
        strategy_id: ç­–ç•¥ ID
        results: RD-Agent å„ªåŒ–çµæœ

    Returns:
        log_dir: æ—¥èªŒç›®éŒ„è·¯å¾‘
    """
    from pathlib import Path
    import pickle
    import json
    from datetime import datetime

    # å‰µå»ºæ—¥èªŒç›®éŒ„
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = Path(f"/app/log/qlib_strategy_opt_{strategy_id}_{timestamp}")
    log_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜çµæœ
    with open(log_dir / "results.pkl", "wb") as f:
        pickle.dump(results, f)

    # ä¿å­˜æ‘˜è¦ï¼ˆJSON æ ¼å¼ï¼‰
    summary = {
        "strategy_id": strategy_id,
        "timestamp": timestamp,
        "iterations": len(results.get("iterations", [])),
        "best_metrics": results.get("best_metrics", {})
    }

    with open(log_dir / "summary.json", "w") as f:
        json.dump(summary, f, indent=2)

    return str(log_dir)
```

---

### éšæ®µ 2ï¼šçµæœè§£æèˆ‡ä¿å­˜ï¼ˆ1-2 å¤©ï¼‰

#### 2.1 è§£æå„ªåŒ–çµæœ

```python
def parse_qlib_optimization_results(
    self,
    log_dir: str
) -> List[Dict[str, Any]]:
    """è§£æ Qlib ç­–ç•¥å„ªåŒ–çµæœ

    Args:
        log_dir: æ—¥èªŒç›®éŒ„

    Returns:
        optimized_strategies: å„ªåŒ–å¾Œçš„ç­–ç•¥åˆ—è¡¨
    """
    from pathlib import Path
    import pickle

    log_path = Path(log_dir)

    # è®€å–çµæœ
    with open(log_path / "results.pkl", "rb") as f:
        results = pickle.load(f)

    # æå–æ‰€æœ‰è¿­ä»£çš„ç­–ç•¥
    strategies = []

    for iteration in results.get("iterations", []):
        strategy_data = {
            "code": iteration.get("code"),
            "metrics": iteration.get("metrics"),
            "description": iteration.get("description"),
            "loop_num": iteration.get("loop_num")
        }
        strategies.append(strategy_data)

    # æŒ‰ Sharpe Ratio æ’åº
    strategies.sort(
        key=lambda x: x.get("metrics", {}).get("sharpe_ratio", 0),
        reverse=True
    )

    return strategies
```

#### 2.2 ä¿å­˜å„ªåŒ–å¾Œçš„ç­–ç•¥

```python
def save_optimized_qlib_strategy(
    self,
    task_id: int,
    original_strategy_id: int,
    optimized_code: str,
    optimized_metrics: Dict[str, float],
    description: str = None
) -> Strategy:
    """ä¿å­˜å„ªåŒ–å¾Œçš„ Qlib ç­–ç•¥

    Args:
        task_id: RD-Agent ä»»å‹™ ID
        original_strategy_id: åŸå§‹ç­–ç•¥ ID
        optimized_code: å„ªåŒ–å¾Œçš„ä»£ç¢¼
        optimized_metrics: å„ªåŒ–å¾Œçš„æŒ‡æ¨™
        description: å„ªåŒ–èªªæ˜

    Returns:
        strategy: æ–°ç­–ç•¥ç‰©ä»¶
    """
    from app.models.strategy import Strategy, StrategyStatus

    # ç²å–åŸå§‹ç­–ç•¥
    original = self.db.query(Strategy).filter(
        Strategy.id == original_strategy_id
    ).first()

    # å‰µå»ºå„ªåŒ–å¾Œçš„ç­–ç•¥
    optimized_strategy = Strategy(
        user_id=original.user_id,
        name=f"{original.name} (RD-Agent å„ªåŒ– v1)",
        description=description or f"RD-Agent è‡ªå‹•å„ªåŒ–ç‰ˆæœ¬\n\nåŸå§‹ç­–ç•¥ ID: {original_strategy_id}\n\n{original.description}",
        code=optimized_code,
        engine_type='qlib',
        status=StrategyStatus.DRAFT,
        parameters={
            "rdagent_task_id": task_id,
            "original_strategy_id": original_strategy_id,
            "optimization_metrics": optimized_metrics
        }
    )

    self.db.add(optimized_strategy)
    self.db.commit()
    self.db.refresh(optimized_strategy)

    logger.info(f"Saved optimized strategy {optimized_strategy.id}")

    return optimized_strategy
```

---

### éšæ®µ 3ï¼šCelery ä»»å‹™æ•´åˆï¼ˆ1 å¤©ï¼‰

**æ›´æ–°**ï¼š`backend/app/tasks/rdagent_tasks.py`

```python
@celery_app.task(bind=True, name="app.tasks.run_strategy_optimization_task")
def run_strategy_optimization_task(self: Task, task_id: int):
    """åŸ·è¡Œç­–ç•¥å„ªåŒ–ä»»å‹™ï¼ˆQlib ç‰ˆæœ¬ï¼‰"""
    db: Session = SessionLocal()

    try:
        service = RDAgentService(db)
        task = db.query(RDAgentTask).filter(RDAgentTask.id == task_id).first()

        if not task:
            logger.error(f"Task {task_id} not found")
            return {"status": "error", "message": "Task not found"}

        # æ›´æ–°ç‚ºåŸ·è¡Œä¸­
        service.update_task_status(task_id, TaskStatus.RUNNING)

        # æå–åƒæ•¸
        strategy_id = task.input_params.get("strategy_id")
        optimization_goal = task.input_params.get("optimization_goal")
        max_iterations = task.input_params.get("max_iterations", 5)
        llm_model = task.input_params.get("llm_model", "gpt-4-turbo")

        # é©—è­‰æ˜¯ Qlib ç­–ç•¥
        strategy = db.query(Strategy).filter(Strategy.id == strategy_id).first()
        if not strategy or strategy.engine_type != 'qlib':
            raise ValueError(f"Strategy {strategy_id} is not a Qlib strategy")

        logger.info(f"Starting Qlib strategy optimization for strategy {strategy_id}")

        # Step 1: åŸ·è¡Œå„ªåŒ–
        log_dir = service.execute_qlib_strategy_optimization(
            task_id=task_id,
            strategy_id=strategy_id,
            optimization_goal=optimization_goal,
            max_iterations=max_iterations,
            llm_model=llm_model
        )

        # Step 2: è§£æçµæœ
        optimized_strategies = service.parse_qlib_optimization_results(log_dir)

        if not optimized_strategies:
            raise ValueError("No optimized strategies generated")

        # Step 3: ç²å–æœ€ä½³ç‰ˆæœ¬
        best_strategy = optimized_strategies[0]

        # Step 4: ä¿å­˜å„ªåŒ–å¾Œçš„ç­–ç•¥
        optimized_strategy_obj = service.save_optimized_qlib_strategy(
            task_id=task_id,
            original_strategy_id=strategy_id,
            optimized_code=best_strategy["code"],
            optimized_metrics=best_strategy["metrics"],
            description=best_strategy.get("description")
        )

        # Step 5: è¨ˆç®—æˆæœ¬
        llm_calls, llm_cost = service.calculate_llm_costs(log_dir)

        # Step 6: æ›´æ–°ä»»å‹™ç‹€æ…‹
        baseline_metrics = service._backtest_qlib_strategy(strategy)
        optimized_metrics = best_strategy["metrics"]

        service.update_task_status(
            task_id,
            TaskStatus.COMPLETED,
            result={
                "original_strategy_id": strategy_id,
                "optimized_strategy_id": optimized_strategy_obj.id,
                "baseline_metrics": baseline_metrics,
                "optimized_metrics": optimized_metrics,
                "improvements": {
                    "sharpe_ratio": optimized_metrics["sharpe_ratio"] - baseline_metrics["sharpe_ratio"],
                    "annual_return": optimized_metrics["annual_return"] - baseline_metrics["annual_return"],
                    "max_drawdown": optimized_metrics["max_drawdown"] - baseline_metrics["max_drawdown"]
                },
                "total_iterations": len(optimized_strategies),
                "message": "Qlib strategy optimization completed successfully"
            },
            llm_calls=llm_calls,
            llm_cost=llm_cost
        )

        logger.info(f"Strategy optimization task {task_id} completed")
        return {"status": "success", "task_id": task_id}

    except Exception as e:
        logger.error(f"Strategy optimization task {task_id} failed: {str(e)}")
        service.update_task_status(task_id, TaskStatus.FAILED, error_message=str(e))
        return {"status": "error", "message": str(e)}

    finally:
        db.close()
```

---

## ğŸ§ª æ¸¬è©¦è¨ˆåŠƒ

### æ¸¬è©¦ç­–ç•¥ 1ï¼šå‡ç·šç­–ç•¥å„ªåŒ–

**åŸå§‹ç­–ç•¥**ï¼ˆID: 41ï¼‰ï¼š
- 5 æ—¥å‡ç·š Ã— 20 æ—¥å‡ç·šäº¤å‰

**å„ªåŒ–ç›®æ¨™**ï¼š
```
æå‡å¤æ™®æ¯”ç‡ï¼Œå»ºè­°æ–¹å‘ï¼š
1. èª¿æ•´å‡ç·šé€±æœŸ
2. å¢åŠ æˆäº¤é‡ç¢ºèª
3. å¢åŠ æ³¢å‹•ç‡éæ¿¾
```

**é æœŸçµæœ**ï¼š
- Sharpe: 0.8 â†’ 1.3
- Return: 12% â†’ 18%

### æ¸¬è©¦ç­–ç•¥ 2ï¼šLightGBM æ¨¡å‹å„ªåŒ–

**åŸå§‹ç­–ç•¥**ï¼ˆID: 45ï¼‰ï¼š
- 18 å€‹æŠ€è¡“æŒ‡æ¨™
- LightGBM é æ¸¬

**å„ªåŒ–ç›®æ¨™**ï¼š
```
æå‡é æ¸¬æº–ç¢ºåº¦ï¼Œå»ºè­°æ–¹å‘ï¼š
1. å¢åŠ å› å­ï¼ˆå¾ generated_factors é¸æ“‡é«˜ IC å› å­ï¼‰
2. èª¿æ•´ LightGBM è¶…åƒæ•¸
3. å¢åŠ ç‰¹å¾µå·¥ç¨‹ï¼ˆäº¤äº’é …ï¼‰
```

**é æœŸçµæœ**ï¼š
- IC: 0.05 â†’ 0.08
- Sharpe: 1.5 â†’ 2.0

---

## ğŸ’° æˆæœ¬ä¼°ç®—ï¼ˆç°¡åŒ–ç‰ˆï¼‰

### é–‹ç™¼æˆæœ¬

| éšæ®µ | æ™‚é–“ | èªªæ˜ |
|------|------|------|
| RD-Agent æ•´åˆ | 2-3 å¤© | ä½¿ç”¨å®˜æ–¹ Qlib å ´æ™¯ |
| çµæœè§£æ | 1-2 å¤© | æå–å„ªåŒ–çµæœ |
| Celery ä»»å‹™ | 1 å¤© | æ•´åˆç•°æ­¥åŸ·è¡Œ |
| æ¸¬è©¦èˆ‡ä¿®æ­£ | 2 å¤© | æ¸¬è©¦èˆ‡å„ªåŒ– |
| **ç¸½è¨ˆ** | **1 é€±** | **40 å°æ™‚** |

**äººåŠ›æˆæœ¬**ï¼š$2,000-$3,000ï¼ˆå‡è¨­æ™‚è–ª $50ï¼‰

### é‹ç‡Ÿæˆæœ¬

| é …ç›® | æˆæœ¬ |
|------|------|
| LLM APIï¼ˆ5 æ¬¡è¿­ä»£ï¼‰ | $2-4 USD |
| Qlib å›æ¸¬ | å¿½ç•¥ä¸è¨ˆ |
| **ç¸½è¨ˆ** | **$2-4 USD/æ¬¡** |

### ROI

**å‡è¨­**ï¼š
- æœˆæ´»ç”¨æˆ¶ï¼š50 äººï¼ˆåªæœ‰ Qlib ç”¨æˆ¶ï¼‰
- ä½¿ç”¨é »ç‡ï¼š2 æ¬¡/æœˆ
- æ”¶è²»ï¼š$8 USD/æ¬¡

**æœˆæ”¶å…¥**ï¼š50 Ã— 2 Ã— $8 = $800
**æœˆæˆæœ¬**ï¼š50 Ã— 2 Ã— $3 = $300
**æ·¨åˆ©**ï¼š$500/æœˆ

**æŠ•è³‡å›æ”¶æœŸ**ï¼š$2,500 / $500 = **5 å€‹æœˆ**

---

## âœ… å¯¦ç¾æª¢æŸ¥æ¸…å–®

### ç¬¬ 1-2 å¤©ï¼šRD-Agent æ•´åˆ

- [ ] é©—è­‰ RD-Agent Qlib å ´æ™¯å¯ç”¨
- [ ] å¯¦ç¾ `execute_qlib_strategy_optimization()`
- [ ] å¯¦ç¾ `_backtest_qlib_strategy()`
- [ ] å¯¦ç¾ `_build_qlib_optimization_prompt()`
- [ ] æ¸¬è©¦å–®è¼ªå„ªåŒ–

### ç¬¬ 3-4 å¤©ï¼šçµæœè™•ç†

- [ ] å¯¦ç¾ `parse_qlib_optimization_results()`
- [ ] å¯¦ç¾ `save_optimized_qlib_strategy()`
- [ ] å¯¦ç¾ `_save_qlib_optimization_logs()`
- [ ] æ¸¬è©¦çµæœä¿å­˜

### ç¬¬ 5 å¤©ï¼šCelery æ•´åˆ

- [ ] æ›´æ–° `run_strategy_optimization_task()`
- [ ] å¢åŠ  Qlib ç­–ç•¥é©—è­‰
- [ ] æ¸¬è©¦ç•°æ­¥åŸ·è¡Œ

### ç¬¬ 6-7 å¤©ï¼šæ¸¬è©¦èˆ‡éƒ¨ç½²

- [ ] æ¸¬è©¦å‡ç·šç­–ç•¥å„ªåŒ–
- [ ] æ¸¬è©¦ ML ç­–ç•¥å„ªåŒ–
- [ ] ä¿®æ­£ Bug
- [ ] æ–‡æª”æ’°å¯«
- [ ] éƒ¨ç½²ä¸Šç·š

---

## ğŸ¯ ç«‹å³å¯åŸ·è¡Œçš„æ¸¬è©¦

### æ¸¬è©¦ RD-Agent Qlib å ´æ™¯

```bash
# é€²å…¥å®¹å™¨
docker compose exec backend bash

# æ¸¬è©¦ RD-Agent Qlib æ¨¡çµ„
python << 'PYEOF'
from rdagent.scenarios.qlib.experiment.model_experiment import QlibModelScenario

try:
    scenario = QlibModelScenario()
    print("âœ… RD-Agent Qlib å ´æ™¯å·²å°±ç·’")
    print(f"ğŸ“Š Scenario é¡å‹ï¼š{type(scenario)}")
except Exception as e:
    print(f"âŒ éŒ¯èª¤ï¼š{e}")
PYEOF
```

### æ¸¬è©¦ Qlib å›æ¸¬

```bash
# æ¸¬è©¦ç¾æœ‰ç­–ç•¥çš„å›æ¸¬
python << 'PYEOF'
import qlib
from qlib.data import D

# åˆå§‹åŒ– Qlib
qlib.init(provider_uri='/data/qlib/tw_stock_v2', region='tw')

# è®€å–å°æŒ‡æœŸè²¨æ•¸æ“š
data = D.features(
    ['TXCONT'],
    ['$close', 'Mean($close, 20)'],
    start_time='2024-01-01',
    end_time='2024-12-31'
)

print("âœ… Qlib æ•¸æ“šè®€å–æˆåŠŸ")
print(f"ğŸ“Š æ•¸æ“šå½¢ç‹€ï¼š{data.shape}")
print(data.tail())
PYEOF
```

---

## ğŸ“š åƒè€ƒè³‡æº

### RD-Agent Qlib æ–‡æª”

- **å®˜æ–¹ç¯„ä¾‹**ï¼šhttps://github.com/microsoft/RD-Agent/tree/main/rdagent/scenarios/qlib
- **Qlib ç­–ç•¥å„ªåŒ–**ï¼šhttps://github.com/microsoft/RD-Agent/blob/main/rdagent/scenarios/qlib/experiment/model_experiment.py
- **CoSTEER æ¡†æ¶**ï¼šhttps://github.com/microsoft/RD-Agent/blob/main/rdagent/core/evolving_framework.py

### Qlib æ–‡æª”

- **å›æ¸¬ API**ï¼šhttps://qlib.readthedocs.io/en/latest/component/backtest.html
- **ç­–ç•¥ API**ï¼šhttps://qlib.readthedocs.io/en/latest/component/strategy.html
- **Alpha158**ï¼šhttps://qlib.readthedocs.io/en/latest/component/data.html#alpha158

---

## ğŸ‰ ç¸½çµ

### ç‚ºä½•é¸æ“‡ Qlibï¼Ÿ

âœ… **åŸç”Ÿæ”¯æ´**ï¼šRD-Agent ç‚º Qlib è¨­è¨ˆï¼Œç„¡éœ€è‡ªè¨‚
âœ… **å·²æœ‰æ•¸æ“š**ï¼šå®Œæ•´çš„ Qlib æ•¸æ“šåŸºç¤è¨­æ–½
âœ… **å·²æœ‰ç­–ç•¥**ï¼š5 å€‹ç¾æˆçš„ Qlib ç­–ç•¥å¯å„ªåŒ–
âœ… **é–‹ç™¼å¿«é€Ÿ**ï¼š1 é€± vs Backtrader ç‰ˆæœ¬çš„ 3 é€±
âœ… **æˆæœ¬æ›´ä½**ï¼š$2,500 vs $5,000

### ä¸‹ä¸€æ­¥

1. **æ¸¬è©¦ RD-Agent Qlib å ´æ™¯**ï¼ˆ10 åˆ†é˜ï¼‰
2. **å¯¦ç¾åŸºç¤å„ªåŒ–åŠŸèƒ½**ï¼ˆ2-3 å¤©ï¼‰
3. **æ¸¬è©¦å‡ç·šç­–ç•¥å„ªåŒ–**ï¼ˆ1 å¤©ï¼‰
4. **å®Œå–„åŠŸèƒ½ä¸¦ä¸Šç·š**ï¼ˆ2-3 å¤©ï¼‰

**ç«‹å³é–‹å§‹ï¼Ÿé‚„æ˜¯å…ˆæ¸¬è©¦ RD-Agent Qlib æ¨¡çµ„ï¼Ÿ**

---

**æ–‡æª”ç‰ˆæœ¬**ï¼šv1.0
**å‰µå»ºæ™‚é–“**ï¼š2025-12-25 21:45
**é è¨ˆå¯¦ç¾æ™‚é–“**ï¼š1 é€±ï¼ˆ40 å°æ™‚ï¼‰
