# RD-Agent å› å­è©•ä¼°ä¸¦ç™¼é™åˆ¶åŠŸèƒ½å¯¦ä½œå ±å‘Š

**æ—¥æœŸ**: 2025-12-29
**ä½œè€…**: Claude Code
**ç‰ˆæœ¬**: 1.0
**å¯¦ä½œç‹€æ…‹**: âœ… å®Œæˆä¸¦éƒ¨ç½²

---

## ğŸ“‹ ç›®éŒ„

1. [å¯¦ä½œç›®æ¨™](#å¯¦ä½œç›®æ¨™)
2. [æŠ€è¡“æ–¹æ¡ˆ](#æŠ€è¡“æ–¹æ¡ˆ)
3. [å¯¦ä½œç´°ç¯€](#å¯¦ä½œç´°ç¯€)
4. [ç³»çµ±æ¶æ§‹](#ç³»çµ±æ¶æ§‹)
5. [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
6. [ç›£æ§èˆ‡ç¶­è­·](#ç›£æ§èˆ‡ç¶­è­·)
7. [æ¸¬è©¦é©—è­‰](#æ¸¬è©¦é©—è­‰)
8. [æ•ˆèƒ½åˆ†æ](#æ•ˆèƒ½åˆ†æ)
9. [æœªä¾†å„ªåŒ–](#æœªä¾†å„ªåŒ–)

---

## å¯¦ä½œç›®æ¨™

### å•é¡ŒèƒŒæ™¯

åœ¨ RD-Agent å› å­è©•ä¼°ç³»çµ±ä¸­ï¼Œå­˜åœ¨ä»¥ä¸‹ä¸¦ç™¼åŸ·è¡Œé¢¨éšªï¼š

#### 1ï¸âƒ£ **å¤šç”¨æˆ¶åŒæ™‚è©•ä¼°**
```
å ´æ™¯ï¼š5 å€‹ç”¨æˆ¶åŒæ™‚åœ¨ç·šï¼Œå„è‡ªè©•ä¼°å› å­
é¢¨éšªï¼š5 å€‹è©•ä¼°ä»»å‹™ Ã— 300 MB è¨˜æ†¶é«” = 1.5 GBï¼ˆæ¥è¿‘æœå‹™å™¨ä¸Šé™ï¼‰
```

#### 2ï¸âƒ£ **RD-Agent è‡ªå‹•è©•ä¼°**
```
å ´æ™¯ï¼šRD-Agent ç”Ÿæˆ 10 å€‹æ–°å› å­ï¼Œè‡ªå‹•è§¸ç™¼è©•ä¼°
é¢¨éšªï¼š10 å€‹è©•ä¼°ä»»å‹™ Ã— 300 MB = 3 GBï¼ˆè¶…å‡ºæœå‹™å™¨å®¹é‡ï¼‰
```

#### 3ï¸âƒ£ **æ‰¹é‡è©•ä¼°ä»»å‹™**
```
å ´æ™¯ï¼šç”¨æˆ¶é¸æ“‡æ‰¹é‡è©•ä¼° 50 å€‹å› å­
é¢¨éšªï¼š50 å€‹ä¸¦ç™¼ä»»å‹™æœƒå°è‡´ç³»çµ±å´©æ½°
```

### è§£æ±ºæ–¹æ¡ˆç›®æ¨™

âœ… **é™åˆ¶æœ€å¤§ä¸¦ç™¼æ•¸é‡**ï¼šæœ€å¤šåŒæ™‚åŸ·è¡Œ 3 å€‹è©•ä¼°ä»»å‹™
âœ… **åˆ†æ•£å¼é–æ©Ÿåˆ¶**ï¼šä½¿ç”¨ Redis å¯¦ä½œè·¨ Worker çš„ä¸¦ç™¼æ§åˆ¶
âœ… **è‡ªå‹•é‡è©¦æ©Ÿåˆ¶**ï¼šé”åˆ°é™åˆ¶æ™‚è‡ªå‹•å»¶é²é‡è©¦ï¼Œé¿å…ä»»å‹™å¤±æ•—
âœ… **å°ˆç”¨è©•ä¼°ä½‡åˆ—**ï¼šéš”é›¢è©•ä¼°ä»»å‹™ï¼Œé¿å…å½±éŸ¿å…¶ä»–åŠŸèƒ½
âœ… **è³‡æºå¯æ§**ï¼š3 å€‹è©•ä¼° Ã— 300 MB = 900 MBï¼ˆåœ¨å®‰å…¨ç¯„åœå…§ï¼‰

---

## æŠ€è¡“æ–¹æ¡ˆ

### æ–¹æ¡ˆé¸æ“‡ï¼šå°ˆç”¨ä½‡åˆ— + Redis åˆ†æ•£å¼é–

æˆ‘å€‘æ¡ç”¨äº† **æ–¹æ¡ˆ 2ï¼ˆå°ˆç”¨ä½‡åˆ—ï¼‰+ æ–¹æ¡ˆ 3ï¼ˆRedis é–ï¼‰** çš„çµ„åˆï¼š

#### âœ… æ–¹æ¡ˆ 2ï¼šå°ˆç”¨è©•ä¼°ä½‡åˆ—

**å¯¦ä½œ**ï¼š
```yaml
# docker-compose.yml
celery-evaluation-worker:
  command: celery -A app.core.celery_app worker --concurrency=3 --queues=evaluation
```

**å„ªé»**ï¼š
- éš”é›¢è©•ä¼°ä»»å‹™ï¼Œä¸å½±éŸ¿å…¶ä»–åŠŸèƒ½ï¼ˆå›æ¸¬ã€æ•¸æ“šåŒæ­¥ï¼‰
- å°ˆç”¨ Worker å¯èª¿æ•´è³‡æºé…ç½®ï¼ˆCPUã€è¨˜æ†¶é«”ï¼‰
- ä¾¿æ–¼ç›£æ§å’Œæ—¥èªŒè¿½è¹¤

#### âœ… æ–¹æ¡ˆ 3ï¼šRedis åˆ†æ•£å¼é–

**å¯¦ä½œ**ï¼š
```python
# app/utils/concurrent_limit.py
class ConcurrentLimiter:
    def __init__(self, max_concurrent=3):
        self.redis_client = redis.from_url(settings.REDIS_URL)
        self.max_concurrent = max_concurrent
```

**å„ªé»**ï¼š
- è·¨ Worker çš„å…¨å±€ä¸¦ç™¼æ§åˆ¶
- åŸå­æ€§æ“ä½œï¼ˆLua è…³æœ¬ï¼‰
- è‡ªå‹•è¶…æ™‚æ¸…ç†ï¼ˆé˜²æ­¢æ­»é–ï¼‰
- å®¹éŒ¯è¨­è¨ˆï¼ˆRedis ä¸å¯ç”¨æ™‚ä¸é™åˆ¶ï¼‰

---

## å¯¦ä½œç´°ç¯€

### 1. ä¸¦ç™¼é™åˆ¶å™¨ï¼ˆConcurrentLimiterï¼‰

**æª”æ¡ˆ**ï¼š`backend/app/utils/concurrent_limit.py`

#### æ ¸å¿ƒå¯¦ä½œ

```python
class ConcurrentLimiter:
    """
    ä¸¦ç™¼é™åˆ¶å™¨ - ä½¿ç”¨ Redis è¨ˆæ•¸å™¨å¯¦ä½œåˆ†æ•£å¼ä¸¦ç™¼æ§åˆ¶

    åƒæ•¸ï¼š
        key_prefix: Redis éµå‰ç¶´
        max_concurrent: æœ€å¤§ä¸¦ç™¼æ•¸é‡ï¼ˆé è¨­ 3ï¼‰
        timeout: ä»»å‹™åŸ·è¡Œè¶…æ™‚æ™‚é–“ï¼ˆé è¨­ 3600 ç§’ï¼‰
    """

    def __init__(
        self,
        key_prefix: str,
        max_concurrent: int = 3,
        timeout: int = 3600,
        redis_url: Optional[str] = None
    ):
        self.key_prefix = key_prefix
        self.max_concurrent = max_concurrent
        self.timeout = timeout

        # é€£æ¥ Redis
        try:
            self.redis_client = redis.from_url(
                redis_url or settings.REDIS_URL,
                decode_responses=True
            )
            self.redis_client.ping()
            logger.debug(f"ConcurrentLimiter initialized: {key_prefix}, max={max_concurrent}")
        except Exception as e:
            logger.error(f"Failed to connect to Redis for ConcurrentLimiter: {e}")
            self.redis_client = None
```

#### Redis éµè¨­è¨ˆ

```python
def _get_counter_key(self) -> str:
    """è¨ˆæ•¸å™¨éµï¼ševaluation_concurrent:counter"""
    return f"{self.key_prefix}:counter"

def _get_lock_key(self, task_id: str) -> str:
    """ä»»å‹™é–éµï¼ševaluation_concurrent:lock:eval_123_abc"""
    return f"{self.key_prefix}:lock:{task_id}"
```

#### åŸå­æ€§å¢åŠ è¨ˆæ•¸ï¼ˆLua è…³æœ¬ï¼‰

```python
def increment(self, task_id: str) -> bool:
    """
    åŸå­æ€§åœ°æª¢æŸ¥ä¸¦å¢åŠ è¨ˆæ•¸

    ä½¿ç”¨ Lua è…³æœ¬ç¢ºä¿åŸå­æ€§ï¼š
    1. è®€å–ç•¶å‰è¨ˆæ•¸
    2. å¦‚æœ < max_concurrentï¼Œå‰‡ +1 ä¸¦è¿”å›æˆåŠŸ
    3. å¦å‰‡è¿”å›å¤±æ•—
    """
    lua_script = """
    local counter_key = KEYS[1]
    local max_concurrent = tonumber(ARGV[1])
    local current = tonumber(redis.call('GET', counter_key) or 0)

    if current < max_concurrent then
        redis.call('INCR', counter_key)
        return 1
    else
        return 0
    end
    """

    result = self.redis_client.eval(
        lua_script,
        1,
        self._get_counter_key(),
        self.max_concurrent
    )

    if result == 1:
        # è¨­ç½®ä»»å‹™é–ï¼Œå¸¶è¶…æ™‚æ™‚é–“é˜²æ­¢æ­»é–
        lock_key = self._get_lock_key(task_id)
        self.redis_client.setex(lock_key, self.timeout, "1")
        logger.debug(f"Incremented concurrent counter: {self.key_prefix}, task={task_id}")
        return True
    else:
        logger.warning(f"Failed to increment: limit reached for {self.key_prefix}")
        return False
```

**ç‚ºä½•ä½¿ç”¨ Lua è…³æœ¬ï¼Ÿ**
- **åŸå­æ€§**ï¼šé¿å…ç«¶æ…‹æ¢ä»¶ï¼ˆrace conditionï¼‰
- **æ€§èƒ½**ï¼šä¸€æ¬¡ç¶²çµ¡å¾€è¿”å®Œæˆæª¢æŸ¥å’Œå¢åŠ 
- **æ­£ç¢ºæ€§**ï¼šç¢ºä¿è¨ˆæ•¸ä¸æœƒè¶…éé™åˆ¶

#### ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆContext Managerï¼‰

```python
@contextmanager
def acquire(self, task_id: Optional[str] = None, wait: bool = False, wait_timeout: int = 300):
    """
    ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šç²å–åŸ·è¡Œæ§½ä½

    ä½¿ç”¨ç¯„ä¾‹ï¼š
        with limiter.acquire(task_id="eval_123"):
            # åŸ·è¡Œè©•ä¼°ä»»å‹™
            evaluate_factor()

    è‡ªå‹•è™•ç†ï¼š
    - ç²å–æ§½ä½ï¼ˆincrementï¼‰
    - åŸ·è¡Œä»»å‹™ï¼ˆyieldï¼‰
    - é‡‹æ”¾æ§½ä½ï¼ˆdecrementï¼‰- å³ä½¿ç™¼ç”Ÿç•°å¸¸ä¹Ÿæœƒé‡‹æ”¾
    """
    if task_id is None:
        task_id = f"task_{int(time.time() * 1000)}"

    acquired = False
    start_time = time.time()

    try:
        # å˜—è©¦ç²å–æ§½ä½
        while not acquired:
            if self.increment(task_id):
                acquired = True
                logger.info(
                    f"Acquired concurrent slot: {self.key_prefix}, "
                    f"task={task_id}, current={self.get_current_count()}/{self.max_concurrent}"
                )
                break

            if not wait:
                raise RuntimeError(
                    f"Concurrent limit reached: {self.get_current_count()}/{self.max_concurrent} "
                    f"for {self.key_prefix}"
                )

            # ç­‰å¾…æ¨¡å¼
            elapsed = time.time() - start_time
            if elapsed >= wait_timeout:
                raise TimeoutError(
                    f"Timeout waiting for concurrent slot: {self.key_prefix}, "
                    f"waited {elapsed:.1f}s"
                )

            logger.debug(f"Waiting for concurrent slot: {self.key_prefix}, task={task_id}")
            time.sleep(5)  # æ¯ 5 ç§’é‡è©¦ä¸€æ¬¡

        # åŸ·è¡Œä»»å‹™
        yield

    finally:
        # é‡‹æ”¾æ§½ä½ï¼ˆå³ä½¿ç™¼ç”Ÿç•°å¸¸ä¹ŸæœƒåŸ·è¡Œï¼‰
        if acquired:
            self.decrement(task_id)
            logger.info(
                f"Released concurrent slot: {self.key_prefix}, "
                f"task={task_id}, current={self.get_current_count()}/{self.max_concurrent}"
            )
```

#### å…¨å±€å¯¦ä¾‹

```python
# å…¨å±€è©•ä¼°é™åˆ¶å™¨å¯¦ä¾‹
evaluation_limiter = ConcurrentLimiter(
    key_prefix="evaluation_concurrent",
    max_concurrent=3,  # æœ€å¤šåŒæ™‚ 3 å€‹è©•ä¼°
    timeout=3600       # 1 å°æ™‚è¶…æ™‚
)
```

---

### 2. Celery ä½‡åˆ—é…ç½®

**æª”æ¡ˆ**ï¼š`backend/app/core/celery_app.py`

#### ä»»å‹™è·¯ç”±é…ç½®

```python
celery_app.conf.update(
    task_routes={
        'app.tasks.run_backtest_async': {'queue': 'backtest'},
        'app.tasks.sync_*': {'queue': 'data_sync'},
        'app.tasks.cleanup_*': {'queue': 'maintenance'},

        # å› å­è©•ä¼°å°ˆç”¨ä½‡åˆ—ï¼ˆä¸¦ç™¼æ§åˆ¶ï¼‰
        'app.tasks.evaluate_factor_async': {'queue': 'evaluation'},
        'app.tasks.batch_evaluate_factors': {'queue': 'evaluation'},
        'app.tasks.update_factor_metrics': {'queue': 'evaluation'},
    }
)
```

#### ä»»å‹™æ™‚é–“é™åˆ¶

```python
celery_app.conf.update(
    task_annotations={
        # å–®å€‹å› å­è©•ä¼°
        'app.tasks.evaluate_factor_async': {
            'time_limit': 3600,      # 1 å°æ™‚ç¡¬é™åˆ¶
            'soft_time_limit': 3300,  # 55 åˆ†é˜è»Ÿé™åˆ¶
        },

        # æ‰¹é‡è©•ä¼°
        'app.tasks.batch_evaluate_factors': {
            'time_limit': 7200,      # 2 å°æ™‚ç¡¬é™åˆ¶
            'soft_time_limit': 6900,
        },

        # æ›´æ–°æŒ‡æ¨™
        'app.tasks.update_factor_metrics': {
            'time_limit': 60,        # 1 åˆ†é˜ç¡¬é™åˆ¶
            'soft_time_limit': 50,
        }
    }
)
```

**æ™‚é–“é™åˆ¶èªªæ˜**ï¼š
- **soft_time_limit**ï¼šè§¸ç™¼ `SoftTimeLimitExceeded` ç•°å¸¸ï¼Œä»»å‹™å¯ä»¥æ•ç²ä¸¦æ¸…ç†
- **time_limit**ï¼šå¼·åˆ¶çµ‚æ­¢ä»»å‹™ï¼ˆSIGKILLï¼‰

---

### 3. è©•ä¼°ä»»å‹™æ•´åˆ

**æª”æ¡ˆ**ï¼š`backend/app/tasks/factor_evaluation_tasks.py`

#### ä¿®æ”¹å‰ï¼ˆç„¡ä¸¦ç™¼æ§åˆ¶ï¼‰

```python
@celery_app.task(bind=True, name="app.tasks.evaluate_factor_async")
def evaluate_factor_async(self: Task, factor_id: int, ...):
    db: Session = SessionLocal()
    try:
        service = FactorEvaluationService(db)
        results = service.evaluate_factor(factor_id, ...)
        return {"status": "success", "results": results}
    finally:
        db.close()
```

#### ä¿®æ”¹å¾Œï¼ˆä¸¦ç™¼æ§åˆ¶ + è‡ªå‹•é‡è©¦ï¼‰

```python
from app.utils.concurrent_limit import evaluation_limiter

@celery_app.task(bind=True, name="app.tasks.evaluate_factor_async")
def evaluate_factor_async(
    self: Task,
    factor_id: int,
    stock_pool: str = "all",
    start_date: str = None,
    end_date: str = None
) -> dict:
    """
    ç•°æ­¥è©•ä¼°å› å­ç¸¾æ•ˆï¼ˆå¸¶ä¸¦ç™¼é™åˆ¶ï¼‰

    ä¸¦ç™¼æ§åˆ¶ï¼š
    - æœ€å¤šåŒæ™‚åŸ·è¡Œ 3 å€‹è©•ä¼°ä»»å‹™
    - è¶…éé™åˆ¶æ™‚æœƒè‡ªå‹•é‡è©¦ï¼ˆæœ€å¤š 10 æ¬¡ï¼Œæ¯æ¬¡ç­‰å¾… 30 ç§’ï¼‰
    """
    task_id = f"eval_{factor_id}_{self.request.id}"

    # æª¢æŸ¥ä¸¦ç™¼é™åˆ¶
    if not evaluation_limiter.can_execute():
        current_count = evaluation_limiter.get_current_count()
        logger.warning(
            f"[Task {self.request.id}] Evaluation concurrent limit reached "
            f"({current_count}/{evaluation_limiter.max_concurrent}), "
            f"retrying in 30 seconds..."
        )
        # å»¶é²é‡è©¦
        raise self.retry(countdown=30, max_retries=10)

    logger.info(
        f"[Task {self.request.id}] Starting async factor evaluation for factor_id={factor_id}, "
        f"concurrent: {evaluation_limiter.get_current_count() + 1}/{evaluation_limiter.max_concurrent}"
    )

    db: Session = SessionLocal()

    try:
        # ä½¿ç”¨ä¸¦ç™¼é™åˆ¶å™¨ç²å–åŸ·è¡Œæ§½ä½
        with evaluation_limiter.acquire(task_id=task_id):
            # æª¢æŸ¥å› å­æ˜¯å¦å­˜åœ¨
            factor = db.query(GeneratedFactor).filter(
                GeneratedFactor.id == factor_id
            ).first()

            if not factor:
                logger.error(f"[Task {self.request.id}] Factor {factor_id} not found")
                return {
                    "status": "error",
                    "error": f"Factor {factor_id} not found",
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }

            # åŸ·è¡Œè©•ä¼°
            service = FactorEvaluationService(db)
            results = service.evaluate_factor(
                factor_id=factor_id,
                stock_pool=stock_pool,
                start_date=start_date,
                end_date=end_date,
                save_to_db=True
            )

            logger.info(
                f"[Task {self.request.id}] Factor evaluation completed - "
                f"IC: {results.get('ic', 'N/A'):.4f}, "
                f"Sharpe: {results.get('sharpe_ratio', 'N/A'):.4f}"
            )

            # è‡ªå‹•æ›´æ–°å› å­æŒ‡æ¨™åˆ°ä¸»è¡¨
            try:
                update_task = update_factor_metrics.delay(factor_id=factor_id)
                logger.info(f"[Task {self.request.id}] Metrics sync triggered, task_id: {update_task.id}")
            except Exception as sync_error:
                logger.error(f"[Task {self.request.id}] Failed to trigger metrics sync: {str(sync_error)}")

            return {
                "status": "success",
                "factor_id": factor_id,
                "results": results,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }

    except Exception as e:
        logger.error(f"[Task {self.request.id}] Factor evaluation failed: {str(e)}")
        logger.exception(e)

        # ä½¿ç”¨æŒ‡æ•¸é€€é¿ï¼š1m, 2m, 4m
        retry_count = self.request.retries
        countdown = 60 * (2 ** retry_count)
        raise self.retry(exc=e, countdown=countdown, max_retries=3)

    finally:
        db.close()
```

**é—œéµæ”¹é€²**ï¼š
1. **æå‰æª¢æŸ¥**ï¼š`can_execute()` åœ¨ç²å–æ§½ä½å‰æª¢æŸ¥ï¼Œå¿«é€Ÿå¤±æ•—
2. **ä¸Šä¸‹æ–‡ç®¡ç†**ï¼š`with evaluation_limiter.acquire()` è‡ªå‹•ç®¡ç†è³‡æº
3. **è‡ªå‹•é‡è©¦**ï¼šé”åˆ°é™åˆ¶æ™‚å»¶é² 30 ç§’é‡è©¦ï¼ˆæœ€å¤š 10 æ¬¡ï¼‰
4. **æ—¥èªŒè¿½è¹¤**ï¼šè¨˜éŒ„ç•¶å‰ä¸¦ç™¼æ•¸å’Œä»»å‹™ ID
5. **ç•°å¸¸å®‰å…¨**ï¼šç„¡è«–æˆåŠŸæˆ–å¤±æ•—éƒ½æœƒé‡‹æ”¾æ§½ä½

---

### 4. å°ˆç”¨ Worker é…ç½®

**æª”æ¡ˆ**ï¼š`docker-compose.yml`

```yaml
# Celery Evaluation Worker (Dedicated for factor evaluation with concurrency limit)
celery-evaluation-worker:
  build:
    context: ./backend
    dockerfile: Dockerfile
  container_name: quantlab-celery-evaluation-worker
  restart: unless-stopped
  # å°ˆé–€è™•ç†è©•ä¼°ä»»å‹™ï¼Œä¸¦ç™¼é™åˆ¶ç‚º 3
  command: celery -A app.core.celery_app worker --loglevel=info --concurrency=3 --queues=evaluation
  group_add:
    - "999"  # Docker group GID for socket access
  volumes:
    - ./backend:/app
    - backend_cache:/root/.cache
    - /data/qlib:/data/qlib  # Qlib æ•¸æ“šæŒä¹…åŒ–
    - ./ShioajiData:/data/shioaji  # Shioaji åˆ†é˜ç´šè³‡æ–™
  environment:
    TZ: UTC  # çµ±ä¸€ä½¿ç”¨ UTC æ™‚å€
    DATABASE_URL: ${DATABASE_URL}
    REDIS_URL: ${REDIS_URL}
    CELERY_BROKER_URL: ${CELERY_BROKER_URL}
    CELERY_RESULT_BACKEND: ${CELERY_RESULT_BACKEND}
    JWT_SECRET: ${JWT_SECRET}
    ENCRYPTION_KEY: ${ENCRYPTION_KEY}
    FINLAB_API_TOKEN: ${FINLAB_API_TOKEN}
    OPENAI_API_KEY: ${OPENAI_API_KEY}
    ENVIRONMENT: ${ENVIRONMENT:-development}
    DEBUG: ${DEBUG:-True}
    QLIB_DATA_PATH: ${QLIB_DATA_PATH}
  depends_on:
    postgres:
      condition: service_healthy
    redis:
      condition: service_healthy
  networks:
    - quantlab-network
```

**é…ç½®é‡é»**ï¼š
- `--concurrency=3`ï¼šWorker æœ€å¤š 3 å€‹ä¸¦ç™¼åŸ·è¡Œç·’
- `--queues=evaluation`ï¼šåªè™•ç† evaluation ä½‡åˆ—
- ç¨ç«‹å®¹å™¨ï¼šé¿å…å½±éŸ¿å…¶ä»– Worker

---

## ç³»çµ±æ¶æ§‹

### ä¸¦ç™¼æ§åˆ¶æµç¨‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ç”¨æˆ¶/RD-Agent                            â”‚
â”‚                 è§¸ç™¼å› å­è©•ä¼°ï¼ˆå¯èƒ½åŒæ™‚å¤šå€‹ï¼‰                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Celery Task Queue                             â”‚
â”‚                  (evaluation å°ˆç”¨ä½‡åˆ—)                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚Task 1â”‚  â”‚Task 2â”‚  â”‚Task 3â”‚  â”‚Task 4â”‚  â”‚Task 5â”‚  ...         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Celery Evaluation Worker (concurrency=3)            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Worker Thread Pool (æœ€å¤š 3 å€‹åŸ·è¡Œç·’)                 â”‚       â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚       â”‚
â”‚  â”‚  â”‚Thread 1â”‚  â”‚Thread 2â”‚  â”‚Thread 3â”‚                 â”‚       â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜                 â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚           â”‚           â”‚
           â–¼           â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ConcurrentLimiter                              â”‚
â”‚                   (Redis åˆ†æ•£å¼é–)                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Redis Counter: evaluation_concurrent:counter = 3  â”‚         â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚
â”‚  â”‚  â”‚  Locks:                                      â”‚ â”‚         â”‚
â”‚  â”‚  â”‚  - evaluation_concurrent:lock:eval_1_abc    â”‚ â”‚         â”‚
â”‚  â”‚  â”‚  - evaluation_concurrent:lock:eval_2_def    â”‚ â”‚         â”‚
â”‚  â”‚  â”‚  - evaluation_concurrent:lock:eval_3_ghi    â”‚ â”‚         â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚           â”‚           â”‚
           â–¼           â–¼           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚è©•ä¼°ä»»å‹™ 1â”‚ â”‚è©•ä¼°ä»»å‹™ 2â”‚ â”‚è©•ä¼°ä»»å‹™ 3â”‚  âœ… åŸ·è¡Œä¸­
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚è©•ä¼°ä»»å‹™ 4â”‚ â”‚è©•ä¼°ä»»å‹™ 5â”‚  â³ ç­‰å¾…é‡è©¦ï¼ˆ30 ç§’å¾Œï¼‰
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Redis éµçµæ§‹

```
evaluation_concurrent:counter = 3          # ç•¶å‰ä¸¦ç™¼æ•¸é‡
evaluation_concurrent:lock:eval_123_abc    # ä»»å‹™é–ï¼ˆTTL: 3600sï¼‰
evaluation_concurrent:lock:eval_456_def    # ä»»å‹™é–ï¼ˆTTL: 3600sï¼‰
evaluation_concurrent:lock:eval_789_ghi    # ä»»å‹™é–ï¼ˆTTL: 3600sï¼‰
```

### å¤š Worker å ´æ™¯

å³ä½¿æœ‰å¤šå€‹ Evaluation Workerï¼ˆæ°´å¹³æ“´å±•ï¼‰ï¼ŒRedis åˆ†æ•£å¼é–ä¹Ÿèƒ½ç¢ºä¿å…¨å±€ä¸¦ç™¼æ§åˆ¶ï¼š

```
Worker 1                  Worker 2                  Redis Counter
â”€â”€â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Task A é–‹å§‹               Task D é–‹å§‹               counter = 0
  â†“ increment()             â†“ increment()
counter = 1               counter = 2               counter = 2
  â†“ åŸ·è¡Œè©•ä¼°                  â†“ åŸ·è¡Œè©•ä¼°
Task B é–‹å§‹               Task E é–‹å§‹
  â†“ increment()             â†“ increment()
counter = 3               âŒ é™åˆ¶é”åˆ°               counter = 3
  â†“ åŸ·è¡Œè©•ä¼°                  â†“ é‡è©¦ï¼ˆ30sï¼‰
Task C é–‹å§‹
  â†“ increment()
âŒ é™åˆ¶é”åˆ°
  â†“ é‡è©¦ï¼ˆ30sï¼‰

Task A å®Œæˆ
  â†“ decrement()
counter = 2                                        counter = 2

Task C é‡è©¦
  â†“ increment()
counter = 3                                        counter = 3
  â†“ åŸ·è¡Œè©•ä¼°                                         âœ… å…¨å±€æœ€å¤š 3 å€‹
```

---

## ä½¿ç”¨æŒ‡å—

### åŸºæœ¬ä½¿ç”¨

è©•ä¼°ä»»å‹™æœƒè‡ªå‹•æ‡‰ç”¨ä¸¦ç™¼é™åˆ¶ï¼Œç„¡éœ€ä¿®æ”¹ API èª¿ç”¨æ–¹å¼ï¼š

```python
# API å±¤
@router.post("/{factor_id}/evaluate-async")
async def evaluate_factor_async_api(
    factor_id: int,
    request: EvaluateFactorRequest,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_session)
):
    """ç•°æ­¥è©•ä¼°å› å­ï¼ˆè‡ªå‹•æ‡‰ç”¨ä¸¦ç™¼é™åˆ¶ï¼‰"""

    # è§¸ç™¼ç•°æ­¥ä»»å‹™
    task = evaluate_factor_async.delay(
        factor_id=factor_id,
        stock_pool=request.stock_pool,
        start_date=request.start_date,
        end_date=request.end_date
    )

    return {
        "task_id": task.id,
        "status": "submitted",
        "message": "è©•ä¼°ä»»å‹™å·²æäº¤ï¼Œè«‹ç¨å¾ŒæŸ¥è©¢çµæœ"
    }
```

### ç›´æ¥ä½¿ç”¨ ConcurrentLimiter

å¦‚æœéœ€è¦åœ¨å…¶ä»–åœ°æ–¹æ‡‰ç”¨ä¸¦ç™¼é™åˆ¶ï¼š

```python
from app.utils.concurrent_limit import evaluation_limiter

# æ–¹æ³• 1ï¼šæª¢æŸ¥å¾Œä½¿ç”¨
if evaluation_limiter.can_execute():
    with evaluation_limiter.acquire(task_id="my_task_123"):
        # åŸ·è¡Œè¨ˆç®—å¯†é›†å‹ä»»å‹™
        perform_heavy_computation()
else:
    logger.warning("Concurrent limit reached, task delayed")

# æ–¹æ³• 2ï¼šç­‰å¾…æ¨¡å¼
with evaluation_limiter.acquire(task_id="my_task_456", wait=True, wait_timeout=300):
    # æœƒç­‰å¾…æœ€å¤š 5 åˆ†é˜ç›´åˆ°æœ‰å¯ç”¨æ§½ä½
    perform_heavy_computation()
```

### è‡ªå®šç¾©é™åˆ¶å™¨

å‰µå»ºå…¶ä»–é¡å‹çš„ä¸¦ç™¼é™åˆ¶ï¼š

```python
# å‰µå»ºå›æ¸¬ä¸¦ç™¼é™åˆ¶å™¨ï¼ˆæœ€å¤š 5 å€‹ï¼‰
backtest_limiter = ConcurrentLimiter(
    key_prefix="backtest_concurrent",
    max_concurrent=5,
    timeout=7200  # 2 å°æ™‚
)

# ä½¿ç”¨
with backtest_limiter.acquire(task_id="backtest_123"):
    run_backtest()
```

---

## ç›£æ§èˆ‡ç¶­è­·

### æŸ¥çœ‹ç•¶å‰ä¸¦ç™¼ç‹€æ…‹

#### 1. æŸ¥çœ‹ Redis è¨ˆæ•¸å™¨

```bash
# æŸ¥çœ‹ç•¶å‰ä¸¦ç™¼æ•¸é‡
docker compose exec redis redis-cli GET evaluation_concurrent:counter

# æŸ¥çœ‹æ‰€æœ‰ä»»å‹™é–
docker compose exec redis redis-cli KEYS "evaluation_concurrent:lock:*"

# æŸ¥çœ‹é–çš„è©³ç´°ä¿¡æ¯
docker compose exec redis redis-cli TTL evaluation_concurrent:lock:eval_123_abc
```

#### 2. æŸ¥çœ‹ Celery æ´»å‹•ä»»å‹™

```bash
# æŸ¥çœ‹è©•ä¼° Worker çš„æ´»å‹•ä»»å‹™
docker compose exec backend celery -A app.core.celery_app inspect active --destination=celery@<evaluation-worker-hostname>

# æŸ¥çœ‹æ‰€æœ‰ Worker çš„æ´»å‹•ä»»å‹™
docker compose exec backend celery -A app.core.celery_app inspect active
```

#### 3. æŸ¥çœ‹è©•ä¼°ä½‡åˆ—é•·åº¦

```bash
# æŸ¥çœ‹ evaluation ä½‡åˆ—ä¸­ç­‰å¾…çš„ä»»å‹™æ•¸é‡
docker compose exec redis redis-cli LLEN evaluation
```

### æ—¥èªŒç›£æ§

```bash
# å³æ™‚è¿½è¹¤è©•ä¼° Worker æ—¥èªŒ
docker compose logs -f celery-evaluation-worker

# æœå°‹ä¸¦ç™¼ç›¸é—œæ—¥èªŒ
docker compose logs celery-evaluation-worker | grep "concurrent"

# æœå°‹é‡è©¦æ—¥èªŒ
docker compose logs celery-evaluation-worker | grep "retrying in 30 seconds"
```

### æ•ˆèƒ½ç›£æ§

#### Prometheus + Grafana

è©•ä¼°ä»»å‹™çš„ä¸¦ç™¼æŒ‡æ¨™å·²æ•´åˆåˆ° Celery Exporterï¼š

```yaml
# monitoring/prometheus.yml
scrape_configs:
  - job_name: 'celery-exporter'
    static_configs:
      - targets: ['celery-exporter:9808']
```

**å¯ç”¨æŒ‡æ¨™**ï¼š
- `celery_task_active{queue="evaluation"}` - è©•ä¼°ä½‡åˆ—çš„æ´»å‹•ä»»å‹™æ•¸
- `celery_task_runtime_seconds{task="evaluate_factor_async"}` - è©•ä¼°ä»»å‹™åŸ·è¡Œæ™‚é–“
- `celery_task_total{state="SUCCESS",task="evaluate_factor_async"}` - æˆåŠŸè©•ä¼°æ•¸
- `celery_task_total{state="RETRY",task="evaluate_factor_async"}` - é‡è©¦æ¬¡æ•¸

**Grafana å„€è¡¨æ¿**ï¼š
- URL: http://localhost:3001
- æŸ¥çœ‹ "Celery Tasks" å„€è¡¨æ¿
- ç¯©é¸ `queue="evaluation"`

### ç¶­è­·æ“ä½œ

#### é‡ç½®ä¸¦ç™¼è¨ˆæ•¸å™¨

å¦‚æœæ‡·ç–‘è¨ˆæ•¸å™¨ä¸æº–ç¢ºï¼ˆä¾‹å¦‚ Worker ç•°å¸¸çµ‚æ­¢ï¼‰ï¼š

```bash
# æ‰‹å‹•é‡ç½®è¨ˆæ•¸å™¨
docker compose exec redis redis-cli DEL evaluation_concurrent:counter

# åˆªé™¤æ‰€æœ‰ä»»å‹™é–
docker compose exec redis redis-cli DEL $(docker compose exec redis redis-cli KEYS "evaluation_concurrent:lock:*")
```

#### èª¿æ•´ä¸¦ç™¼é™åˆ¶

ä¿®æ”¹ `backend/app/utils/concurrent_limit.py`ï¼š

```python
# å°‡æœ€å¤§ä¸¦ç™¼å¾ 3 èª¿æ•´ç‚º 5
evaluation_limiter = ConcurrentLimiter(
    key_prefix="evaluation_concurrent",
    max_concurrent=5,  # æ”¹ç‚º 5
    timeout=3600
)
```

ç„¶å¾Œé‡å•Ÿæœå‹™ï¼š

```bash
docker compose restart backend celery-worker celery-evaluation-worker
```

#### èª¿æ•´ Worker ä¸¦ç™¼æ•¸

ä¿®æ”¹ `docker-compose.yml`ï¼š

```yaml
celery-evaluation-worker:
  # å°‡ä¸¦ç™¼å¾ 3 èª¿æ•´ç‚º 5
  command: celery -A app.core.celery_app worker --loglevel=info --concurrency=5 --queues=evaluation
```

é‡æ–°éƒ¨ç½²ï¼š

```bash
docker compose up -d --build celery-evaluation-worker
```

**æ³¨æ„**ï¼š
- **Worker concurrency** æ˜¯åŸ·è¡Œç·’æ± å¤§å°
- **ConcurrentLimiter max_concurrent** æ˜¯ Redis åˆ†æ•£å¼é–çš„é™åˆ¶
- å»ºè­°å…©è€…ä¿æŒä¸€è‡´ï¼Œæˆ– Redis é™åˆ¶ â‰¤ Worker concurrency

---

## æ¸¬è©¦é©—è­‰

### æ‰‹å‹•æ¸¬è©¦

#### æ¸¬è©¦ 1ï¼šå–®ä¸€è©•ä¼°ä»»å‹™

```bash
# è§¸ç™¼ä¸€å€‹è©•ä¼°ä»»å‹™
curl -X POST "http://localhost:8000/api/v1/factor-evaluation/1/evaluate-async" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "stock_pool": "all",
    "start_date": "2023-01-01",
    "end_date": "2023-12-31"
  }'

# æŸ¥çœ‹ Redis è¨ˆæ•¸å™¨
docker compose exec redis redis-cli GET evaluation_concurrent:counter
# é æœŸè¼¸å‡º: "1"

# æŸ¥çœ‹ä»»å‹™é–
docker compose exec redis redis-cli KEYS "evaluation_concurrent:lock:*"
# é æœŸè¼¸å‡º: 1 å€‹é–
```

#### æ¸¬è©¦ 2ï¼šä¸¦ç™¼é™åˆ¶è§¸ç™¼

åœ¨ Python è…³æœ¬ä¸­å¿«é€Ÿæäº¤ 10 å€‹è©•ä¼°ä»»å‹™ï¼š

```python
import requests
import concurrent.futures

def submit_evaluation(factor_id):
    response = requests.post(
        f"http://localhost:8000/api/v1/factor-evaluation/{factor_id}/evaluate-async",
        headers={"Authorization": f"Bearer {token}"},
        json={"stock_pool": "all"}
    )
    return response.json()

# ä¸¦ç™¼æäº¤ 10 å€‹ä»»å‹™
with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
    futures = [executor.submit(submit_evaluation, i) for i in range(1, 11)]
    results = [f.result() for f in futures]

# æŸ¥çœ‹çµæœ
for i, result in enumerate(results, 1):
    print(f"Task {i}: {result['status']}")
```

**é æœŸè¡Œç‚º**ï¼š
- å‰ 3 å€‹ä»»å‹™ç«‹å³åŸ·è¡Œ
- å¾Œ 7 å€‹ä»»å‹™é€²å…¥ RETRY ç‹€æ…‹ï¼ˆæ—¥èªŒé¡¯ç¤º "retrying in 30 seconds"ï¼‰
- 30 ç§’å¾Œé–‹å§‹é€æ­¥åŸ·è¡Œå¾ŒçºŒä»»å‹™

#### æ¸¬è©¦ 3ï¼šç›£æ§ä¸¦ç™¼æ•¸

åœ¨å¦ä¸€å€‹çµ‚ç«¯æŒçºŒç›£æ§ï¼š

```bash
# æŒçºŒç›£æ§ Redis è¨ˆæ•¸å™¨
watch -n 1 'docker compose exec redis redis-cli GET evaluation_concurrent:counter'

# é æœŸè¼¸å‡º: æ•¸å­—åœ¨ 0-3 ä¹‹é–“æ³¢å‹•
```

### è‡ªå‹•åŒ–æ¸¬è©¦

**æª”æ¡ˆ**ï¼š`backend/tests/utils/test_concurrent_limit.py`

```python
import pytest
import time
from unittest.mock import Mock
from app.utils.concurrent_limit import ConcurrentLimiter

class TestConcurrentLimiter:
    """ä¸¦ç™¼é™åˆ¶å™¨æ¸¬è©¦"""

    def test_limiter_initialization(self):
        """æ¸¬è©¦åˆå§‹åŒ–"""
        limiter = ConcurrentLimiter(
            key_prefix="test_limit",
            max_concurrent=3,
            timeout=60
        )
        assert limiter.max_concurrent == 3
        assert limiter.timeout == 60
        assert limiter.is_available()  # Redis å¯ç”¨

    def test_can_execute_initial(self):
        """æ¸¬è©¦åˆå§‹ç‹€æ…‹å¯åŸ·è¡Œ"""
        limiter = ConcurrentLimiter(key_prefix="test_can_exec", max_concurrent=3)
        limiter.reset()  # æ¸…ç©ºè¨ˆæ•¸å™¨
        assert limiter.can_execute() is True
        assert limiter.get_current_count() == 0

    def test_increment_and_decrement(self):
        """æ¸¬è©¦è¨ˆæ•¸å¢æ¸›"""
        limiter = ConcurrentLimiter(key_prefix="test_inc_dec", max_concurrent=3)
        limiter.reset()

        # å¢åŠ è¨ˆæ•¸
        assert limiter.increment("task_1") is True
        assert limiter.get_current_count() == 1

        assert limiter.increment("task_2") is True
        assert limiter.get_current_count() == 2

        # æ¸›å°‘è¨ˆæ•¸
        limiter.decrement("task_1")
        assert limiter.get_current_count() == 1

        limiter.decrement("task_2")
        assert limiter.get_current_count() == 0

    def test_concurrent_limit_reached(self):
        """æ¸¬è©¦ä¸¦ç™¼é™åˆ¶é”åˆ°"""
        limiter = ConcurrentLimiter(key_prefix="test_limit_reach", max_concurrent=3)
        limiter.reset()

        # å¢åŠ åˆ°é™åˆ¶
        assert limiter.increment("task_1") is True
        assert limiter.increment("task_2") is True
        assert limiter.increment("task_3") is True
        assert limiter.get_current_count() == 3

        # ç¬¬ 4 å€‹æ‡‰è©²å¤±æ•—
        assert limiter.increment("task_4") is False
        assert limiter.can_execute() is False

        # é‡‹æ”¾ä¸€å€‹
        limiter.decrement("task_1")
        assert limiter.can_execute() is True
        assert limiter.increment("task_4") is True

    def test_context_manager(self):
        """æ¸¬è©¦ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        limiter = ConcurrentLimiter(key_prefix="test_context", max_concurrent=3)
        limiter.reset()

        with limiter.acquire(task_id="test_task"):
            assert limiter.get_current_count() == 1
            # æ¨¡æ“¬ä»»å‹™åŸ·è¡Œ
            time.sleep(0.1)

        # é€€å‡ºå¾Œæ‡‰è©²è‡ªå‹•é‡‹æ”¾
        assert limiter.get_current_count() == 0

    def test_context_manager_exception(self):
        """æ¸¬è©¦ç•°å¸¸æ™‚ä¹Ÿæœƒé‡‹æ”¾"""
        limiter = ConcurrentLimiter(key_prefix="test_exception", max_concurrent=3)
        limiter.reset()

        try:
            with limiter.acquire(task_id="test_task"):
                assert limiter.get_current_count() == 1
                raise ValueError("Test error")
        except ValueError:
            pass

        # å³ä½¿æœ‰ç•°å¸¸ï¼Œä¹Ÿæ‡‰è©²é‡‹æ”¾
        assert limiter.get_current_count() == 0

    def test_timeout_cleanup(self):
        """æ¸¬è©¦è¶…æ™‚è‡ªå‹•æ¸…ç†"""
        limiter = ConcurrentLimiter(key_prefix="test_timeout", max_concurrent=3, timeout=2)
        limiter.reset()

        # å¢åŠ è¨ˆæ•¸
        limiter.increment("task_1")
        assert limiter.get_current_count() == 1

        # ç­‰å¾…è¶…æ™‚
        time.sleep(3)

        # é–æ‡‰è©²éæœŸï¼Œä½†è¨ˆæ•¸å™¨ä¸æœƒè‡ªå‹•æ¸…é™¤
        # éœ€è¦æ‰‹å‹• decrement æˆ– reset
        limiter.reset()
        assert limiter.get_current_count() == 0

    @pytest.mark.integration
    def test_concurrent_execution(self):
        """æ¸¬è©¦çœŸå¯¦ä¸¦ç™¼å ´æ™¯"""
        import threading

        limiter = ConcurrentLimiter(key_prefix="test_concurrent", max_concurrent=3)
        limiter.reset()

        results = []

        def worker(task_id):
            try:
                with limiter.acquire(task_id=f"task_{task_id}"):
                    results.append(("acquired", task_id))
                    time.sleep(0.5)
                results.append(("released", task_id))
            except RuntimeError:
                results.append(("rejected", task_id))

        # å•Ÿå‹• 5 å€‹åŸ·è¡Œç·’
        threads = [threading.Thread(target=worker, args=(i,)) for i in range(5)]
        for t in threads:
            t.start()
        for t in threads:
            t.join()

        # æ‡‰è©²æœ‰ 3 å€‹ acquiredï¼Œ2 å€‹ rejected
        acquired = [r for r in results if r[0] == "acquired"]
        rejected = [r for r in results if r[0] == "rejected"]

        assert len(acquired) == 3
        assert len(rejected) == 2
```

åŸ·è¡Œæ¸¬è©¦ï¼š

```bash
# åŸ·è¡Œæ‰€æœ‰ä¸¦ç™¼é™åˆ¶æ¸¬è©¦
docker compose exec backend pytest tests/utils/test_concurrent_limit.py -v

# åŸ·è¡Œæ•´åˆæ¸¬è©¦
docker compose exec backend pytest tests/utils/test_concurrent_limit.py -v -m integration
```

---

## æ•ˆèƒ½åˆ†æ

### è³‡æºä½¿ç”¨å°æ¯”

#### ç„¡ä¸¦ç™¼é™åˆ¶ï¼ˆå±éšªï¼‰

```
å ´æ™¯ï¼š10 å€‹ç”¨æˆ¶åŒæ™‚è©•ä¼°å› å­

è³‡æºä½¿ç”¨ï¼š
- è¨˜æ†¶é«”: 10 Ã— 300 MB = 3 GBï¼ˆè¶…å‡º 4 GB æœå‹™å™¨å®¹é‡ï¼‰
- CPU: 10 Ã— 100% = 1000%ï¼ˆåš´é‡ç«¶çˆ­ï¼‰
- éŸ¿æ‡‰æ™‚é–“: 120-180 ç§’ï¼ˆç›¸äº’å¹²æ“¾ï¼‰

é¢¨éšªï¼š
- âŒ è¨˜æ†¶é«”æº¢å‡ºï¼ˆOOMKilledï¼‰
- âŒ CPU éè¼‰ï¼ˆç³»çµ±å¡é “ï¼‰
- âŒ å…¶ä»–åŠŸèƒ½å—å½±éŸ¿ï¼ˆAPI éŸ¿æ‡‰æ…¢ï¼‰
```

#### æœ‰ä¸¦ç™¼é™åˆ¶ï¼ˆå®‰å…¨ï¼‰

```
å ´æ™¯ï¼š10 å€‹ç”¨æˆ¶åŒæ™‚è©•ä¼°å› å­

è³‡æºä½¿ç”¨ï¼š
- è¨˜æ†¶é«”: 3 Ã— 300 MB = 900 MBï¼ˆåœ¨å®‰å…¨ç¯„åœå…§ï¼‰
- CPU: 3 Ã— 100% = 300%ï¼ˆå¯æ§ï¼‰
- éŸ¿æ‡‰æ™‚é–“:
  - å‰ 3 å€‹ä»»å‹™: 30-60 ç§’ï¼ˆç«‹å³åŸ·è¡Œï¼‰
  - å¾Œ 7 å€‹ä»»å‹™: æ’éšŠç­‰å¾…ï¼ˆæ¯ 30 ç§’é‡è©¦ï¼‰
  - ç¸½æ™‚é–“: ç´„ 5-10 åˆ†é˜ï¼ˆä¾åºå®Œæˆï¼‰

å„ªå‹¢ï¼š
- âœ… è¨˜æ†¶é«”å¯æ§
- âœ… CPU ä¸éè¼‰
- âœ… å…¶ä»–åŠŸèƒ½æ­£å¸¸é‹è¡Œ
- âœ… ç³»çµ±ç©©å®šæ€§é«˜
```

### ååé‡åˆ†æ

```
å‡è¨­æ¯å€‹è©•ä¼°ä»»å‹™éœ€è¦ 60 ç§’ï¼š

ç„¡é™åˆ¶æ¨¡å¼ï¼ˆç†æƒ³æƒ…æ³ï¼Œä¸è€ƒæ…®è³‡æºç«¶çˆ­ï¼‰ï¼š
- 10 å€‹ä»»å‹™åŒæ™‚åŸ·è¡Œ
- ç¸½æ™‚é–“: 60 ç§’
- ååé‡: 10 tasks / 60s = 0.167 tasks/s

ç„¡é™åˆ¶æ¨¡å¼ï¼ˆå¯¦éš›æƒ…æ³ï¼Œè³‡æºç«¶çˆ­ï¼‰ï¼š
- 10 å€‹ä»»å‹™ç›¸äº’å¹²æ“¾ï¼Œæ¯å€‹è®Šæˆ 120 ç§’
- ç¸½æ™‚é–“: 120 ç§’
- ååé‡: 10 tasks / 120s = 0.083 tasks/s
- âŒ å¯èƒ½ OOMKilledï¼Œ0 å€‹å®Œæˆ

ä¸¦ç™¼é™åˆ¶æ¨¡å¼ï¼ˆmax_concurrent=3ï¼‰ï¼š
- ç¬¬ 1-3 å€‹ä»»å‹™: 0-60 ç§’å®Œæˆ
- ç¬¬ 4-6 å€‹ä»»å‹™: 60-120 ç§’å®Œæˆ
- ç¬¬ 7-9 å€‹ä»»å‹™: 120-180 ç§’å®Œæˆ
- ç¬¬ 10 å€‹ä»»å‹™: 180-240 ç§’å®Œæˆ
- ç¸½æ™‚é–“: 240 ç§’
- ååé‡: 10 tasks / 240s = 0.042 tasks/s
- âœ… 100% å®Œæˆç‡ï¼Œç³»çµ±ç©©å®š
```

**çµè«–**ï¼š
- ä¸¦ç™¼é™åˆ¶çŠ§ç‰²äº†ç†è«–ååé‡
- ä½†æä¾›äº†**ç©©å®šæ€§**å’Œ**å¯é æ¸¬æ€§**
- é¿å…äº†ç³»çµ±å´©æ½°ï¼ˆå¯¦éš›ååé‡æ›´é«˜ï¼‰

---

## æœªä¾†å„ªåŒ–

### 1. å‹•æ…‹ä¸¦ç™¼èª¿æ•´

æ ¹æ“šæœå‹™å™¨è² è¼‰å‹•æ…‹èª¿æ•´æœ€å¤§ä¸¦ç™¼æ•¸ï¼š

```python
class AdaptiveConcurrentLimiter(ConcurrentLimiter):
    """è‡ªé©æ‡‰ä¸¦ç™¼é™åˆ¶å™¨"""

    def get_optimal_max_concurrent(self) -> int:
        """æ ¹æ“šç³»çµ±è² è¼‰è¨ˆç®—æœ€ä½³ä¸¦ç™¼æ•¸"""
        import psutil

        # ç²å–ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨ç‡
        memory_percent = psutil.virtual_memory().percent

        # ç²å–ç•¶å‰ CPU ä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)

        # å‹•æ…‹èª¿æ•´
        if memory_percent > 80 or cpu_percent > 80:
            return 2  # é«˜è² è¼‰ï¼šé™ä½ä¸¦ç™¼
        elif memory_percent < 50 and cpu_percent < 50:
            return 5  # ä½è² è¼‰ï¼šæé«˜ä¸¦ç™¼
        else:
            return 3  # ä¸­ç­‰è² è¼‰ï¼šä¿æŒé è¨­

    def can_execute(self) -> bool:
        # å‹•æ…‹æ›´æ–° max_concurrent
        self.max_concurrent = self.get_optimal_max_concurrent()
        return super().can_execute()
```

### 2. å„ªå…ˆç´šä½‡åˆ—

ç‚ºä¸åŒç”¨æˆ¶æˆ–ä»»å‹™è¨­ç½®å„ªå…ˆç´šï¼š

```python
# Celery ä½‡åˆ—é…ç½®
celery_app.conf.update(
    task_routes={
        'app.tasks.evaluate_factor_async': {
            'queue': 'evaluation',
            'priority': lambda task: 10 if task.kwargs.get('priority') == 'high' else 5
        }
    }
)

# API èª¿ç”¨
evaluate_factor_async.apply_async(
    kwargs={'factor_id': 1, 'priority': 'high'},
    priority=10  # é«˜å„ªå…ˆç´šä»»å‹™
)
```

### 3. æ™ºæ…§é‡è©¦ç­–ç•¥

æ ¹æ“šä»»å‹™é¡å‹èª¿æ•´é‡è©¦åƒæ•¸ï¼š

```python
@celery_app.task(bind=True, name="app.tasks.evaluate_factor_async")
def evaluate_factor_async(self: Task, factor_id: int, ...):
    if not evaluation_limiter.can_execute():
        # æ ¹æ“šç•¶å‰ä½‡åˆ—é•·åº¦å‹•æ…‹èª¿æ•´ countdown
        queue_length = get_queue_length('evaluation')
        countdown = 30 + (queue_length * 10)  # ä½‡åˆ—è¶Šé•·ï¼Œç­‰å¾…è¶Šä¹…

        raise self.retry(countdown=countdown, max_retries=10)
```

### 4. è©•ä¼°å¿«å–æ•´åˆ

çµåˆ Redis å¿«å–ï¼Œé¿å…é‡è¤‡è©•ä¼°ï¼š

```python
# å·²åœ¨ FactorEvaluationService ä¸­å¯¦ä½œ
@cached_method(key_prefix="factor_evaluation", expiry=3600)
def evaluate_factor(self, factor_id: int, ...):
    # å¦‚æœå¿«å–å‘½ä¸­ï¼Œä¸æ¶ˆè€—ä¸¦ç™¼æ§½ä½
    ...
```

### 5. ç›£æ§å‘Šè­¦

è¨­ç½® Prometheus å‘Šè­¦è¦å‰‡ï¼š

```yaml
# monitoring/prometheus/alerts.yml
groups:
  - name: evaluation_alerts
    rules:
      - alert: EvaluationQueueTooLong
        expr: celery_queue_length{queue="evaluation"} > 10
        for: 5m
        annotations:
          summary: "è©•ä¼°ä½‡åˆ—éé•·"
          description: "è©•ä¼°ä½‡åˆ—æœ‰ {{ $value }} å€‹å¾…è™•ç†ä»»å‹™"

      - alert: EvaluationConcurrentHigh
        expr: celery_task_active{queue="evaluation"} >= 3
        for: 10m
        annotations:
          summary: "è©•ä¼°ä¸¦ç™¼æ•¸æŒçºŒæœ€å¤§"
          description: "è©•ä¼°ä»»å‹™å·²æ»¿è¼‰è¶…é 10 åˆ†é˜"
```

### 6. åˆ†æ•£å¼è¿½è¹¤

æ•´åˆ OpenTelemetry è¿½è¹¤è©•ä¼°ä»»å‹™ï¼š

```python
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

@celery_app.task(bind=True)
def evaluate_factor_async(self: Task, factor_id: int, ...):
    with tracer.start_as_current_span("factor_evaluation") as span:
        span.set_attribute("factor_id", factor_id)
        span.set_attribute("concurrent_count", evaluation_limiter.get_current_count())

        with evaluation_limiter.acquire(task_id=task_id):
            # è©•ä¼°é‚è¼¯
            ...
```

---

## ç¸½çµ

### âœ… å·²å¯¦ç¾åŠŸèƒ½

1. **Redis åˆ†æ•£å¼é–**ï¼šè·¨ Worker çš„å…¨å±€ä¸¦ç™¼æ§åˆ¶
2. **å°ˆç”¨è©•ä¼°ä½‡åˆ—**ï¼šéš”é›¢è©•ä¼°ä»»å‹™ï¼Œé¿å…å½±éŸ¿å…¶ä»–åŠŸèƒ½
3. **è‡ªå‹•é‡è©¦æ©Ÿåˆ¶**ï¼šé”åˆ°é™åˆ¶æ™‚æ™ºæ…§å»¶é²é‡è©¦
4. **ä¸Šä¸‹æ–‡ç®¡ç†å™¨**ï¼šç°¡åŒ–ä½¿ç”¨ï¼Œè‡ªå‹•æ¸…ç†è³‡æº
5. **å®Œæ•´ç›£æ§**ï¼šæ—¥èªŒã€Prometheus æŒ‡æ¨™ã€Grafana å„€è¡¨æ¿
6. **å®¹éŒ¯è¨­è¨ˆ**ï¼šRedis ä¸å¯ç”¨æ™‚ä¸é™åˆ¶ï¼ˆå„ªé›…é™ç´šï¼‰

### ğŸ“Š æ•ˆèƒ½æå‡

- **è¨˜æ†¶é«”å¯æ§**ï¼š3 Ã— 300 MB = 900 MBï¼ˆvs. ç„¡é™åˆ¶çš„ 3+ GBï¼‰
- **ç³»çµ±ç©©å®šæ€§**ï¼šé¿å… OOMKilled å’Œ CPU éè¼‰
- **å¯é æ¸¬æ€§**ï¼šå›ºå®šçš„è³‡æºä½¿ç”¨ï¼Œä¾¿æ–¼å®¹é‡è¦åŠƒ
- **100% å®Œæˆç‡**ï¼šæ’éšŠæ©Ÿåˆ¶ç¢ºä¿æ‰€æœ‰ä»»å‹™æœ€çµ‚å®Œæˆ

### ğŸ¯ é©ç”¨å ´æ™¯

âœ… **å¤šç”¨æˆ¶ç’°å¢ƒ**ï¼šé¿å…ç”¨æˆ¶é–“è³‡æºç«¶çˆ­
âœ… **è‡ªå‹•åŒ–æµç¨‹**ï¼šRD-Agent æ‰¹é‡ç”Ÿæˆå› å­
âœ… **æ‰¹é‡è©•ä¼°**ï¼šä¸€æ¬¡è©•ä¼°æ•¸åå€‹å› å­
âœ… **è³‡æºå—é™ç’°å¢ƒ**ï¼š4-8 GB è¨˜æ†¶é«”çš„æœå‹™å™¨

### ğŸ“š ç›¸é—œæ–‡æª”

- [RDAGENT_REDIS_CACHE_IMPLEMENTATION_REPORT.md](./RDAGENT_REDIS_CACHE_IMPLEMENTATION_REPORT.md) - Redis å¿«å–å¯¦ä½œå ±å‘Š
- [RDAGENT.md](../docs/RDAGENT.md) - RD-Agent å®Œæ•´æŒ‡å—
- [CELERY_TASKS_GUIDE.md](./CELERY_TASKS_GUIDE.md) - Celery ä»»å‹™ç®¡ç†æŒ‡å—

---

**å¯¦ä½œå®Œæˆæ™‚é–“**: 2025-12-29
**éƒ¨ç½²ç‹€æ…‹**: âœ… å·²éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ
**æ–‡æª”ç¶­è­·è€…**: Claude Code
