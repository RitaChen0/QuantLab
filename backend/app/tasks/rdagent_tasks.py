"""RD-Agent Celery ç•°æ­¥ä»»å‹™"""

from celery import Task
from sqlalchemy.orm import Session
from loguru import logger

from app.core.celery_app import celery_app
from app.db.session import SessionLocal
from app.services.rdagent_service import RDAgentService
from app.models.rdagent import RDAgentTask, TaskStatus


@celery_app.task(bind=True, name="app.tasks.run_factor_mining_task")
def run_factor_mining_task(self: Task, task_id: int):
    """åŸ·è¡Œå› å­æŒ–æ˜ä»»å‹™

    âš ï¸ æ³¨æ„ï¼šæ­¤ä»»å‹™éœ€è¦ RD-Agent ç’°å¢ƒå’Œ LLM API é…ç½®

    ç•¶å‰ç‰ˆæœ¬ç‚ºæ¨¡æ“¬åŸ·è¡Œï¼Œå¯¦éš›ä½¿ç”¨æ™‚éœ€è¦æ•´åˆ RD-Agent æ ¸å¿ƒé‚è¼¯ã€‚

    Args:
        task_id: RD-Agent ä»»å‹™ ID

    Returns:
        dict: ä»»å‹™åŸ·è¡Œçµæœ
    """
    db: Session = SessionLocal()

    try:
        service = RDAgentService(db)
        task = db.query(RDAgentTask).filter(RDAgentTask.id == task_id).first()

        if not task:
            logger.error(f"Task {task_id} not found")
            return {"status": "error", "message": "Task not found"}

        # æ›´æ–°ç‚ºåŸ·è¡Œä¸­
        service.update_task_status(task_id, TaskStatus.RUNNING)

        logger.info(f"Starting factor mining task {task_id}")
        logger.info(f"Task parameters: {task.input_params}")

        # æå–ä»»å‹™åƒæ•¸
        research_goal = task.input_params.get("research_goal", "Generate profitable trading factors")
        max_iterations = task.input_params.get("max_iterations", 3)
        llm_model = task.input_params.get("llm_model", "gpt-4-turbo")

        # ========== æ­¥é©Ÿ 1: åŸ·è¡Œ RD-Agent å› å­æŒ–æ˜ ==========
        logger.info(f"Step 1: Executing RD-Agent with {max_iterations} iterations...")
        log_dir = service.execute_factor_mining(
            task_id=task_id,
            research_goal=research_goal,
            max_iterations=max_iterations,
            llm_model=llm_model
        )
        logger.info(f"RD-Agent execution completed. Log directory: {log_dir}")

        # ========== æ­¥é©Ÿ 2: è§£æ RD-Agent çµæœ ==========
        logger.info("Step 2: Parsing RD-Agent results...")
        factors = service.parse_rdagent_results(log_dir)
        logger.info(f"Parsed {len(factors)} factors from results")

        # ========== æ­¥é©Ÿ 3: ä¿å­˜ç”Ÿæˆçš„å› å­ ==========
        logger.info("Step 3: Saving generated factors to database...")
        logger.info(f"Total factors to save: {len(factors)}")

        saved_factors = []
        failed_factors = []

        for i, factor_data in enumerate(factors, 1):
            factor_name = factor_data.get("name", "Unknown")
            logger.info(f"[{i}/{len(factors)}] Saving factor: {factor_name}")

            # é‡è©¦æ©Ÿåˆ¶ï¼šæœ€å¤šé‡è©¦ 3 æ¬¡
            max_retries = 3
            retry_delay = 1  # ç§’
            saved = False

            for attempt in range(1, max_retries + 1):
                try:
                    factor = service.save_generated_factor(
                        task_id=task_id,
                        user_id=task.user_id,
                        name=factor_data["name"],
                        formula=factor_data["formula"],
                        description=factor_data.get("description"),
                        category=factor_data.get("category"),
                        metadata=factor_data.get("metadata")
                    )

                    saved_factors.append({
                        "id": factor.id,
                        "name": factor.name,
                        "formula": factor.formula
                    })

                    logger.info(f"âœ… Saved factor {factor.id}: {factor.name}")
                    saved = True
                    break  # å„²å­˜æˆåŠŸï¼Œè·³å‡ºé‡è©¦å¾ªç’°

                except Exception as e:
                    logger.error(f"âŒ Attempt {attempt}/{max_retries} failed for factor '{factor_name}': {str(e)}")
                    logger.error(f"   Factor data: name={factor_data.get('name')}, formula_length={len(factor_data.get('formula', ''))}")

                    if attempt < max_retries:
                        import time
                        logger.warning(f"   Retrying in {retry_delay} seconds...")
                        time.sleep(retry_delay)
                    else:
                        # æœ€å¾Œä¸€æ¬¡é‡è©¦ä¹Ÿå¤±æ•—äº†
                        logger.error(f"   All {max_retries} attempts failed. Factor will be skipped.")
                        failed_factors.append({
                            "name": factor_name,
                            "error": str(e),
                            "formula": factor_data.get("formula", "")[:100]  # åªè¨˜éŒ„å‰ 100 å­—å…ƒ
                        })

            if not saved:
                logger.warning(f"âš ï¸  Factor '{factor_name}' was not saved after {max_retries} attempts")

        # ========== æ­¥é©Ÿ 3.5: äº‹å‹™ä¸€è‡´æ€§æª¢æŸ¥ ==========
        logger.info("Step 3.5: Verifying transaction consistency...")
        logger.info(f"Parsed factors: {len(factors)}")
        logger.info(f"Successfully saved: {len(saved_factors)}")
        logger.info(f"Failed to save: {len(failed_factors)}")

        if len(failed_factors) > 0:
            logger.warning("âš ï¸  Some factors failed to save:")
            for failed in failed_factors:
                logger.warning(f"  - {failed['name']}: {failed['error']}")

        if len(saved_factors) != len(factors):
            logger.warning(f"âš ï¸  Transaction consistency issue detected!")
            logger.warning(f"   Expected: {len(factors)} factors")
            logger.warning(f"   Actually saved: {len(saved_factors)} factors")
            logger.warning(f"   Missing: {len(factors) - len(saved_factors)} factors")
        else:
            logger.info("âœ… Transaction consistency verified: All factors saved successfully")

        # ========== æ­¥é©Ÿ 4: è¨ˆç®— LLM æˆæœ¬ ==========
        logger.info("Step 4: Calculating LLM costs...")
        llm_calls, llm_cost = service.calculate_llm_costs(log_dir)
        logger.info(f"LLM API calls: {llm_calls}, Estimated cost: ${llm_cost}")

        # ========== æ­¥é©Ÿ 5: æ›´æ–°ä»»å‹™ç‚ºå®Œæˆ ==========
        # æ§‹å»ºçµæœè¨Šæ¯
        result_message = "Factor mining completed successfully"
        if len(failed_factors) > 0:
            result_message = f"Factor mining completed with warnings: {len(failed_factors)} factors failed to save"

        service.update_task_status(
            task_id,
            TaskStatus.COMPLETED,
            result={
                "generated_factors_count": len(factors),
                "saved_factors_count": len(saved_factors),
                "failed_factors_count": len(failed_factors),
                "log_directory": log_dir,
                "factors": saved_factors,  # åªåŒ…å«æˆåŠŸå„²å­˜çš„å› å­ï¼ˆå« IDï¼‰
                "failed_factors": failed_factors if failed_factors else None,  # å¤±æ•—çš„å› å­è³‡è¨Š
                "consistency_check": {
                    "parsed": len(factors),
                    "saved": len(saved_factors),
                    "failed": len(failed_factors),
                    "passed": len(saved_factors) == len(factors)
                },
                "message": result_message
            },
            llm_calls=llm_calls,
            llm_cost=llm_cost
        )

        logger.info(f"Factor mining task {task_id} completed")
        logger.info(f"Parsed: {len(factors)}, Saved: {len(saved_factors)}, Failed: {len(failed_factors)}")
        logger.info(f"LLM calls: {llm_calls}, Cost: ${llm_cost}")

        # ========== æ­¥é©Ÿ 6: è§¸ç™¼è‡ªå‹•è©•ä¼° ==========
        logger.info("Step 6: Triggering automatic factor evaluation...")

        evaluation_tasks = []
        for factor_info in saved_factors:
            factor_id = factor_info["id"]
            factor_name = factor_info["name"]

            try:
                # ç•°æ­¥è§¸ç™¼è©•ä¼°ä»»å‹™
                from app.tasks.factor_evaluation_tasks import evaluate_factor_async

                task_result = evaluate_factor_async.delay(
                    factor_id=factor_id,
                    stock_pool="all",
                    start_date=None,  # ä½¿ç”¨é è¨­ 2 å¹´
                    end_date=None
                )

                evaluation_tasks.append({
                    "factor_id": factor_id,
                    "factor_name": factor_name,
                    "task_id": task_result.id
                })

                logger.info(f"âœ… Triggered evaluation for factor {factor_id} ({factor_name}), task_id: {task_result.id}")

            except Exception as e:
                logger.error(f"âŒ Failed to trigger evaluation for factor {factor_id} ({factor_name}): {str(e)}")

        logger.info(f"Triggered {len(evaluation_tasks)} evaluation tasks for {len(saved_factors)} factors")

        return {
            "status": "success",
            "task_id": task_id,
            "factors_generated": len(factors),
            "llm_calls": llm_calls,
            "llm_cost": llm_cost,
            "log_directory": log_dir,
            "evaluation_tasks": evaluation_tasks  # æ–°å¢ï¼šè¿”å›è§¸ç™¼çš„è©•ä¼°ä»»å‹™è³‡è¨Š
        }

    except Exception as e:
        logger.error(f"Factor mining task {task_id} failed: {str(e)}")

        service.update_task_status(
            task_id,
            TaskStatus.FAILED,
            error_message=str(e)
        )

        return {"status": "error", "message": str(e)}

    finally:
        db.close()


@celery_app.task(bind=True, name="app.tasks.run_strategy_optimization_task")
def run_strategy_optimization_task(self: Task, task_id: int):
    """åŸ·è¡Œç­–ç•¥å„ªåŒ–ä»»å‹™

    ä½¿ç”¨ LLM åˆ†æç­–ç•¥ä»£ç¢¼å’Œå›æ¸¬çµæœï¼Œæä¾›å„ªåŒ–å»ºè­°

    Args:
        task_id: RD-Agent ä»»å‹™ ID

    Returns:
        dict: ä»»å‹™åŸ·è¡Œçµæœ
    """
    db: Session = SessionLocal()

    try:
        from app.services.strategy_optimizer import StrategyOptimizer

        service = RDAgentService(db)
        task = db.query(RDAgentTask).filter(RDAgentTask.id == task_id).first()

        if not task:
            logger.error(f"Task {task_id} not found")
            return {"status": "error", "message": "Task not found"}

        # æ›´æ–°ç‚ºåŸ·è¡Œä¸­
        service.update_task_status(task_id, TaskStatus.RUNNING)

        logger.info(f"Starting strategy optimization task {task_id}")
        logger.info(f"Task parameters: {task.input_params}")

        # æå–ä»»å‹™åƒæ•¸
        strategy_id = task.input_params.get("strategy_id")
        optimization_goal = task.input_params.get("optimization_goal", "æå‡æ•´é«”ç¸¾æ•ˆè¡¨ç¾")
        llm_model = task.input_params.get("llm_model", "gpt-4-turbo")
        max_iterations = task.input_params.get("max_iterations", 1)

        if not strategy_id:
            raise ValueError("strategy_id is required in input_params")

        # ========== æ­¥é©Ÿ 1: åˆå§‹åŒ–å„ªåŒ–å™¨ ==========
        logger.info("Step 1: Initializing strategy optimizer...")
        optimizer = StrategyOptimizer(db)

        # ========== æ­¥é©Ÿ 2: åˆ†æç­–ç•¥ä¸¦ç”Ÿæˆå„ªåŒ–å»ºè­° ==========
        logger.info(f"Step 2: Analyzing strategy {strategy_id}...")
        analysis_result = optimizer.analyze_strategy(
            strategy_id=strategy_id,
            optimization_goal=optimization_goal,
            llm_model=llm_model
        )

        logger.info(f"âœ… Strategy analysis completed")
        logger.info(f"   Current Sharpe Ratio: {analysis_result['current_performance']['sharpe_ratio']}")
        logger.info(f"   Issues diagnosed: {len(analysis_result['issues_diagnosed'])}")
        logger.info(f"   Suggestions generated: {len(analysis_result['optimization_suggestions'])}")

        # ========== æ­¥é©Ÿ 3: æå–é—œéµæŒ‡æ¨™ ==========
        current_perf = analysis_result["current_performance"]
        suggestions = analysis_result["optimization_suggestions"]

        # ä¼°ç®—æ”¹é€²å¹…åº¦ï¼ˆåŸºæ–¼å»ºè­°çš„å„ªå…ˆç´šï¼‰
        high_priority_count = sum(1 for s in suggestions if s.get("priority") == "high")
        estimated_improvement = min(high_priority_count * 15, 50)  # æ¯å€‹é«˜å„ªå…ˆç´šå»ºè­°é è¨ˆæ”¹å–„ 15%ï¼Œæœ€å¤š 50%

        current_sharpe = current_perf.get("sharpe_ratio") or 0.0
        estimated_sharpe = current_sharpe * (1 + estimated_improvement / 100)

        # ========== æ­¥é©Ÿ 4: æ§‹å»ºçµæœ ==========
        optimization_result = {
            "strategy_info": analysis_result["strategy_info"],
            "current_performance": current_perf,
            "issues_diagnosed": analysis_result["issues_diagnosed"],
            "optimization_suggestions": suggestions,
            "optimized_code": analysis_result.get("optimized_code"),
            "estimated_improvements": {
                "sharpe_ratio_before": current_sharpe,
                "sharpe_ratio_estimated": round(estimated_sharpe, 2),
                "improvement_pct": estimated_improvement,
                "high_priority_suggestions": high_priority_count,
                "total_suggestions": len(suggestions)
            },
            "message": f"ç­–ç•¥å„ªåŒ–åˆ†æå®Œæˆï¼Œç”Ÿæˆ {len(suggestions)} æ¢å„ªåŒ–å»ºè­°"
        }

        # ========== æ­¥é©Ÿ 5: æ›´æ–°ä»»å‹™ç‹€æ…‹ ==========
        llm_metadata = analysis_result.get("llm_metadata", {})
        service.update_task_status(
            task_id,
            TaskStatus.COMPLETED,
            result=optimization_result,
            llm_calls=llm_metadata.get("calls", 1),
            llm_cost=llm_metadata.get("cost", 0.0)
        )

        logger.info(f"Strategy optimization task {task_id} completed")
        logger.info(f"LLM calls: {llm_metadata.get('calls')}, Cost: ${llm_metadata.get('cost')}")

        return {
            "status": "success",
            "task_id": task_id,
            "suggestions_count": len(suggestions),
            "llm_calls": llm_metadata.get("calls", 0),
            "llm_cost": llm_metadata.get("cost", 0.0)
        }

    except Exception as e:
        logger.error(f"Strategy optimization task {task_id} failed: {str(e)}")
        import traceback
        logger.error(f"Full traceback:\n{traceback.format_exc()}")

        service.update_task_status(
            task_id,
            TaskStatus.FAILED,
            error_message=str(e)
        )

        return {"status": "error", "message": str(e)}

    finally:
        db.close()


@celery_app.task(bind=True, name="app.tasks.run_model_generation_task")
def run_model_generation_task(self: Task, task_id: int):
    """åŸ·è¡Œæ¨¡å‹ç”Ÿæˆä»»å‹™

    âš ï¸ æ³¨æ„ï¼šæ­¤ä»»å‹™éœ€è¦ RD-Agent ç’°å¢ƒå’Œ LLM API é…ç½®

    ä½¿ç”¨ RD-Agent çš„ model.py æ¨¡çµ„è‡ªå‹•ç”Ÿæˆé‡åŒ–æ¨¡å‹æ¶æ§‹

    Args:
        task_id: RD-Agent ä»»å‹™ ID

    Returns:
        dict: ä»»å‹™åŸ·è¡Œçµæœ
    """
    db: Session = SessionLocal()

    try:
        service = RDAgentService(db)
        task = db.query(RDAgentTask).filter(RDAgentTask.id == task_id).first()

        if not task:
            logger.error(f"Task {task_id} not found")
            return {"status": "error", "message": "Task not found"}

        # æ›´æ–°ç‚ºåŸ·è¡Œä¸­
        service.update_task_status(task_id, TaskStatus.RUNNING)

        logger.info(f"Starting model generation task {task_id}")
        logger.info(f"Task parameters: {task.input_params}")

        # æå–ä»»å‹™åƒæ•¸
        research_goal = task.input_params.get("research_goal", "Generate quantitative models")
        max_iterations = task.input_params.get("max_iterations", 5)
        llm_model = task.input_params.get("llm_model", "gpt-4-turbo")

        # ========== æ­¥é©Ÿ 1: åŸ·è¡Œ RD-Agent æ¨¡å‹ç”Ÿæˆ ==========
        logger.info(f"Step 1: Executing RD-Agent model generation with {max_iterations} iterations...")
        log_dir = service.execute_model_generation(
            task_id=task_id,
            research_goal=research_goal,
            max_iterations=max_iterations,
            llm_model=llm_model
        )
        logger.info(f"RD-Agent model generation completed. Log directory: {log_dir}")

        # ========== æ­¥é©Ÿ 2: è§£æ RD-Agent çµæœ ==========
        logger.info("Step 2: Parsing RD-Agent model generation results...")
        models = service.parse_model_generation_results(log_dir)
        logger.info(f"Parsed {len(models)} models from results")

        # ========== æ­¥é©Ÿ 3: ä¿å­˜ç”Ÿæˆçš„æ¨¡å‹ ==========
        logger.info("Step 3: Saving generated models to database...")
        logger.info(f"Total models to save: {len(models)}")

        saved_models = []
        failed_models = []

        for i, model_data in enumerate(models, 1):
            model_name = model_data.get("name", "Unknown")
            logger.info(f"[{i}/{len(models)}] Saving model: {model_name}")

            # é‡è©¦æ©Ÿåˆ¶ï¼šæœ€å¤šé‡è©¦ 3 æ¬¡
            max_retries = 3
            retry_delay = 1  # ç§’
            saved = False

            for attempt in range(1, max_retries + 1):
                try:
                    model = service.save_generated_model(
                        task_id=task_id,
                        user_id=task.user_id,
                        name=model_data["name"],
                        model_type=model_data["model_type"],
                        description=model_data.get("description"),
                        formulation=model_data.get("formulation"),
                        architecture=model_data.get("architecture"),
                        variables=model_data.get("variables"),
                        hyperparameters=model_data.get("hyperparameters"),
                        code=model_data.get("code"),  # æ–°å¢ï¼šä¿å­˜ä»£ç¢¼
                        qlib_config=model_data.get("qlib_config"),  # æ–°å¢ï¼šä¿å­˜ Qlib é…ç½®
                        iteration=model_data.get("iteration"),
                        metadata=model_data.get("metadata")
                    )

                    saved_models.append({
                        "id": model.id,
                        "name": model.name,
                        "model_type": model.model_type,
                        "architecture": model.architecture
                    })

                    logger.info(f"âœ… Saved model {model.id}: {model.name}")
                    saved = True
                    break  # å„²å­˜æˆåŠŸï¼Œè·³å‡ºé‡è©¦å¾ªç’°

                except Exception as e:
                    logger.error(f"âŒ Attempt {attempt}/{max_retries} failed for model '{model_name}': {str(e)}")

                    if attempt < max_retries:
                        import time
                        logger.warning(f"   Retrying in {retry_delay} seconds...")
                        time.sleep(retry_delay)
                    else:
                        # æœ€å¾Œä¸€æ¬¡é‡è©¦ä¹Ÿå¤±æ•—äº†
                        logger.error(f"   All {max_retries} attempts failed. Model will be skipped.")
                        failed_models.append({
                            "name": model_name,
                            "error": str(e),
                            "model_type": model_data.get("model_type", "")
                        })

            if not saved:
                logger.warning(f"âš ï¸  Model '{model_name}' was not saved after {max_retries} attempts")

        # ========== æ­¥é©Ÿ 3.5: äº‹å‹™ä¸€è‡´æ€§æª¢æŸ¥ ==========
        logger.info("Step 3.5: Verifying transaction consistency...")
        logger.info(f"Parsed models: {len(models)}")
        logger.info(f"Successfully saved: {len(saved_models)}")
        logger.info(f"Failed to save: {len(failed_models)}")

        if len(failed_models) > 0:
            logger.warning("âš ï¸  Some models failed to save:")
            for failed in failed_models:
                logger.warning(f"  - {failed['name']}: {failed['error']}")

        if len(saved_models) != len(models):
            logger.warning(f"âš ï¸  Transaction consistency issue detected!")
            logger.warning(f"   Expected: {len(models)} models")
            logger.warning(f"   Actually saved: {len(saved_models)} models")
            logger.warning(f"   Missing: {len(models) - len(saved_models)} models")
        else:
            logger.info("âœ… Transaction consistency verified: All models saved successfully")

        # ========== æ­¥é©Ÿ 4: è¨ˆç®— LLM æˆæœ¬ ==========
        logger.info("Step 4: Calculating LLM costs...")
        llm_calls, llm_cost = service.calculate_llm_costs(log_dir)
        logger.info(f"LLM API calls: {llm_calls}, Estimated cost: ${llm_cost}")

        # ========== æ­¥é©Ÿ 5: æ›´æ–°ä»»å‹™ç‚ºå®Œæˆ ==========
        # æ§‹å»ºçµæœè¨Šæ¯
        result_message = "Model generation completed successfully"
        if len(failed_models) > 0:
            result_message = f"Model generation completed with warnings: {len(failed_models)} models failed to save"

        service.update_task_status(
            task_id,
            TaskStatus.COMPLETED,
            result={
                "generated_models_count": len(models),
                "saved_models_count": len(saved_models),
                "failed_models_count": len(failed_models),
                "log_directory": log_dir,
                "models": saved_models,  # åªåŒ…å«æˆåŠŸå„²å­˜çš„æ¨¡å‹ï¼ˆå« IDï¼‰
                "failed_models": failed_models if failed_models else None,
                "consistency_check": {
                    "parsed": len(models),
                    "saved": len(saved_models),
                    "failed": len(failed_models),
                    "passed": len(saved_models) == len(models)
                },
                "message": result_message
            },
            llm_calls=llm_calls,
            llm_cost=llm_cost
        )

        logger.info(f"Model generation task {task_id} completed")
        logger.info(f"Parsed: {len(models)}, Saved: {len(saved_models)}, Failed: {len(failed_models)}")
        logger.info(f"LLM calls: {llm_calls}, Cost: ${llm_cost}")

        return {
            "status": "success",
            "task_id": task_id,
            "models_generated": len(models),
            "llm_calls": llm_calls,
            "llm_cost": llm_cost,
            "log_directory": log_dir
        }

    except Exception as e:
        logger.error(f"Model generation task {task_id} failed: {str(e)}")

        service.update_task_status(
            task_id,
            TaskStatus.FAILED,
            error_message=str(e)
        )

        return {"status": "error", "message": str(e)}

    finally:
        db.close()


@celery_app.task(bind=True, name="app.tasks.cleanup_stuck_rdagent_tasks")
def cleanup_stuck_rdagent_tasks(self: Task, timeout_hours: int = 24) -> dict:
    """æ¸…ç†åŸ·è¡Œè¶…æ™‚çš„ RD-Agent ä»»å‹™

    å®šæœŸæª¢æŸ¥ä¸¦æ¸…ç†è™•æ–¼ RUNNING ç‹€æ…‹è¶…éæŒ‡å®šæ™‚é–“çš„ä»»å‹™ï¼Œ
    é˜²æ­¢ä»»å‹™æ°¸ä¹…å¡ä½ä½”ç”¨è³‡æºã€‚

    Args:
        timeout_hours: è¶…æ™‚æ™‚é–“ï¼ˆå°æ™‚ï¼‰ï¼Œé è¨­ 24 å°æ™‚

    Returns:
        dict: æ¸…ç†çµ±è¨ˆè³‡è¨Š
    """
    from datetime import datetime, timedelta, timezone
    from sqlalchemy import and_

    db: Session = SessionLocal()

    try:
        logger.info(f"ğŸ§¹ é–‹å§‹æ¸…ç†å¡ä½çš„ RD-Agent ä»»å‹™ï¼ˆè¶…æ™‚: {timeout_hours} å°æ™‚ï¼‰")

        # è¨ˆç®—è¶…æ™‚æ™‚é–“é»
        timeout_threshold = datetime.now(timezone.utc) - timedelta(hours=timeout_hours)

        # æŸ¥è©¢å¡ä½çš„ä»»å‹™
        stuck_tasks = db.query(RDAgentTask).filter(
            and_(
                RDAgentTask.status == TaskStatus.RUNNING,
                RDAgentTask.started_at < timeout_threshold
            )
        ).all()

        if not stuck_tasks:
            logger.info("âœ… æ²’æœ‰å¡ä½çš„ä»»å‹™")
            return {
                "status": "success",
                "cleaned_count": 0,
                "tasks": []
            }

        logger.warning(f"âš ï¸  ç™¼ç¾ {len(stuck_tasks)} å€‹å¡ä½çš„ä»»å‹™")

        cleaned_tasks = []
        for task in stuck_tasks:
            running_hours = (datetime.now(timezone.utc) - task.started_at).total_seconds() / 3600

            logger.info(f"ğŸ“‹ æ¸…ç†ä»»å‹™ {task.id}:")
            logger.info(f"   - é¡å‹: {task.task_type}")
            logger.info(f"   - ç”¨æˆ¶: {task.user_id}")
            logger.info(f"   - é‹è¡Œæ™‚é–“: {running_hours:.1f} å°æ™‚")

            # æ›´æ–°ä»»å‹™ç‹€æ…‹
            task.status = TaskStatus.FAILED
            task.error_message = f"Task timeout after {running_hours:.1f} hours (auto-cleanup on {datetime.now(timezone.utc).date()})"
            task.completed_at = datetime.now(timezone.utc)

            cleaned_tasks.append({
                "id": task.id,
                "task_type": task.task_type.value,
                "user_id": task.user_id,
                "running_hours": round(running_hours, 1)
            })

        db.commit()

        logger.info(f"âœ… æˆåŠŸæ¸…ç† {len(cleaned_tasks)} å€‹å¡ä½çš„ä»»å‹™")

        return {
            "status": "success",
            "cleaned_count": len(cleaned_tasks),
            "tasks": cleaned_tasks
        }

    except Exception as e:
        logger.error(f"âŒ æ¸…ç†å¡ä½ä»»å‹™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        db.rollback()

        return {
            "status": "error",
            "message": str(e),
            "cleaned_count": 0
        }

    finally:
        db.close()


@celery_app.task(bind=True, name="app.tasks.monitor_rdagent_tasks")
def monitor_rdagent_tasks(self: Task) -> dict:
    """ç›£æ§ RD-Agent ä»»å‹™ç‹€æ…‹ä¸¦ç™¼é€å‘Šè­¦

    æª¢æŸ¥é …ç›®ï¼š
    1. é•·æ™‚é–“é‹è¡Œçš„ä»»å‹™ï¼ˆè¶…éè»Ÿè¶…æ™‚ 80%ï¼‰
    2. æœ€è¿‘å¤±æ•—çš„ä»»å‹™
    3. ç•°å¸¸é«˜é »ç‡å¤±æ•—
    4. ä»»å‹™åŸ·è¡Œæ™‚é–“ç•°å¸¸

    Returns:
        dict: ç›£æ§çµ±è¨ˆè³‡è¨Š
    """
    from datetime import datetime, timedelta, timezone
    from sqlalchemy import and_, func
    from app.models.rdagent import TaskType  # æ·»åŠ å°å…¥

    db: Session = SessionLocal()

    try:
        logger.info("ğŸ” é–‹å§‹ç›£æ§ RD-Agent ä»»å‹™ç‹€æ…‹...")

        alerts = []
        stats = {
            "running_tasks": 0,
            "long_running_tasks": 0,
            "recent_failures": 0,
            "high_failure_rate": False,
            "alerts_sent": 0,
            "errors": []
        }

        # ==================== æª¢æŸ¥ 1: é•·æ™‚é–“é‹è¡Œçš„ä»»å‹™ ====================
        # å®šç¾©å‘Šè­¦é–¾å€¼ï¼ˆé”åˆ°è»Ÿè¶…æ™‚çš„ 80%ï¼‰
        thresholds = {
            TaskType.FACTOR_MINING: timedelta(minutes=44),  # 55 åˆ†é˜ * 80% = 44 åˆ†é˜
            TaskType.MODEL_GENERATION: timedelta(minutes=22),  # 28 åˆ†é˜ * 80% = 22 åˆ†é˜
            TaskType.STRATEGY_OPTIMIZATION: timedelta(minutes=22)
        }

        running_tasks = db.query(RDAgentTask).filter(
            RDAgentTask.status == TaskStatus.RUNNING
        ).all()

        stats["running_tasks"] = len(running_tasks)

        for task in running_tasks:
            if task.started_at:
                running_time = datetime.now(timezone.utc) - task.started_at
                threshold = thresholds.get(task.task_type, timedelta(minutes=30))

                if running_time > threshold:
                    stats["long_running_tasks"] += 1
                    running_minutes = running_time.total_seconds() / 60

                    alerts.append({
                        "severity": "WARNING",
                        "type": "LONG_RUNNING_TASK",
                        "task_id": task.id,
                        "task_type": task.task_type.value,
                        "user_id": task.user_id,
                        "running_minutes": round(running_minutes, 1),
                        "threshold_minutes": threshold.total_seconds() / 60,
                        "message": (
                            f"âš ï¸ RD-Agent ä»»å‹™ #{task.id} ({task.task_type.value}) "
                            f"å·²é‹è¡Œ {running_minutes:.1f} åˆ†é˜ï¼Œè¶…éå‘Šè­¦é–¾å€¼"
                        )
                    })

                    logger.warning(
                        f"âš ï¸ Task {task.id} ({task.task_type.value}) "
                        f"running for {running_minutes:.1f} minutes"
                    )

        # ==================== æª¢æŸ¥ 2: æœ€è¿‘å¤±æ•—çš„ä»»å‹™ ====================
        recent_failures = db.query(RDAgentTask).filter(
            and_(
                RDAgentTask.status == TaskStatus.FAILED,
                RDAgentTask.completed_at >= datetime.now(timezone.utc) - timedelta(hours=1)
            )
        ).all()

        stats["recent_failures"] = len(recent_failures)

        for task in recent_failures:
            alerts.append({
                "severity": "ERROR",
                "type": "TASK_FAILED",
                "task_id": task.id,
                "task_type": task.task_type.value,
                "user_id": task.user_id,
                "error_message": task.error_message or "Unknown error",
                "message": (
                    f"âŒ RD-Agent ä»»å‹™ #{task.id} ({task.task_type.value}) å¤±æ•—\n"
                    f"éŒ¯èª¤: {task.error_message or 'Unknown error'}"
                )
            })

            logger.error(
                f"âŒ Task {task.id} ({task.task_type.value}) failed: "
                f"{task.error_message}"
            )

        # ==================== æª¢æŸ¥ 3: å¤±æ•—ç‡éé«˜ ====================
        # æª¢æŸ¥æœ€è¿‘ 24 å°æ™‚çš„ä»»å‹™å¤±æ•—ç‡
        one_day_ago = datetime.now(timezone.utc) - timedelta(hours=24)

        total_tasks_24h = db.query(func.count(RDAgentTask.id)).filter(
            RDAgentTask.created_at >= one_day_ago
        ).scalar()

        failed_tasks_24h = db.query(func.count(RDAgentTask.id)).filter(
            and_(
                RDAgentTask.status == TaskStatus.FAILED,
                RDAgentTask.created_at >= one_day_ago
            )
        ).scalar()

        if total_tasks_24h and total_tasks_24h > 0:
            failure_rate = (failed_tasks_24h / total_tasks_24h) * 100

            # å¤±æ•—ç‡è¶…é 30% è¦–ç‚ºç•°å¸¸
            if failure_rate > 30:
                stats["high_failure_rate"] = True

                alerts.append({
                    "severity": "CRITICAL",
                    "type": "HIGH_FAILURE_RATE",
                    "failure_rate": round(failure_rate, 1),
                    "total_tasks": total_tasks_24h,
                    "failed_tasks": failed_tasks_24h,
                    "message": (
                        f"ğŸš¨ RD-Agent ä»»å‹™å¤±æ•—ç‡éé«˜ï¼\n"
                        f"æœ€è¿‘ 24 å°æ™‚: {failed_tasks_24h}/{total_tasks_24h} å¤±æ•— "
                        f"({failure_rate:.1f}%)"
                    )
                })

                logger.critical(
                    f"ğŸš¨ High failure rate detected: {failure_rate:.1f}% "
                    f"({failed_tasks_24h}/{total_tasks_24h})"
                )

        # ==================== ç™¼é€å‘Šè­¦ ====================
        if alerts:
            logger.info(f"ğŸ“Š æª¢æ¸¬åˆ° {len(alerts)} å€‹å‘Šè­¦ï¼Œæº–å‚™ç™¼é€é€šçŸ¥...")

            # æŒ‰ç”¨æˆ¶åˆ†çµ„å‘Šè­¦
            alerts_by_user = {}
            for alert in alerts:
                user_id = alert.get("user_id")
                if user_id:
                    if user_id not in alerts_by_user:
                        alerts_by_user[user_id] = []
                    alerts_by_user[user_id].append(alert)

            # ç™¼é€å‘Šè­¦é€šçŸ¥
            for user_id, user_alerts in alerts_by_user.items():
                try:
                    # æ§‹å»ºå‘Šè­¦æ¶ˆæ¯
                    severity_emoji = {
                        "WARNING": "âš ï¸",
                        "ERROR": "âŒ",
                        "CRITICAL": "ğŸš¨"
                    }

                    message_lines = ["<b>ğŸ¤– RD-Agent ä»»å‹™å‘Šè­¦</b>\n"]

                    for alert in user_alerts:
                        emoji = severity_emoji.get(alert["severity"], "â„¹ï¸")
                        message_lines.append(
                            f"{emoji} <b>{alert['type']}</b>\n"
                            f"{alert['message']}\n"
                        )

                    message = "\n".join(message_lines)

                    # èª¿ç”¨ Telegram é€šçŸ¥ä»»å‹™
                    from app.tasks.telegram_notifications import send_telegram_notification

                    send_telegram_notification.delay(
                        user_id=user_id,
                        notification_type="system_alert",
                        title="ğŸ¤– RD-Agent ä»»å‹™å‘Šè­¦",
                        message=message,
                        related_object_type="rdagent_monitoring",
                        related_object_id=None
                    )

                    stats["alerts_sent"] += 1

                except Exception as e:
                    error_msg = f"Failed to send alert to user {user_id}: {str(e)}"
                    logger.error(error_msg)
                    stats["errors"].append(error_msg)

            # ç³»çµ±ç´šå‘Šè­¦ï¼ˆé«˜å¤±æ•—ç‡ï¼‰ç™¼é€çµ¦ç®¡ç†å“¡
            critical_alerts = [a for a in alerts if a["severity"] == "CRITICAL"]
            if critical_alerts:
                # TODO: æ·»åŠ ç®¡ç†å“¡é€šçŸ¥é‚è¼¯
                logger.critical(
                    f"ğŸš¨ {len(critical_alerts)} critical alerts detected, "
                    f"admin notification required"
                )

        else:
            logger.info("âœ… æ‰€æœ‰ RD-Agent ä»»å‹™ç‹€æ…‹æ­£å¸¸ï¼Œç„¡å‘Šè­¦")

        # è¨˜éŒ„ç›£æ§çµ±è¨ˆ
        logger.info(f"ğŸ“Š ç›£æ§çµ±è¨ˆ:")
        logger.info(f"   - é‹è¡Œä¸­ä»»å‹™: {stats['running_tasks']}")
        logger.info(f"   - é•·æ™‚é–“é‹è¡Œ: {stats['long_running_tasks']}")
        logger.info(f"   - æœ€è¿‘å¤±æ•—: {stats['recent_failures']}")
        logger.info(f"   - é«˜å¤±æ•—ç‡: {stats['high_failure_rate']}")
        logger.info(f"   - å‘Šè­¦å·²ç™¼é€: {stats['alerts_sent']}")

        return {
            "status": "success",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "stats": stats,
            "alerts": alerts
        }

    except Exception as e:
        logger.error(f"âŒ RD-Agent ç›£æ§ä»»å‹™å¤±æ•—: {str(e)}")

        return {
            "status": "error",
            "message": str(e),
            "stats": stats
        }

    finally:
        db.close()
