"""æ¨¡å‹è¨“ç·´ Celery ä»»å‹™"""

import sys
import os
import torch
import numpy as np
import pandas as pd
from typing import Dict, Any, List, Optional
from datetime import datetime, timezone
from celery import shared_task
from sqlalchemy.orm import Session

# Qlib imports
import qlib
from qlib.data import D
from qlib.contrib.model.pytorch_transformer import Transformer
from qlib.contrib.model.pytorch_nn import DNNModelPytorch

from app.db.session import SessionLocal
from app.repositories.model_training_job import ModelTrainingJobRepository
from app.repositories.model_factor import ModelFactorRepository
from app.repositories.generated_model import GeneratedModelRepository
from app.repositories.generated_factor import GeneratedFactorRepository
from app.utils.timezone_helpers import now_utc
from app.core.config import settings


# åˆå§‹åŒ– Qlibï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
if not hasattr(qlib, '_initialized'):
    qlib.init(provider_uri="/data/qlib/tw_stock_v2", region="cn")
    qlib._initialized = True


@shared_task(bind=True, name="app.tasks.model_training_tasks.train_model_async")
def train_model_async(
    self,
    job_id: int,
    model_id: int,
    user_id: int,
    factor_ids: List[int],
    dataset_config: Dict[str, Any],
    training_params: Dict[str, Any],
    use_alpha158: bool = False
) -> Dict[str, Any]:
    """
    ç•°æ­¥è¨“ç·´æ¨¡å‹ä»»å‹™

    Args:
        self: Celery task instance
        job_id: è¨“ç·´ä»»å‹™ ID
        model_id: æ¨¡å‹ ID
        user_id: ç”¨æˆ¶ ID
        factor_ids: å› å­ ID åˆ—è¡¨
        dataset_config: æ•¸æ“šé›†é…ç½®
        training_params: è¨“ç·´åƒæ•¸

    Returns:
        è¨“ç·´çµæœå­—å…¸
    """
    db = SessionLocal()

    try:
        # ========== æ­¥é©Ÿ 1ï¼šåˆå§‹åŒ–ä»»å‹™ ==========
        ModelTrainingJobRepository.update_status(
            db, job_id, "RUNNING"
        )
        ModelTrainingJobRepository.update_progress(
            db, job_id,
            progress=0.0,
            current_epoch=0,
            current_step="æ­£åœ¨åˆå§‹åŒ–è¨“ç·´ç’°å¢ƒ..."
        )
        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"é–‹å§‹è¨“ç·´ä»»å‹™ (Job ID: {job_id}, Model ID: {model_id})"
        )

        # ========== æ­¥é©Ÿ 2ï¼šè¼‰å…¥å› å­è³‡è¨Š ==========
        ModelTrainingJobRepository.update_progress(
            db, job_id,
            progress=0.1,
            current_epoch=0,
            current_step="æ­£åœ¨è¼‰å…¥å› å­è³‡è¨Š..."
        )

        if use_alpha158:
            # ä½¿ç”¨ Alpha158 å®Œæ•´å› å­é›†
            ModelTrainingJobRepository.append_log(
                db, job_id,
                "ä½¿ç”¨ Alpha158+ å¢å¼·å› å­é›†ï¼ˆ179 å€‹é‡åŒ–å› å­ï¼‰"
            )
            # Alpha158 å› å­å°‡åœ¨æ•¸æ“šæº–å‚™éšæ®µè¨ˆç®—
            factor_formulas = None  # æ¨™è¨˜ä½¿ç”¨ Alpha158
        else:
            # ä½¿ç”¨æ‰‹å‹•é¸æ“‡çš„å› å­
            factors = GeneratedFactorRepository.get_by_ids(db, factor_ids)
            if len(factors) != len(factor_ids):
                raise ValueError(f"éƒ¨åˆ†å› å­ä¸å­˜åœ¨ã€‚è«‹æ±‚ {len(factor_ids)} å€‹ï¼Œæ‰¾åˆ° {len(factors)} å€‹")

            # æå– Qlib è¡¨é”å¼
            factor_formulas = [f.formula for f in factors]
            ModelTrainingJobRepository.append_log(
                db, job_id,
                f"è¼‰å…¥ {len(factor_formulas)} å€‹å› å­ï¼š{', '.join([f.name for f in factors])}"
            )

        # ========== æ­¥é©Ÿ 3ï¼šæº–å‚™æ•¸æ“šé›† ==========
        ModelTrainingJobRepository.update_progress(
            db, job_id,
            progress=0.2,
            current_epoch=0,
            current_step="æ­£åœ¨æº–å‚™æ•¸æ“šé›†..."
        )

        instruments_config = dataset_config['instruments']
        start_time = dataset_config['start_time']
        end_time = dataset_config['end_time']

        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"æ•¸æ“šé›†é…ç½®ï¼šè‚¡ç¥¨æ± ={instruments_config}, æ™‚é–“ç¯„åœ={start_time} ~ {end_time}"
        )

        # Convert instruments string to list of stock IDs
        if isinstance(instruments_config, str):
            from app.models.stock import Stock
            from app.models.stock_price import StockPrice
            from sqlalchemy import func, desc
            from datetime import datetime, timedelta

            # Parse stock pool configuration
            config_lower = instruments_config.lower().replace('ï¼ˆ', '(').replace('ï¼‰', ')')

            # Determine the number of stocks needed
            if 'å…¨å¸‚å ´' in instruments_config or 'all' in config_lower:
                stock_limit = None  # No limit for all market
            elif 'å°è‚¡30' in instruments_config:
                stock_limit = 30
            elif 'å°è‚¡50' in instruments_config:
                stock_limit = 50
            elif 'å°è‚¡100' in instruments_config:
                stock_limit = 100
            elif 'å°è‚¡150' in instruments_config:
                stock_limit = 150
            elif 'å°è‚¡200' in instruments_config:
                stock_limit = 200
            else:
                stock_limit = 50  # Default to 50 stocks

            # Get stocks sorted by average trading volume (last 30 days)
            # Only include individual stocks (4-digit codes starting with 1-9)
            cutoff_date = datetime.now().date() - timedelta(days=30)

            query = (
                db.query(
                    Stock.stock_id,
                    func.avg(StockPrice.volume).label('avg_volume')
                )
                .join(StockPrice, Stock.stock_id == StockPrice.stock_id)
                .filter(
                    Stock.is_active == 'active',
                    Stock.stock_id.op('~')('^[1-9][0-9]{3}$'),  # Only 4-digit stock codes
                    StockPrice.date >= cutoff_date
                )
                .group_by(Stock.stock_id)
                .order_by(desc('avg_volume'))
            )

            if stock_limit is not None:
                query = query.limit(stock_limit)

            stock_results = query.all()
            instruments = [row.stock_id for row in stock_results]
            ModelTrainingJobRepository.append_log(
                db, job_id,
                f"å·²é¸æ“‡ {len(instruments)} æ”¯è‚¡ç¥¨é€²è¡Œè¨“ç·´ï¼ˆè‚¡ç¥¨æ± ï¼š{instruments_config}ï¼‰"
            )
        elif isinstance(instruments_config, list):
            instruments = instruments_config
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„ instruments æ ¼å¼: {type(instruments_config)}")

        # è¼‰å…¥æ•¸æ“š
        label_formula = "Ref($close, -1) / $close - 1"  # ä¸‹ä¸€å¤©æ”¶ç›Šç‡

        if use_alpha158:
            # ä½¿ç”¨ Alpha158ï¼šå…ˆè¼‰å…¥åŸå§‹ OHLCV æ•¸æ“šï¼Œå†è¨ˆç®—å› å­
            from app.services.alpha158_factors import alpha158_calculator

            ModelTrainingJobRepository.append_log(
                db, job_id,
                "æ­£åœ¨è¼‰å…¥åŸå§‹ OHLCV æ•¸æ“šä»¥è¨ˆç®— Alpha158+ å› å­..."
            )

            # è¼‰å…¥åŸå§‹ OHLCV æ•¸æ“š
            raw_fields = ['$open', '$high', '$low', '$close', '$volume', label_formula]
            df_raw = D.features(
                instruments=instruments,
                fields=raw_fields,
                start_time=start_time,
                end_time=end_time,
                freq='day'
            )

            if df_raw is None or df_raw.empty:
                raise ValueError(f"ç„¡æ³•è¼‰å…¥åŸå§‹æ•¸æ“šã€‚è«‹æª¢æŸ¥ Qlib æ•¸æ“šæ˜¯å¦å­˜åœ¨æ–¼ {start_time} ~ {end_time}")

            ModelTrainingJobRepository.append_log(
                db, job_id,
                f"æ­£åœ¨è¨ˆç®— Alpha158+ å› å­ï¼ˆé€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜ï¼‰..."
            )

            # ç‚ºæ¯æ”¯è‚¡ç¥¨åˆ†åˆ¥è¨ˆç®— Alpha158 å› å­
            all_stock_dfs = []
            stock_symbols = df_raw.index.get_level_values(0).unique()

            for i, symbol in enumerate(stock_symbols):
                if (i + 1) % 10 == 0:  # æ¯10æ”¯è‚¡ç¥¨æ›´æ–°ä¸€æ¬¡é€²åº¦
                    ModelTrainingJobRepository.append_log(
                        db, job_id,
                        f"è¨ˆç®—é€²åº¦: {i+1}/{len(stock_symbols)} æ”¯è‚¡ç¥¨"
                    )

                # ç²å–å–®æ”¯è‚¡ç¥¨æ•¸æ“š
                stock_data = df_raw.xs(symbol, level=0)

                # è¨ˆç®— Alpha158 å› å­
                alpha158_df, _ = alpha158_calculator.compute_all_factors(stock_data)

                # æ·»åŠ æ¨™ç±¤åˆ—
                alpha158_df['label'] = stock_data[label_formula]

                # æ·»åŠ è‚¡ç¥¨ä»£ç¢¼ä½œç‚º index
                alpha158_df['instrument'] = symbol
                alpha158_df.set_index('instrument', append=True, inplace=True)
                alpha158_df = alpha158_df.swaplevel()

                all_stock_dfs.append(alpha158_df)

            # åˆä½µæ‰€æœ‰è‚¡ç¥¨çš„æ•¸æ“š
            df = pd.concat(all_stock_dfs)

            # åªä¿ç•™å› å­åˆ—å’Œæ¨™ç±¤åˆ—ï¼ˆæ’é™¤åŸå§‹ OHLCVï¼‰
            factor_columns = [col for col in df.columns if col not in raw_fields]
            df = df[factor_columns]

            ModelTrainingJobRepository.append_log(
                db, job_id,
                f"âœ… æˆåŠŸè¨ˆç®— Alpha158+ å› å­ï¼š{len(factor_columns) - 1} å€‹ç‰¹å¾µ"
            )
        else:
            # ä½¿ç”¨æ‰‹å‹•é¸æ“‡çš„å› å­
            all_fields = factor_formulas + [label_formula]

            df = D.features(
                instruments=instruments,
                fields=all_fields,
                start_time=start_time,
                end_time=end_time,
                freq='day'
            )

            if df is None or df.empty:
                raise ValueError(f"ç„¡æ³•è¼‰å…¥æ•¸æ“šã€‚è«‹æª¢æŸ¥ Qlib æ•¸æ“šæ˜¯å¦å­˜åœ¨æ–¼ {start_time} ~ {end_time}")

        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"æˆåŠŸè¼‰å…¥ {len(df)} ç­†æ•¸æ“šï¼ˆ{len(df.index.get_level_values(0).unique())} æ”¯è‚¡ç¥¨ï¼‰"
        )

        # ========== æ­¥é©Ÿ 4ï¼šæ•¸æ“šé è™•ç† ==========
        ModelTrainingJobRepository.update_progress(
            db, job_id,
            progress=0.3,
            current_epoch=0,
            current_step="æ­£åœ¨é è™•ç†æ•¸æ“š..."
        )

        # å¡«è£œç¼ºå¤±å€¼
        df = df.fillna(method='ffill').fillna(0)

        # åˆ†å‰²ç‰¹å¾µå’Œæ¨™ç±¤
        X = df.iloc[:, :-1].values  # å‰ N åˆ—ç‚ºç‰¹å¾µ
        y = df.iloc[:, -1].values   # æœ€å¾Œä¸€åˆ—ç‚ºæ¨™ç±¤

        # Clean infinite values BEFORE computing statistics
        inf_count = np.isinf(X).sum()
        nan_count = np.isnan(X).sum()

        if inf_count > 0 or nan_count > 0:
            ModelTrainingJobRepository.append_log(
                db, job_id,
                f"âš ï¸ æ¸…ç†ç•°å¸¸å€¼: {inf_count} å€‹ Inf, {nan_count} å€‹ NaN"
            )

            # Use percentile-based clipping for more robust outlier handling
            # Compute 99th percentile of non-infinite values
            valid_X = X[~np.isinf(X)]
            if len(valid_X) > 0:
                p99 = np.percentile(valid_X, 99)
                p1 = np.percentile(valid_X, 1)

                ModelTrainingJobRepository.append_log(
                    db, job_id,
                    f"ä½¿ç”¨ç™¾åˆ†ä½æ•¸è£å‰ª: P1={p1:.2f}, P99={p99:.2f}"
                )

                # Clip to percentile range
                X = np.clip(X, p1, p99)
            else:
                # Fallback: use median
                X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)

            # Clean labels
            y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)

        # Log data statistics before normalization
        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"æ¸…ç†å¾Œæ•¸æ“šç¯„åœ: X mean={np.mean(X):.6f}, std={np.std(X):.6f}, "
            f"min={np.min(X):.6f}, max={np.max(X):.6f}"
        )

        # åˆ†å‰²è¨“ç·´/é©—è­‰/æ¸¬è©¦é›†
        train_ratio = dataset_config.get('train_ratio', 0.7)
        valid_ratio = dataset_config.get('valid_ratio', 0.15)

        n_samples = len(X)
        train_end = int(n_samples * train_ratio)
        valid_end = int(n_samples * (train_ratio + valid_ratio))

        X_train, y_train = X[:train_end], y[:train_end]
        X_valid, y_valid = X[train_end:valid_end], y[train_end:valid_end]
        X_test, y_test = X[valid_end:], y[valid_end:]

        # Standardize features using RobustScaler (more robust to outliers)
        # RobustScaler uses median and IQR, better for financial data with extreme values
        from sklearn.preprocessing import RobustScaler

        ModelTrainingJobRepository.append_log(
            db, job_id,
            "ä½¿ç”¨ RobustScaler æ¨™æº–åŒ–ï¼ˆåŸºæ–¼ä¸­ä½æ•¸å’Œ IQRï¼Œå°ç•°å¸¸å€¼ç©©å¥ï¼‰"
        )

        scaler = RobustScaler()
        X_train = scaler.fit_transform(X_train)
        X_valid = scaler.transform(X_valid)
        X_test = scaler.transform(X_test)

        # Safety check: replace any remaining NaN/Inf (rare with RobustScaler)
        if np.any(np.isnan(X_train)) or np.any(np.isinf(X_train)):
            ModelTrainingJobRepository.append_log(
                db, job_id,
                "âš ï¸ ç™¼ç¾æ¥µå°‘æ•¸ NaN/Infï¼Œå·²æ›¿æ›ç‚º 0"
            )
            X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)
            X_valid = np.nan_to_num(X_valid, nan=0.0, posinf=0.0, neginf=0.0)
            X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)

        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"è¨“ç·´é›†: {len(X_train)} ç­†, é©—è­‰é›†: {len(X_valid)} ç­†, æ¸¬è©¦é›†: {len(X_test)} ç­†"
        )

        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"æ¨™æº–åŒ–å¾Œ: X_train mean={np.mean(X_train):.6f}, std={np.std(X_train):.6f}"
        )

        # ========== æ­¥é©Ÿ 5ï¼šå»ºç«‹æ¨¡å‹ ==========
        ModelTrainingJobRepository.update_progress(
            db, job_id,
            progress=0.4,
            current_epoch=0,
            current_step="æ­£åœ¨å»ºç«‹æ¨¡å‹..."
        )

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"ä½¿ç”¨è¨­å‚™: {device}"
        )

        # ç²å–æ¨¡å‹è³‡è¨Š
        model_info = GeneratedModelRepository.get_by_id(db, model_id)
        if not model_info:
            raise ValueError(f"æ¨¡å‹ ID {model_id} ä¸å­˜åœ¨")

        # æ ¹æ“šæ¨¡å‹é¡å‹å»ºç«‹æ¨¡å‹ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨ PyTorch MLPï¼‰
        if use_alpha158:
            # Alpha158 çš„ç‰¹å¾µæ•¸é‡ = df çš„åˆ—æ•¸ - 1ï¼ˆæ¨™ç±¤åˆ—ï¼‰
            d_feat = len(df.columns) - 1
            ModelTrainingJobRepository.append_log(
                db, job_id,
                f"Alpha158+ ç‰¹å¾µæ•¸é‡: {d_feat}"
            )
        else:
            d_feat = len(factor_formulas)

        # Create simple PyTorch MLP model
        class SimpleMLP(torch.nn.Module):
            def __init__(self, input_dim, hidden_dims=(128, 64), dropout=0.1):
                super().__init__()
                layers = []
                prev_dim = input_dim

                for hidden_dim in hidden_dims:
                    layers.extend([
                        torch.nn.Linear(prev_dim, hidden_dim),
                        torch.nn.ReLU(),
                        torch.nn.Dropout(dropout)
                    ])
                    prev_dim = hidden_dim

                layers.append(torch.nn.Linear(prev_dim, 1))  # Output layer
                self.model = torch.nn.Sequential(*layers)

            def forward(self, x):
                return self.model(x)

        model = SimpleMLP(input_dim=d_feat).to(device)

        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"æ¨¡å‹æ¶æ§‹: MLP (è¼¸å…¥ç¶­åº¦={d_feat}, éš±è—å±¤=[128, 64])"
        )

        # ========== æ­¥é©Ÿ 6ï¼šè¨“ç·´æ¨¡å‹ ==========
        num_epochs = training_params.get('num_epochs', 100)
        batch_size = training_params.get('batch_size', 800)
        learning_rate = training_params.get('learning_rate', 0.001)
        early_stop_rounds = training_params.get('early_stop_rounds', 20)

        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        criterion = torch.nn.MSELoss()

        # è½‰æ›ç‚º Tensor
        X_train_tensor = torch.FloatTensor(X_train).to(device)
        y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device)
        X_valid_tensor = torch.FloatTensor(X_valid).to(device)
        y_valid_tensor = torch.FloatTensor(y_valid).unsqueeze(1).to(device)

        best_valid_loss = float('inf')
        patience_counter = 0

        # Initialize model weight path
        model_weight_dir = "/app/models/trained"
        os.makedirs(model_weight_dir, exist_ok=True)
        model_weight_path = os.path.join(
            model_weight_dir,
            f"model_{model_id}_job_{job_id}_best.pth"
        )

        # Check training data quality before starting
        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"è¨“ç·´æ•¸æ“šæª¢æŸ¥: X shape={X_train.shape}, y shape={y_train.shape}, "
            f"X NaN={np.isnan(X_train).sum()}, y NaN={np.isnan(y_train).sum()}"
        )

        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"é–‹å§‹è¨“ç·´ï¼š{num_epochs} è¼ª, æ‰¹æ¬¡å¤§å°={batch_size}, å­¸ç¿’ç‡={learning_rate}"
        )

        for epoch in range(1, num_epochs + 1):
            # è¨“ç·´æ¨¡å¼
            model.train()
            train_losses = []

            # Mini-batch è¨“ç·´
            for i in range(0, len(X_train_tensor), batch_size):
                batch_X = X_train_tensor[i:i+batch_size]
                batch_y = y_train_tensor[i:i+batch_size]

                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)

                # Check for NaN loss
                if torch.isnan(loss):
                    ModelTrainingJobRepository.append_log(
                        db, job_id,
                        f"âš ï¸ Epoch {epoch} Batch {i//batch_size}: Loss is NaN! "
                        f"Output stats: mean={outputs.mean():.6f}, std={outputs.std():.6f}"
                    )
                    # Skip this batch if loss is NaN
                    continue

                loss.backward()

                # Gradient clipping to prevent exploding gradients
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                optimizer.step()

                train_losses.append(loss.item())

            if len(train_losses) == 0:
                ModelTrainingJobRepository.append_log(
                    db, job_id,
                    f"âš ï¸ Epoch {epoch}: æ‰€æœ‰ batch çš„ loss éƒ½æ˜¯ NaNï¼Œè¨“ç·´å¤±æ•—"
                )
                train_loss = float('nan')
            else:
                train_loss = np.mean(train_losses)

            # é©—è­‰æ¨¡å¼
            model.eval()
            with torch.no_grad():
                valid_outputs = model(X_valid_tensor)
                valid_loss = criterion(valid_outputs, y_valid_tensor).item()

            # æ›´æ–°é€²åº¦ï¼ˆæ¯è¼ªï¼‰
            progress = 0.4 + 0.5 * (epoch / num_epochs)  # 0.4 åˆ° 0.9

            ModelTrainingJobRepository.update_progress(
                db, job_id,
                progress=progress,
                current_epoch=epoch,
                current_step=f"è¨“ç·´ä¸­ Epoch {epoch}/{num_epochs}",
                train_loss=train_loss if not np.isnan(train_loss) else None,
                valid_loss=valid_loss if not np.isnan(valid_loss) else None
            )

            # Log first, last epoch, or every 10 epochs
            if epoch == 1 or epoch == num_epochs or epoch % 10 == 0:
                ModelTrainingJobRepository.append_log(
                    db, job_id,
                    f"Epoch {epoch}/{num_epochs}: train_loss={train_loss:.6f}, valid_loss={valid_loss:.6f}"
                )

            # Early Stopping
            if valid_loss < best_valid_loss:
                best_valid_loss = valid_loss
                patience_counter = 0

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(model.state_dict(), model_weight_path)
            else:
                patience_counter += 1
                if patience_counter >= early_stop_rounds:
                    ModelTrainingJobRepository.append_log(
                        db, job_id,
                        f"Early stopping triggered at epoch {epoch} (æœ€ä½³é©—è­‰æå¤±: {best_valid_loss:.6f})"
                    )
                    break

        # Save final model if it wasn't saved during training (safety fallback)
        if not os.path.exists(model_weight_path):
            torch.save(model.state_dict(), model_weight_path)
            ModelTrainingJobRepository.append_log(
                db, job_id,
                "ä¿å­˜æœ€çµ‚æ¨¡å‹æ¬Šé‡ï¼ˆæœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹ï¼‰"
            )

        # ========== æ­¥é©Ÿ 7ï¼šæ¸¬è©¦æ¨¡å‹ ==========
        ModelTrainingJobRepository.update_progress(
            db, job_id,
            progress=0.95,
            current_epoch=epoch,
            current_step="æ­£åœ¨æ¸¬è©¦æ¨¡å‹..."
        )

        # è¼‰å…¥æœ€ä½³æ¨¡å‹
        model.load_state_dict(torch.load(model_weight_path))
        model.eval()

        X_test_tensor = torch.FloatTensor(X_test).to(device)
        y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1).to(device)

        with torch.no_grad():
            test_outputs = model(X_test_tensor)
            test_predictions = test_outputs.cpu().numpy().flatten()
            test_actuals = y_test

            # Debug logging
            ModelTrainingJobRepository.append_log(
                db, job_id,
                f"é æ¸¬å€¼çµ±è¨ˆ: mean={np.mean(test_predictions):.6f}, std={np.std(test_predictions):.6f}, "
                f"min={np.min(test_predictions):.6f}, max={np.max(test_predictions):.6f}, "
                f"NaN count={np.isnan(test_predictions).sum()}"
            )

            # Check for NaN or Inf in predictions
            if np.any(np.isnan(test_predictions)) or np.any(np.isinf(test_predictions)):
                ModelTrainingJobRepository.append_log(
                    db, job_id,
                    "âš ï¸ è­¦å‘Š: é æ¸¬å€¼åŒ…å« NaN æˆ– Infï¼Œå°‡æ›¿æ›ç‚º 0"
                )
                test_predictions = np.nan_to_num(test_predictions, nan=0.0, posinf=0.0, neginf=0.0)

            # Calculate IC (Information Coefficient)
            # Handle case where std is 0 or correlation can't be computed
            if np.std(test_predictions) < 1e-10 or np.std(test_actuals) < 1e-10:
                test_ic = 0.0
                ModelTrainingJobRepository.append_log(
                    db, job_id,
                    "âš ï¸ é æ¸¬å€¼æˆ–å¯¦éš›å€¼æ¨™æº–å·®æ¥è¿‘ 0ï¼ŒIC è¨­ç‚º 0"
                )
            else:
                try:
                    correlation_matrix = np.corrcoef(test_predictions, test_actuals)
                    test_ic = correlation_matrix[0, 1]
                    if np.isnan(test_ic):
                        test_ic = 0.0
                except:
                    test_ic = 0.0

            # Calculate other metrics
            mse = np.mean((test_predictions - test_actuals) ** 2)
            mae = np.mean(np.abs(test_predictions - test_actuals))
            pred_mean = np.mean(test_predictions)
            pred_std = np.std(test_predictions)

            # Convert NaN to None for JSON compatibility
            def safe_float(value):
                if np.isnan(value) or np.isinf(value):
                    return None
                return float(value)

            test_metrics = {
                'ic': safe_float(test_ic),
                'mse': safe_float(mse),
                'mae': safe_float(mae),
                'predictions_mean': safe_float(pred_mean),
                'predictions_std': safe_float(pred_std)
            }

            # Also convert test_ic for database save
            test_ic_safe = safe_float(test_ic)

        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"æ¸¬è©¦çµæœ: IC={test_ic_safe if test_ic_safe is not None else 'N/A'}, "
            f"MSE={test_metrics['mse'] if test_metrics['mse'] is not None else 'N/A'}"
        )

        # ========== æ­¥é©Ÿ 8ï¼šæ›´æ–°æ¨¡å‹é…ç½® ==========
        # ç”Ÿæˆæ­£ç¢ºçš„ Qlib é…ç½®ï¼ˆæ ¹æ“šå¯¦éš›ä½¿ç”¨çš„å› å­ï¼‰
        import json

        if use_alpha158:
            # ä½¿ç”¨ Alpha158 Handler
            data_config = {
                "handler": {
                    "class": "Alpha158",
                    "module_path": "qlib.contrib.data.handler",
                    "kwargs": {
                        "start_time": start_time,
                        "end_time": end_time,
                        "instruments": instruments_config,
                        "label": ["Ref($close, -1) / $close - 1"]
                    }
                }
            }
            ModelTrainingJobRepository.append_log(
                db, job_id,
                "æ›´æ–°æ¨¡å‹é…ç½®ï¼šä½¿ç”¨ Alpha158+ Handler"
            )
        else:
            # ä½¿ç”¨è‡ªå®šç¾©å› å­
            data_config = {
                "handler": {
                    "class": "CustomFactorHandler",
                    "factors": factor_formulas,
                    "kwargs": {
                        "start_time": start_time,
                        "end_time": end_time,
                        "instruments": instruments_config,
                        "label": ["Ref($close, -1) / $close - 1"]
                    }
                }
            }
            ModelTrainingJobRepository.append_log(
                db, job_id,
                f"æ›´æ–°æ¨¡å‹é…ç½®ï¼šä½¿ç”¨ {len(factor_formulas)} å€‹è‡ªå®šç¾©å› å­"
            )

        # æ›´æ–°æ¨¡å‹çš„ qlib_config
        current_config = model_info.qlib_config or {}
        if isinstance(current_config, str):
            current_config = json.loads(current_config)

        current_config['data'] = data_config
        current_config['model']['kwargs']['d_feat'] = d_feat
        current_config['training'] = {
            'num_epochs': training_params['num_epochs'],
            'batch_size': training_params['batch_size'],
            'learning_rate': training_params['learning_rate'],
            'optimizer': training_params['optimizer'],
            'loss_function': training_params['loss_function']
        }

        # ä¿å­˜é…ç½®
        model_info.qlib_config = current_config
        db.commit()

        # ========== æ­¥é©Ÿ 9ï¼šå®Œæˆè¨“ç·´ ==========
        ModelTrainingJobRepository.update_completed(
            db, job_id,
            model_weight_path=model_weight_path,
            test_ic=test_ic_safe,
            test_metrics=test_metrics
        )

        ModelTrainingJobRepository.append_log(
            db, job_id,
            "âœ… è¨“ç·´å®Œæˆï¼æ¨¡å‹æ¬Šé‡å·²ä¿å­˜ã€‚"
        )

        return {
            'status': 'success',
            'job_id': job_id,
            'model_weight_path': model_weight_path,
            'test_ic': test_ic_safe,
            'test_metrics': test_metrics
        }

    except Exception as e:
        # Rollback current transaction to allow new queries
        db.rollback()

        # è¨˜éŒ„éŒ¯èª¤
        error_msg = f"è¨“ç·´å¤±æ•—: {str(e)}"
        try:
            ModelTrainingJobRepository.update_status(
                db, job_id,
                status="FAILED",
                error_message=error_msg
            )
            ModelTrainingJobRepository.append_log(
                db, job_id,
                f"âŒ {error_msg}"
            )
        except Exception as update_error:
            # If we can't update the database, at least log it
            print(f"Failed to update job status: {update_error}")

        raise e

    finally:
        db.close()


@shared_task(name="app.tasks.model_training_tasks.cancel_training_job")
def cancel_training_job(job_id: int) -> Dict[str, Any]:
    """
    å–æ¶ˆè¨“ç·´ä»»å‹™

    Args:
        job_id: è¨“ç·´ä»»å‹™ ID

    Returns:
        å–æ¶ˆçµæœ
    """
    db = SessionLocal()

    try:
        job = ModelTrainingJobRepository.get_by_id(db, job_id)
        if not job:
            return {'status': 'error', 'message': f'ä»»å‹™ {job_id} ä¸å­˜åœ¨'}

        if job.status not in ['PENDING', 'RUNNING']:
            return {
                'status': 'error',
                'message': f'ä»»å‹™ç‹€æ…‹ç‚º {job.status}ï¼Œç„¡æ³•å–æ¶ˆ'
            }

        # æ›´æ–°ç‹€æ…‹ç‚º CANCELLED
        ModelTrainingJobRepository.update_status(
            db, job_id,
            status="CANCELLED"
        )

        ModelTrainingJobRepository.append_log(
            db, job_id,
            "ğŸš« è¨“ç·´ä»»å‹™å·²è¢«ç”¨æˆ¶å–æ¶ˆ"
        )

        return {
            'status': 'success',
            'job_id': job_id,
            'message': 'è¨“ç·´ä»»å‹™å·²å–æ¶ˆ'
        }

    finally:
        db.close()
