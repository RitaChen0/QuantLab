"""æ¨¡å‹è¨“ç·´ Celery ä»»å‹™"""

import sys
import os
import torch
import numpy as np
import pandas as pd
from typing import Dict, Any, List, Optional
from datetime import datetime, timezone
from celery import shared_task
from sqlalchemy.orm import Session

# Qlib imports
import qlib
from qlib.data import D
from qlib.contrib.model.pytorch_transformer import Transformer
from qlib.contrib.model.pytorch_nn import DNNModelPytorch

from app.db.session import SessionLocal
from app.repositories.model_training_job import ModelTrainingJobRepository
from app.repositories.model_factor import ModelFactorRepository
from app.repositories.generated_model import GeneratedModelRepository
from app.repositories.generated_factor import GeneratedFactorRepository
from app.utils.timezone_helpers import now_utc
from app.core.config import settings


# åˆå§‹åŒ– Qlibï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
if not hasattr(qlib, '_initialized'):
    qlib.init(provider_uri="/data/qlib/tw_stock_v2", region="cn")
    qlib._initialized = True


@shared_task(bind=True, name="app.tasks.model_training_tasks.train_model_async")
def train_model_async(
    self,
    job_id: int,
    model_id: int,
    user_id: int,
    factor_ids: List[int],
    dataset_config: Dict[str, Any],
    training_params: Dict[str, Any]
) -> Dict[str, Any]:
    """
    ç•°æ­¥è¨“ç·´æ¨¡å‹ä»»å‹™

    Args:
        self: Celery task instance
        job_id: è¨“ç·´ä»»å‹™ ID
        model_id: æ¨¡å‹ ID
        user_id: ç”¨æˆ¶ ID
        factor_ids: å› å­ ID åˆ—è¡¨
        dataset_config: æ•¸æ“šé›†é…ç½®
        training_params: è¨“ç·´åƒæ•¸

    Returns:
        è¨“ç·´çµæœå­—å…¸
    """
    db = SessionLocal()

    try:
        # ========== æ­¥é©Ÿ 1ï¼šåˆå§‹åŒ–ä»»å‹™ ==========
        ModelTrainingJobRepository.update_status(
            db, job_id, "RUNNING"
        )
        ModelTrainingJobRepository.update_progress(
            db, job_id,
            progress=0.0,
            current_epoch=0,
            current_step="æ­£åœ¨åˆå§‹åŒ–è¨“ç·´ç’°å¢ƒ..."
        )
        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"é–‹å§‹è¨“ç·´ä»»å‹™ (Job ID: {job_id}, Model ID: {model_id})"
        )

        # ========== æ­¥é©Ÿ 2ï¼šè¼‰å…¥å› å­è³‡è¨Š ==========
        ModelTrainingJobRepository.update_progress(
            db, job_id,
            progress=0.1,
            current_epoch=0,
            current_step="æ­£åœ¨è¼‰å…¥å› å­è³‡è¨Š..."
        )

        factors = GeneratedFactorRepository.get_by_ids(db, factor_ids)
        if len(factors) != len(factor_ids):
            raise ValueError(f"éƒ¨åˆ†å› å­ä¸å­˜åœ¨ã€‚è«‹æ±‚ {len(factor_ids)} å€‹ï¼Œæ‰¾åˆ° {len(factors)} å€‹")

        # æå– Qlib è¡¨é”å¼
        factor_formulas = [f.formula for f in factors]
        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"è¼‰å…¥ {len(factor_formulas)} å€‹å› å­ï¼š{', '.join([f.name for f in factors])}"
        )

        # ========== æ­¥é©Ÿ 3ï¼šæº–å‚™æ•¸æ“šé›† ==========
        ModelTrainingJobRepository.update_progress(
            db, job_id,
            progress=0.2,
            current_epoch=0,
            current_step="æ­£åœ¨æº–å‚™æ•¸æ“šé›†..."
        )

        instruments_config = dataset_config['instruments']
        start_time = dataset_config['start_time']
        end_time = dataset_config['end_time']

        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"æ•¸æ“šé›†é…ç½®ï¼šè‚¡ç¥¨æ± ={instruments_config}, æ™‚é–“ç¯„åœ={start_time} ~ {end_time}"
        )

        # Convert instruments string to list of stock IDs
        if isinstance(instruments_config, str):
            from app.models.stock import Stock

            if instruments_config.lower() in ['all', 'å…¨éƒ¨', 'æ‰€æœ‰']:
                # Get all active stocks
                stocks = db.query(Stock).filter(Stock.is_active == 'active').all()
            else:
                # For specific pools like "å°è‚¡50", we'll use first 50 active stocks
                # TODO: Implement proper stock pool mapping (e.g., Taiwan 50 index)
                stocks = db.query(Stock).filter(Stock.is_active == 'active').limit(50).all()

            instruments = [stock.stock_id for stock in stocks]
            ModelTrainingJobRepository.append_log(
                db, job_id,
                f"å·²é¸æ“‡ {len(instruments)} æ”¯è‚¡ç¥¨é€²è¡Œè¨“ç·´"
            )
        elif isinstance(instruments_config, list):
            instruments = instruments_config
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„ instruments æ ¼å¼: {type(instruments_config)}")

        # è¼‰å…¥æ•¸æ“š
        label_formula = "Ref($close, -1) / $close - 1"  # ä¸‹ä¸€å¤©æ”¶ç›Šç‡
        all_fields = factor_formulas + [label_formula]

        df = D.features(
            instruments=instruments,
            fields=all_fields,
            start_time=start_time,
            end_time=end_time,
            freq='day'
        )

        if df is None or df.empty:
            raise ValueError(f"ç„¡æ³•è¼‰å…¥æ•¸æ“šã€‚è«‹æª¢æŸ¥ Qlib æ•¸æ“šæ˜¯å¦å­˜åœ¨æ–¼ {start_time} ~ {end_time}")

        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"æˆåŠŸè¼‰å…¥ {len(df)} ç­†æ•¸æ“šï¼ˆ{len(df.index.get_level_values(0).unique())} æ”¯è‚¡ç¥¨ï¼‰"
        )

        # ========== æ­¥é©Ÿ 4ï¼šæ•¸æ“šé è™•ç† ==========
        ModelTrainingJobRepository.update_progress(
            db, job_id,
            progress=0.3,
            current_epoch=0,
            current_step="æ­£åœ¨é è™•ç†æ•¸æ“š..."
        )

        # å¡«è£œç¼ºå¤±å€¼
        df = df.fillna(method='ffill').fillna(0)

        # åˆ†å‰²ç‰¹å¾µå’Œæ¨™ç±¤
        X = df.iloc[:, :-1].values  # å‰ N åˆ—ç‚ºç‰¹å¾µ
        y = df.iloc[:, -1].values   # æœ€å¾Œä¸€åˆ—ç‚ºæ¨™ç±¤

        # Clean infinite values BEFORE computing statistics
        inf_count = np.isinf(X).sum()
        nan_count = np.isnan(X).sum()

        if inf_count > 0 or nan_count > 0:
            ModelTrainingJobRepository.append_log(
                db, job_id,
                f"âš ï¸ æ¸…ç†ç•°å¸¸å€¼: {inf_count} å€‹ Inf, {nan_count} å€‹ NaN"
            )

            # Use percentile-based clipping for more robust outlier handling
            # Compute 99th percentile of non-infinite values
            valid_X = X[~np.isinf(X)]
            if len(valid_X) > 0:
                p99 = np.percentile(valid_X, 99)
                p1 = np.percentile(valid_X, 1)

                ModelTrainingJobRepository.append_log(
                    db, job_id,
                    f"ä½¿ç”¨ç™¾åˆ†ä½æ•¸è£å‰ª: P1={p1:.2f}, P99={p99:.2f}"
                )

                # Clip to percentile range
                X = np.clip(X, p1, p99)
            else:
                # Fallback: use median
                X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)

            # Clean labels
            y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)

        # Log data statistics before normalization
        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"æ¸…ç†å¾Œæ•¸æ“šç¯„åœ: X mean={np.mean(X):.6f}, std={np.std(X):.6f}, "
            f"min={np.min(X):.6f}, max={np.max(X):.6f}"
        )

        # åˆ†å‰²è¨“ç·´/é©—è­‰/æ¸¬è©¦é›†
        train_ratio = dataset_config.get('train_ratio', 0.7)
        valid_ratio = dataset_config.get('valid_ratio', 0.15)

        n_samples = len(X)
        train_end = int(n_samples * train_ratio)
        valid_end = int(n_samples * (train_ratio + valid_ratio))

        X_train, y_train = X[:train_end], y[:train_end]
        X_valid, y_valid = X[train_end:valid_end], y[train_end:valid_end]
        X_test, y_test = X[valid_end:], y[valid_end:]

        # Standardize features (fit on training set only)
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_valid = scaler.transform(X_valid)
        X_test = scaler.transform(X_test)

        # Check for NaN or Inf after scaling
        if np.any(np.isnan(X_train)) or np.any(np.isinf(X_train)):
            ModelTrainingJobRepository.append_log(
                db, job_id,
                "âš ï¸ æ¨™æº–åŒ–å¾Œç™¼ç¾ NaN/Infï¼Œä½¿ç”¨ç©©å¥æ¨™æº–åŒ–"
            )
            # Use robust scaling if standard scaling fails
            from sklearn.preprocessing import RobustScaler
            scaler = RobustScaler()
            X_train = scaler.fit_transform(X[:train_end])
            X_valid = scaler.transform(X[train_end:valid_end])
            X_test = scaler.transform(X[valid_end:])
            # Replace any remaining NaN with 0
            X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)
            X_valid = np.nan_to_num(X_valid, nan=0.0, posinf=0.0, neginf=0.0)
            X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)

        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"è¨“ç·´é›†: {len(X_train)} ç­†, é©—è­‰é›†: {len(X_valid)} ç­†, æ¸¬è©¦é›†: {len(X_test)} ç­†"
        )

        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"æ¨™æº–åŒ–å¾Œ: X_train mean={np.mean(X_train):.6f}, std={np.std(X_train):.6f}"
        )

        # ========== æ­¥é©Ÿ 5ï¼šå»ºç«‹æ¨¡å‹ ==========
        ModelTrainingJobRepository.update_progress(
            db, job_id,
            progress=0.4,
            current_epoch=0,
            current_step="æ­£åœ¨å»ºç«‹æ¨¡å‹..."
        )

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"ä½¿ç”¨è¨­å‚™: {device}"
        )

        # ç²å–æ¨¡å‹è³‡è¨Š
        model_info = GeneratedModelRepository.get_by_id(db, model_id)
        if not model_info:
            raise ValueError(f"æ¨¡å‹ ID {model_id} ä¸å­˜åœ¨")

        # æ ¹æ“šæ¨¡å‹é¡å‹å»ºç«‹æ¨¡å‹ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨ PyTorch MLPï¼‰
        d_feat = len(factor_formulas)

        # Create simple PyTorch MLP model
        class SimpleMLP(torch.nn.Module):
            def __init__(self, input_dim, hidden_dims=(128, 64), dropout=0.1):
                super().__init__()
                layers = []
                prev_dim = input_dim

                for hidden_dim in hidden_dims:
                    layers.extend([
                        torch.nn.Linear(prev_dim, hidden_dim),
                        torch.nn.ReLU(),
                        torch.nn.Dropout(dropout)
                    ])
                    prev_dim = hidden_dim

                layers.append(torch.nn.Linear(prev_dim, 1))  # Output layer
                self.model = torch.nn.Sequential(*layers)

            def forward(self, x):
                return self.model(x)

        model = SimpleMLP(input_dim=d_feat).to(device)

        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"æ¨¡å‹æ¶æ§‹: MLP (è¼¸å…¥ç¶­åº¦={d_feat}, éš±è—å±¤=[128, 64])"
        )

        # ========== æ­¥é©Ÿ 6ï¼šè¨“ç·´æ¨¡å‹ ==========
        num_epochs = training_params.get('num_epochs', 100)
        batch_size = training_params.get('batch_size', 800)
        learning_rate = training_params.get('learning_rate', 0.001)
        early_stop_rounds = training_params.get('early_stop_rounds', 20)

        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        criterion = torch.nn.MSELoss()

        # è½‰æ›ç‚º Tensor
        X_train_tensor = torch.FloatTensor(X_train).to(device)
        y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device)
        X_valid_tensor = torch.FloatTensor(X_valid).to(device)
        y_valid_tensor = torch.FloatTensor(y_valid).unsqueeze(1).to(device)

        best_valid_loss = float('inf')
        patience_counter = 0

        # Initialize model weight path
        model_weight_dir = "/app/models/trained"
        os.makedirs(model_weight_dir, exist_ok=True)
        model_weight_path = os.path.join(
            model_weight_dir,
            f"model_{model_id}_job_{job_id}_best.pth"
        )

        # Check training data quality before starting
        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"è¨“ç·´æ•¸æ“šæª¢æŸ¥: X shape={X_train.shape}, y shape={y_train.shape}, "
            f"X NaN={np.isnan(X_train).sum()}, y NaN={np.isnan(y_train).sum()}"
        )

        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"é–‹å§‹è¨“ç·´ï¼š{num_epochs} è¼ª, æ‰¹æ¬¡å¤§å°={batch_size}, å­¸ç¿’ç‡={learning_rate}"
        )

        for epoch in range(1, num_epochs + 1):
            # è¨“ç·´æ¨¡å¼
            model.train()
            train_losses = []

            # Mini-batch è¨“ç·´
            for i in range(0, len(X_train_tensor), batch_size):
                batch_X = X_train_tensor[i:i+batch_size]
                batch_y = y_train_tensor[i:i+batch_size]

                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)

                # Check for NaN loss
                if torch.isnan(loss):
                    ModelTrainingJobRepository.append_log(
                        db, job_id,
                        f"âš ï¸ Epoch {epoch} Batch {i//batch_size}: Loss is NaN! "
                        f"Output stats: mean={outputs.mean():.6f}, std={outputs.std():.6f}"
                    )
                    # Skip this batch if loss is NaN
                    continue

                loss.backward()

                # Gradient clipping to prevent exploding gradients
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                optimizer.step()

                train_losses.append(loss.item())

            if len(train_losses) == 0:
                ModelTrainingJobRepository.append_log(
                    db, job_id,
                    f"âš ï¸ Epoch {epoch}: æ‰€æœ‰ batch çš„ loss éƒ½æ˜¯ NaNï¼Œè¨“ç·´å¤±æ•—"
                )
                train_loss = float('nan')
            else:
                train_loss = np.mean(train_losses)

            # é©—è­‰æ¨¡å¼
            model.eval()
            with torch.no_grad():
                valid_outputs = model(X_valid_tensor)
                valid_loss = criterion(valid_outputs, y_valid_tensor).item()

            # æ›´æ–°é€²åº¦ï¼ˆæ¯è¼ªï¼‰
            progress = 0.4 + 0.5 * (epoch / num_epochs)  # 0.4 åˆ° 0.9

            ModelTrainingJobRepository.update_progress(
                db, job_id,
                progress=progress,
                current_epoch=epoch,
                current_step=f"è¨“ç·´ä¸­ Epoch {epoch}/{num_epochs}",
                train_loss=train_loss if not np.isnan(train_loss) else None,
                valid_loss=valid_loss if not np.isnan(valid_loss) else None
            )

            # Log first, last epoch, or every 10 epochs
            if epoch == 1 or epoch == num_epochs or epoch % 10 == 0:
                ModelTrainingJobRepository.append_log(
                    db, job_id,
                    f"Epoch {epoch}/{num_epochs}: train_loss={train_loss:.6f}, valid_loss={valid_loss:.6f}"
                )

            # Early Stopping
            if valid_loss < best_valid_loss:
                best_valid_loss = valid_loss
                patience_counter = 0

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(model.state_dict(), model_weight_path)
            else:
                patience_counter += 1
                if patience_counter >= early_stop_rounds:
                    ModelTrainingJobRepository.append_log(
                        db, job_id,
                        f"Early stopping triggered at epoch {epoch} (æœ€ä½³é©—è­‰æå¤±: {best_valid_loss:.6f})"
                    )
                    break

        # Save final model if it wasn't saved during training (safety fallback)
        if not os.path.exists(model_weight_path):
            torch.save(model.state_dict(), model_weight_path)
            ModelTrainingJobRepository.append_log(
                db, job_id,
                "ä¿å­˜æœ€çµ‚æ¨¡å‹æ¬Šé‡ï¼ˆæœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹ï¼‰"
            )

        # ========== æ­¥é©Ÿ 7ï¼šæ¸¬è©¦æ¨¡å‹ ==========
        ModelTrainingJobRepository.update_progress(
            db, job_id,
            progress=0.95,
            current_epoch=epoch,
            current_step="æ­£åœ¨æ¸¬è©¦æ¨¡å‹..."
        )

        # è¼‰å…¥æœ€ä½³æ¨¡å‹
        model.load_state_dict(torch.load(model_weight_path))
        model.eval()

        X_test_tensor = torch.FloatTensor(X_test).to(device)
        y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1).to(device)

        with torch.no_grad():
            test_outputs = model(X_test_tensor)
            test_predictions = test_outputs.cpu().numpy().flatten()
            test_actuals = y_test

            # Debug logging
            ModelTrainingJobRepository.append_log(
                db, job_id,
                f"é æ¸¬å€¼çµ±è¨ˆ: mean={np.mean(test_predictions):.6f}, std={np.std(test_predictions):.6f}, "
                f"min={np.min(test_predictions):.6f}, max={np.max(test_predictions):.6f}, "
                f"NaN count={np.isnan(test_predictions).sum()}"
            )

            # Check for NaN or Inf in predictions
            if np.any(np.isnan(test_predictions)) or np.any(np.isinf(test_predictions)):
                ModelTrainingJobRepository.append_log(
                    db, job_id,
                    "âš ï¸ è­¦å‘Š: é æ¸¬å€¼åŒ…å« NaN æˆ– Infï¼Œå°‡æ›¿æ›ç‚º 0"
                )
                test_predictions = np.nan_to_num(test_predictions, nan=0.0, posinf=0.0, neginf=0.0)

            # Calculate IC (Information Coefficient)
            # Handle case where std is 0 or correlation can't be computed
            if np.std(test_predictions) < 1e-10 or np.std(test_actuals) < 1e-10:
                test_ic = 0.0
                ModelTrainingJobRepository.append_log(
                    db, job_id,
                    "âš ï¸ é æ¸¬å€¼æˆ–å¯¦éš›å€¼æ¨™æº–å·®æ¥è¿‘ 0ï¼ŒIC è¨­ç‚º 0"
                )
            else:
                try:
                    correlation_matrix = np.corrcoef(test_predictions, test_actuals)
                    test_ic = correlation_matrix[0, 1]
                    if np.isnan(test_ic):
                        test_ic = 0.0
                except:
                    test_ic = 0.0

            # Calculate other metrics
            mse = np.mean((test_predictions - test_actuals) ** 2)
            mae = np.mean(np.abs(test_predictions - test_actuals))
            pred_mean = np.mean(test_predictions)
            pred_std = np.std(test_predictions)

            # Convert NaN to None for JSON compatibility
            def safe_float(value):
                if np.isnan(value) or np.isinf(value):
                    return None
                return float(value)

            test_metrics = {
                'ic': safe_float(test_ic),
                'mse': safe_float(mse),
                'mae': safe_float(mae),
                'predictions_mean': safe_float(pred_mean),
                'predictions_std': safe_float(pred_std)
            }

            # Also convert test_ic for database save
            test_ic_safe = safe_float(test_ic)

        ModelTrainingJobRepository.append_log(
            db, job_id,
            f"æ¸¬è©¦çµæœ: IC={test_ic_safe if test_ic_safe is not None else 'N/A'}, "
            f"MSE={test_metrics['mse'] if test_metrics['mse'] is not None else 'N/A'}"
        )

        # ========== æ­¥é©Ÿ 8ï¼šå®Œæˆè¨“ç·´ ==========
        ModelTrainingJobRepository.update_completed(
            db, job_id,
            model_weight_path=model_weight_path,
            test_ic=test_ic_safe,
            test_metrics=test_metrics
        )

        ModelTrainingJobRepository.append_log(
            db, job_id,
            "âœ… è¨“ç·´å®Œæˆï¼æ¨¡å‹æ¬Šé‡å·²ä¿å­˜ã€‚"
        )

        return {
            'status': 'success',
            'job_id': job_id,
            'model_weight_path': model_weight_path,
            'test_ic': test_ic_safe,
            'test_metrics': test_metrics
        }

    except Exception as e:
        # Rollback current transaction to allow new queries
        db.rollback()

        # è¨˜éŒ„éŒ¯èª¤
        error_msg = f"è¨“ç·´å¤±æ•—: {str(e)}"
        try:
            ModelTrainingJobRepository.update_status(
                db, job_id,
                status="FAILED",
                error_message=error_msg
            )
            ModelTrainingJobRepository.append_log(
                db, job_id,
                f"âŒ {error_msg}"
            )
        except Exception as update_error:
            # If we can't update the database, at least log it
            print(f"Failed to update job status: {update_error}")

        raise e

    finally:
        db.close()


@shared_task(name="app.tasks.model_training_tasks.cancel_training_job")
def cancel_training_job(job_id: int) -> Dict[str, Any]:
    """
    å–æ¶ˆè¨“ç·´ä»»å‹™

    Args:
        job_id: è¨“ç·´ä»»å‹™ ID

    Returns:
        å–æ¶ˆçµæœ
    """
    db = SessionLocal()

    try:
        job = ModelTrainingJobRepository.get_by_id(db, job_id)
        if not job:
            return {'status': 'error', 'message': f'ä»»å‹™ {job_id} ä¸å­˜åœ¨'}

        if job.status not in ['PENDING', 'RUNNING']:
            return {
                'status': 'error',
                'message': f'ä»»å‹™ç‹€æ…‹ç‚º {job.status}ï¼Œç„¡æ³•å–æ¶ˆ'
            }

        # æ›´æ–°ç‹€æ…‹ç‚º CANCELLED
        ModelTrainingJobRepository.update_status(
            db, job_id,
            status="CANCELLED"
        )

        ModelTrainingJobRepository.append_log(
            db, job_id,
            "ğŸš« è¨“ç·´ä»»å‹™å·²è¢«ç”¨æˆ¶å–æ¶ˆ"
        )

        return {
            'status': 'success',
            'job_id': job_id,
            'message': 'è¨“ç·´ä»»å‹™å·²å–æ¶ˆ'
        }

    finally:
        db.close()
