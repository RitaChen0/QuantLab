"""
è²¡å‹™æŒ‡æ¨™å®šæ™‚åŒæ­¥ä»»å‹™

å®šæœŸå¾ FinLab API åŒæ­¥è²¡å‹™æ•¸æ“šåˆ° PostgreSQL
"""
from celery import Task
from sqlalchemy.orm import Session
from app.core.celery_app import celery_app
from app.db.session import SessionLocal
from app.services.fundamental_service import FundamentalService
from app.services.finlab_client import FinLabClient
from app.utils.task_history import record_task_history
from app.utils.task_deduplication import skip_if_recently_executed
from app.utils.cache import cache
from loguru import logger
from datetime import datetime, timezone
from typing import List


@celery_app.task(bind=True, name="app.tasks.sync_fundamental_data")
@skip_if_recently_executed(min_interval_hours=168)  # é€±ä»»å‹™ï¼š7 å¤© = 168 å°æ™‚
@record_task_history
def sync_fundamental_data(
    self: Task,
    stock_ids: List[str] = None,
    indicators: List[str] = None
) -> dict:
    """
    åŒæ­¥è²¡å‹™æŒ‡æ¨™æ•¸æ“š

    Args:
        stock_ids: è¦åŒæ­¥çš„è‚¡ç¥¨åˆ—è¡¨ï¼ˆNone = ç†±é–€è‚¡ç¥¨ï¼‰
        indicators: è¦åŒæ­¥çš„æŒ‡æ¨™åˆ—è¡¨ï¼ˆNone = æ‰€æœ‰18å€‹æŒ‡æ¨™ï¼‰

    Returns:
        åŒæ­¥çµæœçµ±è¨ˆ
    """
    # ğŸ”’ Distributed lock - prevent concurrent execution
    redis_client = cache.redis_client
    lock_key = f"task_lock:{self.name}"
    # 2 å°æ™‚è¶…æ™‚ï¼ˆä»»å‹™é è¨ˆåŸ·è¡Œæ™‚é–“ï¼š30-60 åˆ†é˜ï¼Œå–æ±ºæ–¼è‚¡ç¥¨å’ŒæŒ‡æ¨™æ•¸é‡ï¼‰
    lock = redis_client.lock(lock_key, timeout=7200)

    # å˜—è©¦ç²å–é–ï¼ˆéé˜»å¡ï¼‰
    if not lock.acquire(blocking=False):
        logger.warning(f"âš ï¸  ä»»å‹™ {self.name} å·²åœ¨åŸ·è¡Œä¸­ï¼Œè·³éæ­¤æ¬¡è§¸ç™¼")
        logger.info(f"   é–å®š Key: {lock_key}")
        return {
            "status": "skipped",
            "reason": "task_already_running",
            "message": f"Task {self.name} is already running, skipped duplicate execution",
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

    logger.info(f"ğŸ” å·²ç²å–ä»»å‹™é–: {lock_key}")

    db: Session = SessionLocal()

    try:
        logger.info("=" * 60)
        logger.info("é–‹å§‹åŒæ­¥è²¡å‹™æŒ‡æ¨™æ•¸æ“š")
        logger.info("=" * 60)

        # ä½¿ç”¨ç†±é–€è‚¡ç¥¨å¦‚æœæœªæŒ‡å®š
        if stock_ids is None:
            stock_ids = [
                "2330",  # å°ç©é›»
                "2317",  # é´»æµ·
                "2454",  # è¯ç™¼ç§‘
                "2412",  # ä¸­è¯é›»
                "2882",  # åœ‹æ³°é‡‘
                "2881",  # å¯Œé‚¦é‡‘
                "2308",  # å°é”é›»
                "2303",  # è¯é›»
            ]

        # ä½¿ç”¨æ‰€æœ‰æŒ‡æ¨™å¦‚æœæœªæŒ‡å®š
        if indicators is None:
            indicators = FinLabClient.get_common_fundamental_indicators()

        service = FundamentalService(db)
        total_synced = 0
        failed = []

        for stock_id in stock_ids:
            for indicator in indicators:
                try:
                    count = service.sync_indicator_data(
                        stock_id=stock_id,
                        indicator=indicator,
                        start_date=None,  # Sync all available data
                        end_date=None
                    )
                    total_synced += count
                    logger.info(f"âœ… {stock_id} - {indicator}: {count} points")

                except Exception as e:
                    logger.warning(f"âŒ Failed {stock_id} - {indicator}: {str(e)}")
                    failed.append(f"{stock_id}:{indicator}")
                    continue

        logger.info("=" * 60)
        logger.info(f"åŒæ­¥å®Œæˆ: {total_synced} å€‹æ•¸æ“šé»")
        logger.info(f"å¤±æ•—: {len(failed)} å€‹æŒ‡æ¨™")
        logger.info("=" * 60)

        return {
            "status": "success",
            "total_synced": total_synced,
            "stocks_count": len(stock_ids),
            "indicators_count": len(indicators),
            "failed_count": len(failed),
            "failed_items": failed[:10],  # Only return first 10 failures
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

    except Exception as e:
        logger.error(f"åŒæ­¥ä»»å‹™å¤±æ•—: {str(e)}")
        # ä½¿ç”¨æŒ‡æ•¸é€€é¿ï¼š5m, 10m, 20m
        retry_count = self.request.retries
        countdown = 300 * (2 ** retry_count)
        raise self.retry(exc=e, countdown=countdown, max_retries=3)

    finally:
        db.close()
        # ç¢ºä¿é‡‹æ”¾é–
        try:
            lock.release()
            logger.info("ğŸ”“ ä»»å‹™é–å·²é‡‹æ”¾")
        except Exception as e:
            logger.error(f"Failed to release lock: {e}")


@celery_app.task(bind=True, name="app.tasks.sync_fundamental_latest")
@skip_if_recently_executed(min_interval_hours=24)
@record_task_history
def sync_fundamental_latest(self: Task) -> dict:
    """
    å¿«é€ŸåŒæ­¥æœ€æ–°å­£åº¦çš„è²¡å‹™æ•¸æ“š

    åªåŒæ­¥ç†±é–€è‚¡ç¥¨çš„æœ€æ–°ä¸€å­£æ•¸æ“šï¼ˆç”¨æ–¼é »ç¹æ›´æ–°ï¼‰

    Returns:
        åŒæ­¥çµæœçµ±è¨ˆ
    """
    db: Session = SessionLocal()

    try:
        logger.info("é–‹å§‹å¿«é€ŸåŒæ­¥æœ€æ–°è²¡å‹™æ•¸æ“š")

        # ç†±é–€è‚¡ç¥¨å‰ 5 å
        hot_stocks = ["2330", "2317", "2454", "2412", "2882"]

        # åªåŒæ­¥æœ€é‡è¦çš„æŒ‡æ¨™
        important_indicators = [
            "ROEç¨…å¾Œ",
            "æ¯è‚¡ç¨…å¾Œæ·¨åˆ©",
            "ç‡Ÿæ¥­åˆ©ç›Šç‡",
            "ç‡Ÿæ”¶æˆé•·ç‡",
        ]

        service = FundamentalService(db)
        total_synced = 0

        # åªåŒæ­¥æœ€è¿‘1å¹´çš„æ•¸æ“š
        from datetime import datetime, timedelta
        end_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
        start_date = (datetime.now(timezone.utc) - timedelta(days=365)).strftime("%Y-%m-%d")

        for stock_id in hot_stocks:
            for indicator in important_indicators:
                try:
                    count = service.sync_indicator_data(
                        stock_id=stock_id,
                        indicator=indicator,
                        start_date=start_date,
                        end_date=end_date
                    )
                    total_synced += count

                except Exception as e:
                    logger.warning(f"Failed {stock_id} - {indicator}: {str(e)}")
                    continue

        logger.info(f"å¿«é€ŸåŒæ­¥å®Œæˆ: {total_synced} å€‹æ•¸æ“šé»")

        return {
            "status": "success",
            "total_synced": total_synced,
            "stocks": hot_stocks,
            "indicators": important_indicators,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

    except Exception as e:
        logger.error(f"å¿«é€ŸåŒæ­¥å¤±æ•—: {str(e)}")
        # ä½¿ç”¨æŒ‡æ•¸é€€é¿ï¼š3m, 6m, 12m
        retry_count = self.request.retries
        countdown = 180 * (2 ** retry_count)
        raise self.retry(exc=e, countdown=countdown, max_retries=3)

    finally:
        db.close()
