"""
Admin API Endpoints

Requires superuser authentication.
"""
from typing import List, Optional
from fastapi import APIRouter, Depends, HTTPException, status, Query
from sqlalchemy.orm import Session
from sqlalchemy import text
from datetime import datetime, timezone, timedelta
from celery.schedules import crontab

from app.api.dependencies import get_db, get_current_superuser
from app.db.session import SessionLocal
from app.models.user import User
from app.models.strategy import Strategy
from app.models.backtest import Backtest
from app.schemas.admin import (
    UserListResponse,
    UserUpdateAdmin,
    SystemStats,
    ServiceHealth,
    SyncTaskInfo,
    SyncHistoryItem,
    ManualSyncRequest,
    LogQueryRequest,
    LogQueryResponse,
    LogEntry,
    CeleryWorkerInfo,
    CeleryTaskStatus,
    SecurityEvent,
    SecurityStats,
    SecurityEventsResponse,
)
from app.core.celery_app import celery_app
from app.utils.logging import logger
from app.services.admin_service import AdminService

router = APIRouter()


def get_admin_service(db: Session = Depends(get_db)) -> AdminService:
    """Dependency to get AdminService instance"""
    return AdminService(db)


def format_crontab_schedule(schedule) -> str:
    """
    å°‡ crontab schedule è½‰æ›ç‚ºäººé¡å¯è®€çš„æ–‡å­—

    é‡è¦ï¼šCelery é…ç½®ç‚º timezone="UTC", enable_utc=True
    å› æ­¤ crontab çš„æ™‚é–“åƒæ•¸æ˜¯ UTC æ™‚é–“ï¼Œéœ€è¦è½‰æ›ç‚ºå°åŒ—æ™‚é–“é¡¯ç¤ºï¼

    Examples:
        crontab(hour=1, minute=0) -> "æ¯å¤© 09:00 (å°åŒ—æ™‚é–“)"ï¼ˆUTC 01:00 = å°åŒ— 09:00ï¼‰
        crontab(hour='1-5', minute='*/15') -> "äº¤æ˜“æ—¥ 09:00-13:59 æ¯ 15 åˆ†é˜ (å°åŒ—æ™‚é–“)"
    """
    if not isinstance(schedule, crontab):
        return str(schedule)

    # è§£æ crontab å„å€‹æ¬„ä½ï¼ˆUTC æ™‚é–“ï¼‰
    minute = schedule._orig_minute
    hour = schedule._orig_hour
    day_of_month = schedule._orig_day_of_month
    month_of_year = schedule._orig_month_of_year
    day_of_week = schedule._orig_day_of_week

    # Helper function: UTC â†’ å°åŒ—æ™‚é–“ï¼ˆ+8 å°æ™‚ï¼‰
    def utc_to_taipei(hour_utc: int) -> int:
        """Convert UTC hour to Taipei hour (UTC+8)"""
        return (hour_utc + 8) % 24

    # æ˜ŸæœŸå°ç…§è¡¨
    weekday_map = {
        '0': 'é€±æ—¥', '1': 'é€±ä¸€', '2': 'é€±äºŒ', '3': 'é€±ä¸‰',
        '4': 'é€±å››', '5': 'é€±äº”', '6': 'é€±å…­'
    }

    # æœˆä»½å°ç…§è¡¨
    month_map = {
        '1': '1æœˆ', '2': '2æœˆ', '3': '3æœˆ', '4': '4æœˆ',
        '5': '5æœˆ', '6': '6æœˆ', '7': '7æœˆ', '8': '8æœˆ',
        '9': '9æœˆ', '10': '10æœˆ', '11': '11æœˆ', '12': '12æœˆ'
    }

    # è™•ç†æ¯é€±ç‰¹å®šæ—¥æœŸï¼ˆéæ™‚é–“ç¯„åœï¼‰
    if day_of_week != '*' and hour != '*' and minute != '*' and not (isinstance(hour, str) and '-' in hour):
        # è½‰æ› UTC â†’ å°åŒ—æ™‚é–“
        taipei_hour = utc_to_taipei(int(hour))
        if day_of_week == 'mon,tue,wed,thu,fri':
            return f"äº¤æ˜“æ—¥ {str(taipei_hour).zfill(2)}:{str(minute).zfill(2)} (å°åŒ—æ™‚é–“)"
        weekday = weekday_map.get(str(day_of_week), f'é€±{day_of_week}')
        return f"æ¯{weekday} {str(taipei_hour).zfill(2)}:{str(minute).zfill(2)} (å°åŒ—æ™‚é–“)"

    # è™•ç†æ¯æœˆç‰¹å®šæ—¥æœŸ
    if day_of_month != '*' and hour != '*' and minute != '*':
        taipei_hour = utc_to_taipei(int(hour))
        return f"æ¯æœˆ {day_of_month} æ—¥ {str(taipei_hour).zfill(2)}:{str(minute).zfill(2)} (å°åŒ—æ™‚é–“)"

    # è™•ç†æ¯å¹´ç‰¹å®šæ—¥æœŸ
    if month_of_year != '*' and day_of_month != '*' and hour != '*' and minute != '*':
        taipei_hour = utc_to_taipei(int(hour))
        month = month_map.get(str(month_of_year), f'{month_of_year}æœˆ')
        return f"æ¯å¹´ {month} {day_of_month} æ—¥ {str(taipei_hour).zfill(2)}:{str(minute).zfill(2)} (å°åŒ—æ™‚é–“)"

    # è™•ç†æ¯å°æ™‚
    if hour == '*' and minute != '*':
        if isinstance(minute, str) and minute.startswith('*/'):
            interval = minute.replace('*/', '')
            return f"æ¯ {interval} åˆ†é˜"
        return f"æ¯å°æ™‚ {str(minute).zfill(2)} åˆ†"

    # è™•ç†ç‰¹å®šæ™‚é–“ç¯„åœï¼ˆå« day_of_weekï¼‰
    if hour != '*' and isinstance(hour, str) and '-' in hour:
        start_hour_utc, end_hour_utc = hour.split('-')
        # è½‰æ› UTC â†’ å°åŒ—æ™‚é–“
        start_hour_taipei = utc_to_taipei(int(start_hour_utc))
        end_hour_taipei = utc_to_taipei(int(end_hour_utc))

        if isinstance(minute, str) and minute.startswith('*/'):
            interval = minute.replace('*/', '')
            if day_of_week == 'mon,tue,wed,thu,fri':
                prefix = "äº¤æ˜“æ—¥"
            elif day_of_week != '*':
                prefix = f"æ¯{weekday_map.get(str(day_of_week), 'å¤©')}"
            else:
                prefix = "æ¯å¤©"
            return f"{prefix} {str(start_hour_taipei).zfill(2)}:00-{str(end_hour_taipei).zfill(2)}:59 æ¯ {interval} åˆ†é˜ (å°åŒ—æ™‚é–“)"

    # è™•ç†æ¯å¤©ç‰¹å®šæ™‚é–“
    if day_of_week == '*' and day_of_month == '*' and hour != '*' and minute != '*':
        # æª¢æŸ¥æ˜¯å¦ç‚ºé–“éš” (ä¾‹å¦‚ */4)
        if isinstance(hour, str) and hour.startswith('*/'):
            interval = hour.replace('*/', '')
            return f"æ¯ {interval} å°æ™‚"

        # è½‰æ› UTC â†’ å°åŒ—æ™‚é–“
        taipei_hour = utc_to_taipei(int(hour))
        return f"æ¯å¤© {str(taipei_hour).zfill(2)}:{str(minute).zfill(2)} (å°åŒ—æ™‚é–“)"

    # é è¨­è¿”å›åŸå§‹æ ¼å¼ï¼ˆUTC æ™‚é–“ + æ¨™è¨»ï¼‰
    return f"{minute} {hour} {day_of_month} {month_of_year} {day_of_week} (UTC)"


# ============ User Management ============

@router.get("/users", response_model=List[UserListResponse])
async def list_users(
    skip: int = Query(0, ge=0),
    limit: int = Query(50, ge=1, le=100),
    current_user: User = Depends(get_current_superuser),
    admin_service: AdminService = Depends(get_admin_service),
):
    """Get all users (admin only)"""
    users = admin_service.list_users(skip=skip, limit=limit)
    return users


@router.get("/users/{user_id}", response_model=UserListResponse)
async def get_user_detail(
    user_id: int,
    current_user: User = Depends(get_current_superuser),
    admin_service: AdminService = Depends(get_admin_service),
):
    """Get user detail (admin only)"""
    user = admin_service.get_user(user_id)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )
    return user


@router.patch("/users/{user_id}", response_model=UserListResponse)
async def update_user(
    user_id: int,
    user_update: UserUpdateAdmin,
    current_user: User = Depends(get_current_superuser),
    admin_service: AdminService = Depends(get_admin_service),
):
    """Update user (admin only)"""
    user = admin_service.update_user(user_id, user_update)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )

    logger.info(f"Admin {current_user.username} updated user {user.username}")
    return user


@router.delete("/users/{user_id}")
async def delete_user(
    user_id: int,
    current_user: User = Depends(get_current_superuser),
    admin_service: AdminService = Depends(get_admin_service),
):
    """
    Delete user (admin only)

    Returns detailed information about the deletion result,
    including related data counts and error details if deletion fails.
    """
    from app.core.exceptions import DatabaseError

    result = admin_service.delete_user(user_id, current_user.id)

    if not result["success"]:
        error_code = result.get("error_code", "UNKNOWN_ERROR")

        # Map error codes to HTTP status codes
        status_code_map = {
            "CANNOT_DELETE_SELF": status.HTTP_400_BAD_REQUEST,
            "CANNOT_DELETE_SYSTEM_ADMIN": status.HTTP_403_FORBIDDEN,
            "CANNOT_DELETE_PROTECTED_ACCOUNT": status.HTTP_403_FORBIDDEN,
            "USER_NOT_FOUND": status.HTTP_404_NOT_FOUND,
            "FOREIGN_KEY_VIOLATION": status.HTTP_409_CONFLICT,
            "DATABASE_ERROR": status.HTTP_500_INTERNAL_SERVER_ERROR,
            "UNKNOWN_ERROR": status.HTTP_500_INTERNAL_SERVER_ERROR,
        }

        http_status = status_code_map.get(error_code, status.HTTP_500_INTERNAL_SERVER_ERROR)

        # Use DatabaseError for better error formatting
        if error_code in ["FOREIGN_KEY_VIOLATION", "DATABASE_ERROR"]:
            raise DatabaseError(
                message=result["message"],
                details=result.get("details", {})
            )
        else:
            raise HTTPException(
                status_code=http_status,
                detail={
                    "message": result["message"],
                    "error_code": error_code,
                    "details": result.get("details")
                }
            )

    logger.info(f"Admin {current_user.username} deleted user ID {user_id}")

    return {
        "success": True,
        "message": result["message"],
        "details": result.get("details", {})
    }


# ============ System Stats ============

@router.get("/stats", response_model=SystemStats)
async def get_system_stats(
    current_user: User = Depends(get_current_superuser),
    admin_service: AdminService = Depends(get_admin_service),
):
    """Get system statistics (admin only)"""
    stats = admin_service.get_system_stats()
    return SystemStats(**stats)


@router.get("/health", response_model=List[ServiceHealth])
async def get_services_health(
    current_user: User = Depends(get_current_superuser),
    db: Session = Depends(get_db),
):
    """Check health of all services (admin only)"""
    services = []

    # Check PostgreSQL
    try:
        # Execute a simple query to test database connectivity
        # No explicit transaction needed for read-only health checks
        result = db.execute(text("SELECT 1"))
        result.fetchone()  # Ensure query executes
        services.append(ServiceHealth(
            service_name="PostgreSQL",
            status="healthy",
            last_check=datetime.now(timezone.utc)
        ))
    except Exception as e:
        logger.error(f"PostgreSQL health check failed: {str(e)}")
        services.append(ServiceHealth(
            service_name="PostgreSQL",
            status="unhealthy",
            last_check=datetime.now(timezone.utc)
        ))

    # Check Redis
    from app.utils.cache import cache
    try:
        if cache.is_available():
            cache.redis_client.ping()
            services.append(ServiceHealth(
                service_name="Redis",
                status="healthy",
                last_check=datetime.now(timezone.utc)
            ))
        else:
            services.append(ServiceHealth(
                service_name="Redis",
                status="unhealthy",
                last_check=datetime.now(timezone.utc)
            ))
    except Exception as e:
        logger.error(f"Redis health check failed: {str(e)}")
        services.append(ServiceHealth(
            service_name="Redis",
            status="unhealthy",
            last_check=datetime.now(timezone.utc)
        ))

    # Check Celery Worker
    try:
        inspect = celery_app.control.inspect()
        stats = inspect.stats()
        if stats:
            services.append(ServiceHealth(
                service_name="Celery Worker",
                status="healthy",
                last_check=datetime.now(timezone.utc)
            ))
        else:
            logger.warning("Celery Worker health check: No workers found")
            services.append(ServiceHealth(
                service_name="Celery Worker",
                status="unhealthy",
                last_check=datetime.now(timezone.utc)
            ))
    except Exception as e:
        logger.error(f"Celery Worker health check failed: {str(e)}")
        services.append(ServiceHealth(
            service_name="Celery Worker",
            status="unknown",
            last_check=datetime.now(timezone.utc)
        ))

    return services


# ============ Data Sync Management ============

@router.get("/sync/tasks", response_model=List[SyncTaskInfo])
async def list_sync_tasks(
    current_user: User = Depends(get_current_superuser),
):
    """Get all sync tasks and their schedules (admin only)"""
    from app.utils.cache import cache
    import json

    schedule = celery_app.conf.beat_schedule
    tasks = []

    # ============================================
    # ä»»å‹™åˆ†é¡ï¼šæ•¸æ“šåŒæ­¥ã€æ•¸æ“šè™•ç†ã€ç­–ç•¥è™•ç†
    # ============================================

    # æ•¸æ“šåŒæ­¥ä»»å‹™ï¼ˆå¤–éƒ¨ API â†’ è³‡æ–™åº«ï¼‰
    data_sync_task_names = [
        "app.tasks.sync_stock_list",
        "app.tasks.sync_daily_prices",
        "app.tasks.sync_ohlcv_data",
        "app.tasks.sync_latest_prices_shioaji",
        "app.tasks.sync_fundamental_data",
        "app.tasks.sync_fundamental_latest",
        "app.tasks.sync_top_stocks_institutional",
        "app.tasks.sync_institutional_investors",
        "app.tasks.sync_single_stock_institutional",
        "app.tasks.sync_shioaji_top_stocks",
        "app.tasks.sync_shioaji_futures",
        "app.tasks.sync_option_daily_factors",
        "app.tasks.register_option_contracts",
    ]

    # æ•¸æ“šè™•ç†ä»»å‹™ï¼ˆè³‡æ–™åº« â†’ è¨ˆç®— â†’ è³‡æ–™åº«/æ–‡ä»¶ï¼‰
    data_processing_task_names = [
        "app.tasks.generate_continuous_contracts",
        "app.tasks.register_new_futures_contracts",
        "app.tasks.cleanup_old_cache",
        "app.tasks.cleanup_old_institutional_data",
        "app.tasks.cleanup_old_signals",
    ]

    # ç­–ç•¥è™•ç†ä»»å‹™ï¼ˆè³‡æ–™åº« â†’ ç­–ç•¥å¼•æ“ â†’ ä¿¡è™Ÿï¼‰
    strategy_processing_task_names = [
        "app.tasks.monitor_active_strategies",
    ]

    task_display_names = {
        # æ•¸æ“šåŒæ­¥ä»»å‹™
        "app.tasks.sync_stock_list": "åŒæ­¥è‚¡ç¥¨åˆ—è¡¨",
        "app.tasks.sync_daily_prices": "åŒæ­¥æ¯æ—¥åƒ¹æ ¼",
        "app.tasks.sync_ohlcv_data": "åŒæ­¥ OHLCV æ•¸æ“š",
        "app.tasks.sync_latest_prices_shioaji": "åŒæ­¥æœ€æ–°åƒ¹æ ¼ï¼ˆShioajiï¼‰",
        "app.tasks.sync_fundamental_data": "åŒæ­¥è²¡å‹™æŒ‡æ¨™ï¼ˆå®Œæ•´ï¼‰",
        "app.tasks.sync_fundamental_latest": "åŒæ­¥è²¡å‹™æŒ‡æ¨™ï¼ˆå¿«é€Ÿï¼‰",
        "app.tasks.sync_top_stocks_institutional": "åŒæ­¥æ³•äººè²·è³£è¶…ï¼ˆå…¨éƒ¨è‚¡ç¥¨ï¼‰",
        "app.tasks.sync_institutional_investors": "åŒæ­¥æ³•äººè²·è³£è¶…",
        "app.tasks.sync_single_stock_institutional": "åŒæ­¥å–®ä¸€è‚¡ç¥¨æ³•äººè²·è³£è¶…",
        "app.tasks.sync_shioaji_top_stocks": "åŒæ­¥ Shioaji åˆ†é˜ç·šï¼ˆå…¨éƒ¨è‚¡ç¥¨ï¼‰",
        "app.tasks.sync_shioaji_futures": "åŒæ­¥ Shioaji æœŸè²¨åˆ†é˜ç·šï¼ˆTX/MTXï¼‰",
        "app.tasks.sync_option_daily_factors": "åŒæ­¥é¸æ“‡æ¬Šå› å­ï¼ˆå«è¨ˆç®—ï¼‰",
        "app.tasks.register_option_contracts": "è¨»å†Šé¸æ“‡æ¬Šåˆç´„",

        # æ•¸æ“šè™•ç†ä»»å‹™
        "app.tasks.generate_continuous_contracts": "ç”ŸæˆæœŸè²¨é€£çºŒåˆç´„",
        "app.tasks.register_new_futures_contracts": "è¨»å†Šæ–°å¹´åº¦æœŸè²¨åˆç´„",
        "app.tasks.cleanup_old_cache": "æ¸…ç†éæœŸå¿«å–",
        "app.tasks.cleanup_old_institutional_data": "æ¸…ç†éæœŸæ³•äººæ•¸æ“š",
        "app.tasks.cleanup_old_signals": "æ¸…ç†èˆŠä¿¡è™Ÿè¨˜éŒ„",

        # ç­–ç•¥è™•ç†ä»»å‹™
        "app.tasks.monitor_active_strategies": "ğŸ”” ç›£æ§ç­–ç•¥ä¿¡è™Ÿï¼ˆå¯¦ç›¤ï¼‰",
    }

    for name, config in schedule.items():
        task_name = config["task"]

        # Only include data sync tasks
        if task_name not in data_sync_task_names:
            continue

        # Get last run information from Redis cache
        last_run = None
        last_run_status = None
        last_run_result = None
        error_message = None

        try:
            if cache.is_available():
                # Try to get task execution history from Redis
                # Key format: task_history:{task_name}
                history_key = f"task_history:{task_name}"
                history_data = cache.get(history_key)

                if history_data:
                    if isinstance(history_data, str):
                        history_data = json.loads(history_data)

                    last_run_str = history_data.get("last_run")
                    if last_run_str:
                        last_run = datetime.fromisoformat(last_run_str.replace('Z', '+00:00'))

                    last_run_status = history_data.get("status", "unknown")
                    last_run_result = history_data.get("result")
                    error_message = history_data.get("error")
        except Exception as e:
            logger.warning(f"Failed to get task history for {task_name}: {str(e)}")

        tasks.append(SyncTaskInfo(
            task_name=task_name,
            display_name=task_display_names.get(task_name, task_name),
            schedule=format_crontab_schedule(config["schedule"]),
            last_run=last_run,
            last_run_status=last_run_status,
            last_run_result=last_run_result,
            error_message=error_message,
            status="active",
        ))

    return tasks


@router.get("/processing/tasks", response_model=List[SyncTaskInfo])
async def list_processing_tasks(
    current_user: User = Depends(get_current_superuser),
):
    """Get all data processing tasks and their schedules (admin only)"""
    from app.utils.cache import cache
    import json

    schedule = celery_app.conf.beat_schedule
    tasks = []

    # æ•¸æ“šè™•ç†ä»»å‹™æ¸…å–®ï¼ˆè³‡æ–™åº« â†’ è¨ˆç®— â†’ è³‡æ–™åº«/æ–‡ä»¶ï¼‰
    data_processing_task_names = [
        "app.tasks.generate_continuous_contracts",
        "app.tasks.register_new_futures_contracts",
        "app.tasks.cleanup_old_cache",
        "app.tasks.cleanup_old_institutional_data",
        "app.tasks.cleanup_old_signals",
    ]

    task_display_names = {
        "app.tasks.generate_continuous_contracts": "ç”ŸæˆæœŸè²¨é€£çºŒåˆç´„",
        "app.tasks.register_new_futures_contracts": "è¨»å†Šæ–°å¹´åº¦æœŸè²¨åˆç´„",
        "app.tasks.cleanup_old_cache": "æ¸…ç†éæœŸå¿«å–",
        "app.tasks.cleanup_old_institutional_data": "æ¸…ç†éæœŸæ³•äººæ•¸æ“š",
        "app.tasks.cleanup_old_signals": "æ¸…ç†èˆŠä¿¡è™Ÿè¨˜éŒ„",
    }

    for name, config in schedule.items():
        task_name = config["task"]

        # Only include data processing tasks
        if task_name not in data_processing_task_names:
            continue

        # Get last run information from Redis cache
        last_run = None
        last_run_status = None
        last_run_result = None
        error_message = None

        try:
            if cache.is_available():
                history_key = f"task_history:{task_name}"
                history_data = cache.get(history_key)

                if history_data:
                    if isinstance(history_data, str):
                        history_data = json.loads(history_data)

                    last_run_str = history_data.get("last_run")
                    if last_run_str:
                        last_run = datetime.fromisoformat(last_run_str.replace('Z', '+00:00'))

                    last_run_status = history_data.get("status", "unknown")
                    last_run_result = history_data.get("result")
                    error_message = history_data.get("error")
        except Exception as e:
            logger.warning(f"Failed to get task history for {task_name}: {str(e)}")

        tasks.append(SyncTaskInfo(
            task_name=task_name,
            display_name=task_display_names.get(task_name, task_name),
            schedule=format_crontab_schedule(config["schedule"]),
            last_run=last_run,
            last_run_status=last_run_status,
            last_run_result=last_run_result,
            error_message=error_message,
            status="active",
        ))

    return tasks


@router.get("/monitoring/tasks", response_model=List[SyncTaskInfo])
async def list_monitoring_tasks(
    current_user: User = Depends(get_current_superuser),
):
    """Get all strategy processing tasks and their schedules (admin only)"""
    from app.utils.cache import cache
    import json

    schedule = celery_app.conf.beat_schedule
    tasks = []

    # ç­–ç•¥è™•ç†ä»»å‹™æ¸…å–®ï¼ˆè³‡æ–™åº« â†’ ç­–ç•¥å¼•æ“ â†’ ä¿¡è™Ÿï¼‰
    strategy_processing_task_names = [
        "app.tasks.monitor_active_strategies",
    ]

    task_display_names = {
        "app.tasks.monitor_active_strategies": "ğŸ”” ç›£æ§ç­–ç•¥ä¿¡è™Ÿï¼ˆå¯¦ç›¤ï¼‰",
    }

    for name, config in schedule.items():
        task_name = config["task"]

        # Only include strategy processing tasks
        if task_name not in strategy_processing_task_names:
            continue

        # Get last run information from Redis cache
        last_run = None
        last_run_status = None
        last_run_result = None
        error_message = None

        try:
            if cache.is_available():
                history_key = f"task_history:{task_name}"
                history_data = cache.get(history_key)

                if history_data:
                    if isinstance(history_data, str):
                        history_data = json.loads(history_data)

                    last_run_str = history_data.get("last_run")
                    if last_run_str:
                        last_run = datetime.fromisoformat(last_run_str.replace('Z', '+00:00'))

                    last_run_status = history_data.get("status", "unknown")
                    last_run_result = history_data.get("result")
                    error_message = history_data.get("error")
        except Exception as e:
            logger.warning(f"Failed to get task history for {task_name}: {str(e)}")

        tasks.append(SyncTaskInfo(
            task_name=task_name,
            display_name=task_display_names.get(task_name, task_name),
            schedule=format_crontab_schedule(config["schedule"]),
            last_run=last_run,
            last_run_status=last_run_status,
            last_run_result=last_run_result,
            error_message=error_message,
            status="active",
        ))

    return tasks


@router.get("/monitoring/stats")
async def get_monitoring_stats(
    current_user: User = Depends(get_current_superuser),
    admin_service: AdminService = Depends(get_admin_service),
):
    """Get strategy monitoring statistics (admin only)"""
    return admin_service.get_monitoring_stats()


@router.post("/sync/trigger")
async def trigger_sync_task(
    request: ManualSyncRequest,
    current_user: User = Depends(get_current_superuser),
):
    """Manually trigger a sync task (admin only)"""
    try:
        result = celery_app.send_task(
            request.task_name,
            kwargs=request.params or {}
        )

        logger.info(f"Admin {current_user.username} triggered task {request.task_name}")

        return {
            "message": "Task submitted successfully",
            "task_id": result.id,
            "task_name": request.task_name,
        }
    except Exception as e:
        logger.error(f"Failed to trigger task {request.task_name}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to trigger task: {str(e)}"
        )


@router.get("/sync/workers", response_model=List[CeleryWorkerInfo])
async def get_celery_workers(
    current_user: User = Depends(get_current_superuser),
):
    """Get Celery worker information (admin only)"""
    try:
        inspect = celery_app.control.inspect()
        stats = inspect.stats()

        if not stats:
            return []

        # Get current active tasks
        active_tasks_dict = inspect.active()

        workers = []
        for hostname, info in stats.items():
            # Calculate total processed tasks from task breakdown
            total_dict = info.get("total", {})
            total_processed = sum(total_dict.values()) if isinstance(total_dict, dict) else 0

            # Get current active task count (actually running now)
            current_active = len(active_tasks_dict.get(hostname, [])) if active_tasks_dict else 0

            # Get uptime in seconds
            uptime_seconds = int(info.get("uptime", 0))

            workers.append(CeleryWorkerInfo(
                hostname=hostname,
                status="active",
                current_active=current_active,
                total_processed=total_processed,
                uptime_seconds=uptime_seconds,
            ))

        return workers
    except Exception as e:
        logger.error(f"Failed to get Celery workers: {str(e)}")
        return []


@router.get("/sync/active-tasks", response_model=List[CeleryTaskStatus])
async def get_active_tasks(
    current_user: User = Depends(get_current_superuser),
):
    """Get currently running tasks (admin only)"""
    try:
        inspect = celery_app.control.inspect()
        active = inspect.active()

        if not active:
            return []

        tasks = []
        for worker, task_list in active.items():
            for task in task_list:
                tasks.append(CeleryTaskStatus(
                    task_id=task["id"],
                    task_name=task["name"],
                    status="running",
                ))

        return tasks
    except Exception as e:
        logger.error(f"Failed to get active tasks: {str(e)}")
        return []


# ============ Log Query ============

@router.post("/logs/query", response_model=LogQueryResponse)
async def query_logs(
    request: LogQueryRequest,
    current_user: User = Depends(get_current_superuser),
):
    """Query application logs (admin only)"""
    import subprocess
    import re
    import shutil

    try:
        # Check if docker command is available
        if not shutil.which("docker"):
            # Docker not available in container, return helpful message
            help_message = (
                "æ—¥èªŒæŸ¥è©¢åŠŸèƒ½éœ€è¦å¾ä¸»æ©ŸåŸ·è¡Œã€‚è«‹ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹æ—¥èªŒï¼š\n\n"
                "# æŸ¥çœ‹æ‰€æœ‰æ—¥èªŒ\n"
                "docker compose logs --tail 100\n\n"
                "# æŸ¥çœ‹ç‰¹å®šæœå‹™\n"
                "docker compose logs --tail 100 backend\n"
                "docker compose logs --tail 100 celery-worker\n\n"
                "# å³æ™‚è·Ÿè¹¤æ—¥èªŒ\n"
                "docker compose logs -f backend"
            )
            return LogQueryResponse(
                total=1,
                logs=[LogEntry(
                    timestamp=datetime.now(timezone.utc).isoformat(),
                    level="INFO",
                    module="system",
                    message=help_message
                )]
            )

        # Build docker logs command
        cmd = ["docker", "compose", "logs", "--tail", str(request.limit)]

        # Add service filter
        if request.module:
            if request.module == "backend":
                cmd.append("backend")
            elif request.module == "celery":
                cmd.extend(["celery-worker", "celery-beat"])
            elif request.module == "frontend":
                cmd.append("frontend")

        # Execute command
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=10
        )

        lines = result.stdout.split("\n")
        logs = []

        # Parse log lines
        for line in lines:
            if not line.strip():
                continue

            # Filter by level
            if request.level:
                if request.level.upper() not in line.upper():
                    continue

            # Filter by keyword
            if request.keyword:
                if request.keyword.lower() not in line.lower():
                    continue

            # Extract log components (basic parsing)
            timestamp_match = re.search(r'\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}', line)
            level_match = re.search(r'(DEBUG|INFO|WARNING|ERROR|CRITICAL)', line)

            logs.append(LogEntry(
                timestamp=timestamp_match.group(0) if timestamp_match else "Unknown",
                level=level_match.group(0) if level_match else "INFO",
                module=request.module or "Unknown",
                message=line,
            ))

        return LogQueryResponse(
            total=len(logs),
            logs=logs[:request.limit]
        )

    except Exception as e:
        logger.error(f"Failed to query logs: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ—¥èªŒæŸ¥è©¢å¤±æ•—ã€‚è«‹ä½¿ç”¨å‘½ä»¤è¡ŒæŸ¥çœ‹: docker compose logs --tail 100"
        )


# ============ Security Monitoring ============

@router.get("/security/events", response_model=SecurityEventsResponse)
async def get_security_events(
    event_type: Optional[str] = Query(None, description="éæ¿¾äº‹ä»¶é¡å‹ï¼šrate_limit, request_size_rejection, cache_tampering"),
    limit: int = Query(100, ge=1, le=1000, description="è¿”å›çš„æœ€å¤§äº‹ä»¶æ•¸"),
    current_user: User = Depends(get_current_superuser),
):
    """
    ç²å–å®‰å…¨äº‹ä»¶è¨˜éŒ„ï¼ˆéœ€ superuser æ¬Šé™ï¼‰

    è¿½è¹¤é€Ÿç‡é™åˆ¶ã€è«‹æ±‚éå¤§æ‹’çµ•ã€å¿«å–ç¯¡æ”¹ç­‰å®‰å…¨äº‹ä»¶
    """
    from app.middleware.monitoring import security_monitoring

    # ç²å–äº‹ä»¶
    events = security_monitoring.get_recent_events(limit=limit, event_type=event_type)

    # ç²å–çµ±è¨ˆè³‡è¨Š
    stats_dict = security_monitoring.get_stats()

    # è½‰æ›ç‚ºéŸ¿æ‡‰æ ¼å¼
    security_events = [SecurityEvent(**event) for event in events]

    # æ§‹å»ºçµ±è¨ˆè³‡è¨Š
    stats = SecurityStats(
        rate_limit_total=stats_dict.get("rate_limit_total", 0),
        request_size_rejection_total=stats_dict.get("request_size_rejection_total", 0),
        cache_tampering_total=stats_dict.get("cache_tampering_total", 0),
        endpoint_stats={
            k: v for k, v in stats_dict.items()
            if k.startswith("rate_limit_") and k != "rate_limit_total"
        }
    )

    return SecurityEventsResponse(
        total=len(events),
        events=security_events,
        stats=stats
    )


@router.get("/security/stats", response_model=SecurityStats)
async def get_security_stats(
    current_user: User = Depends(get_current_superuser),
):
    """
    ç²å–å®‰å…¨çµ±è¨ˆè³‡è¨Šï¼ˆéœ€ superuser æ¬Šé™ï¼‰

    è¿”å›é€Ÿç‡é™åˆ¶ã€è«‹æ±‚æ‹’çµ•ç­‰çµ±è¨ˆæ•¸æ“š
    """
    from app.middleware.monitoring import security_monitoring

    stats_dict = security_monitoring.get_stats()

    return SecurityStats(
        rate_limit_total=stats_dict.get("rate_limit_total", 0),
        request_size_rejection_total=stats_dict.get("request_size_rejection_total", 0),
        cache_tampering_total=stats_dict.get("cache_tampering_total", 0),
        endpoint_stats={
            k: v for k, v in stats_dict.items()
            if k.startswith("rate_limit_") and k != "rate_limit_total"
        }
    )


@router.post("/security/events/clear")
async def clear_security_events(
    keep_last: int = Query(1000, ge=0, description="ä¿ç•™çš„æœ€è¿‘äº‹ä»¶æ•¸é‡"),
    current_user: User = Depends(get_current_superuser),
):
    """
    æ¸…ç†èˆŠçš„å®‰å…¨äº‹ä»¶ï¼ˆéœ€ superuser æ¬Šé™ï¼‰

    ä¿ç•™æœ€è¿‘ N å€‹äº‹ä»¶ï¼Œåˆªé™¤è¼ƒèˆŠçš„è¨˜éŒ„
    """
    from app.middleware.monitoring import security_monitoring

    security_monitoring.clear_old_events(keep_last=keep_last)

    logger.info(f"Admin {current_user.username} cleared old security events, keeping last {keep_last}")

    return {
        "message": "èˆŠäº‹ä»¶å·²æ¸…ç†",
        "kept_events": keep_last
    }
