"""
Stock Data API Routes
Handles stock data fetching from database and FinLab API
"""

from fastapi import APIRouter, Depends, HTTPException, status, Query
from typing import Optional, List
from sqlalchemy.orm import Session
from datetime import datetime, date as date_type
from decimal import Decimal
from app.schemas.stock import (
    StockInfo,
    StockSearchRequest,
    StockSearchResult,
    StockDataResponse,
    LatestPriceResponse,
)
from app.schemas.fundamental import (
    FundamentalIndicatorResponse,
    FundamentalIndicatorBatchRequest,
    FundamentalIndicatorBatchResponse,
    FundamentalIndicatorListResponse,
    FundamentalIndicatorCategoryResponse,
    FundamentalIndicatorInfo,
    FundamentalDataPoint,
    FundamentalSummary,
    FundamentalComparisonRequest,
    FundamentalComparisonResponse,
)
from app.schemas.stock_price import StockPriceCreate
from app.services.finlab_client import FinLabClient
from app.repositories.stock import StockRepository
from app.repositories.stock_price import StockPriceRepository
from app.db.session import get_db
from app.utils.cache import cache
from loguru import logger
import pandas as pd

router = APIRouter()


def get_finlab_client() -> FinLabClient:
    """
    Get FinLab client instance

    Uses the global token from settings
    """
    client = FinLabClient()

    if not client.is_available():
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="FinLab API service is not available. Please check API token configuration.",
        )

    return client


@router.get("/stocks", response_model=List[StockInfo])
async def get_stock_list(
    db: Session = Depends(get_db),
    skip: int = Query(0, ge=0),
    limit: int = Query(1000, ge=1, le=10000),
):
    """
    å–å¾—æ‰€æœ‰å°è‚¡åˆ—è¡¨ï¼ˆå¾è³‡æ–™åº«è®€å–ï¼‰

    Returns:
        æ‰€æœ‰å°è‚¡çš„åŸºæœ¬è³‡è¨Šåˆ—è¡¨
    """
    try:
        # Try cache first
        cache_key = f"stock_list:db:{skip}:{limit}"
        cached_data = cache.get(cache_key)

        if cached_data is not None:
            logger.info("Returning cached stock list from database")
            return cached_data

        # Fetch from database
        stocks = StockRepository.get_all(db, skip=skip, limit=limit, is_active='active')

        # Convert to response format
        result = [
            StockInfo(
                stock_id=stock.stock_id,
                name=stock.name,
                industry=stock.category,
                market=stock.market,
            )
            for stock in stocks
        ]

        # Cache for 1 hour
        cache.set(cache_key, result, expiry=3600)

        logger.info(f"Returned {len(result)} stocks from database")
        return result

    except Exception as e:
        logger.error(f"Failed to get stock list: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch stock list: {str(e)}",
        )


@router.post("/stocks/search", response_model=StockSearchResult)
async def search_stocks(
    request: StockSearchRequest,
    db: Session = Depends(get_db),
):
    """
    æœå°‹è‚¡ç¥¨ï¼ˆå¾è³‡æ–™åº«æœå°‹ï¼‰

    Args:
        request: æœå°‹è«‹æ±‚ï¼ˆåŒ…å«é—œéµå­—ï¼‰

    Returns:
        ç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨åˆ—è¡¨
    """
    try:
        # Search in database
        stocks = StockRepository.search(db, request.keyword, skip=0, limit=50)

        stock_infos = [
            StockInfo(
                stock_id=stock.stock_id,
                name=stock.name,
                industry=stock.category,
                market=stock.market,
            )
            for stock in stocks
        ]

        logger.info(f"Found {len(stock_infos)} stocks matching '{request.keyword}'")

        return StockSearchResult(
            results=stock_infos,
            count=len(stock_infos),
        )

    except Exception as e:
        logger.error(f"Failed to search stocks: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Stock search failed: {str(e)}",
        )


@router.get("/price/{stock_id}", response_model=StockDataResponse)
async def get_stock_price(
    stock_id: str,
    start_date: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    finlab_client: FinLabClient = Depends(get_finlab_client),
):
    """
    å–å¾—è‚¡ç¥¨åƒ¹æ ¼è³‡æ–™

    Args:
        stock_id: è‚¡ç¥¨ä»£ç¢¼
        start_date: é–‹å§‹æ—¥æœŸ
        end_date: çµæŸæ—¥æœŸ

    Returns:
        è‚¡ç¥¨åƒ¹æ ¼è³‡æ–™
    """
    try:
        # Try cache first
        cache_key = f"price:{stock_id}:{start_date}:{end_date}"
        cached_data = cache.get(cache_key)

        if cached_data is not None:
            logger.info(f"Returning cached price data for {stock_id}")
            return StockDataResponse(
                stock_id=stock_id,
                data=cached_data,
                cached=True,
            )

        # Fetch from FinLab
        price_df = finlab_client.get_price(
            stock_id=stock_id,
            start_date=start_date,
            end_date=end_date,
        )

        # Convert DataFrame to dict
        data = {
            str(date): float(price)
            for date, price in price_df[stock_id].items()
            if pd.notna(price)
        }

        # Cache for 10 minutes
        cache.set(cache_key, data, expiry=600)

        return StockDataResponse(
            stock_id=stock_id,
            data=data,
            cached=False,
        )

    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=str(e),
        )
    except Exception as e:
        logger.error(f"Failed to get price data: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch price data: {str(e)}",
        )


@router.get("/ohlcv/{stock_id}", response_model=StockDataResponse)
async def get_stock_ohlcv(
    stock_id: str,
    start_date: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    db: Session = Depends(get_db),
    finlab_client: FinLabClient = Depends(get_finlab_client),
):
    """
    å–å¾—è‚¡ç¥¨ OHLCV è³‡æ–™ï¼ˆå„ªå…ˆå¾è³‡æ–™åº«è®€å–ï¼‰

    Args:
        stock_id: è‚¡ç¥¨ä»£ç¢¼
        start_date: é–‹å§‹æ—¥æœŸ
        end_date: çµæŸæ—¥æœŸ

    Returns:
        OHLCV è³‡æ–™ (é–‹ç›¤/æœ€é«˜/æœ€ä½/æ”¶ç›¤/æˆäº¤é‡)
    """
    try:
        # Try cache first
        cache_key = f"ohlcv:{stock_id}:{start_date}:{end_date}"
        cached_data = cache.get(cache_key)

        if cached_data is not None:
            logger.info(f"Returning cached OHLCV data for {stock_id}")
            return StockDataResponse(
                stock_id=stock_id,
                data=cached_data,
                cached=True,
            )

        # Try database
        try:
            start = datetime.strptime(start_date, "%Y-%m-%d").date() if start_date else None
            end = datetime.strptime(end_date, "%Y-%m-%d").date() if end_date else None

            prices = StockPriceRepository.get_by_stock(
                db, stock_id, start_date=start, end_date=end, skip=0, limit=10000
            )

            if prices:
                # Convert to dict
                data = {
                    str(price.date): {
                        'open': float(price.open),
                        'high': float(price.high),
                        'low': float(price.low),
                        'close': float(price.close),
                        'volume': int(price.volume),
                    }
                    for price in prices
                }

                # Cache for 10 minutes
                cache.set(cache_key, data, expiry=600)

                logger.info(f"Returned {len(data)} OHLCV records from database for {stock_id}")
                return StockDataResponse(
                    stock_id=stock_id,
                    data=data,
                    cached=False,
                )
        except Exception as db_error:
            logger.warning(f"Database query failed, falling back to FinLab: {str(db_error)}")

        # Fallback to FinLab if database is empty
        ohlcv_df = finlab_client.get_ohlcv(
            stock_id=stock_id,
            start_date=start_date,
            end_date=end_date,
        )

        # Convert DataFrame to dict
        data = {
            str(date): {
                'open': float(row['open']) if pd.notna(row['open']) else None,
                'high': float(row['high']) if pd.notna(row['high']) else None,
                'low': float(row['low']) if pd.notna(row['low']) else None,
                'close': float(row['close']) if pd.notna(row['close']) else None,
                'volume': int(row['volume']) if pd.notna(row['volume']) else None,
            }
            for date, row in ohlcv_df.iterrows()
        }

        # Save to database for future use
        try:
            for date, row_data in data.items():
                price_create = StockPriceCreate(
                    stock_id=stock_id,
                    date=datetime.strptime(date, "%Y-%m-%d").date(),
                    open=Decimal(str(row_data['open'])) if row_data['open'] else Decimal('0'),
                    high=Decimal(str(row_data['high'])) if row_data['high'] else Decimal('0'),
                    low=Decimal(str(row_data['low'])) if row_data['low'] else Decimal('0'),
                    close=Decimal(str(row_data['close'])) if row_data['close'] else Decimal('0'),
                    volume=row_data['volume'] if row_data['volume'] else 0,
                    adj_close=None
                )
                StockPriceRepository.upsert(db, price_create)
            logger.info(f"Saved {len(data)} OHLCV records to database for {stock_id}")
        except Exception as save_error:
            logger.warning(f"Failed to save to database: {str(save_error)}")

        # Cache for 10 minutes
        cache.set(cache_key, data, expiry=600)

        logger.info(f"Returned {len(data)} OHLCV records from FinLab for {stock_id}")
        return StockDataResponse(
            stock_id=stock_id,
            data=data,
            cached=False,
        )

    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=str(e),
        )
    except Exception as e:
        logger.error(f"Failed to get OHLCV data: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch OHLCV data: {str(e)}",
        )


@router.get("/latest-price/{stock_id}", response_model=LatestPriceResponse)
async def get_latest_price(
    stock_id: str,
    finlab_client: FinLabClient = Depends(get_finlab_client),
):
    """
    å–å¾—è‚¡ç¥¨æœ€æ–°åƒ¹æ ¼

    Args:
        stock_id: è‚¡ç¥¨ä»£ç¢¼

    Returns:
        æœ€æ–°æ”¶ç›¤åƒ¹
    """
    try:
        # Try cache first (cache for 5 minutes)
        cache_key = f"latest_price:{stock_id}"
        cached_price = cache.get(cache_key)

        if cached_price is not None:
            logger.info(f"Returning cached latest price for {stock_id}")
            return LatestPriceResponse(
                stock_id=stock_id,
                price=cached_price,
            )

        # Fetch from FinLab
        price = finlab_client.get_latest_price(stock_id)

        if price is None:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Stock {stock_id} not found or no price data available",
            )

        # Cache for 5 minutes
        cache.set(cache_key, price, expiry=300)

        return LatestPriceResponse(
            stock_id=stock_id,
            price=price,
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get latest price: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch latest price: {str(e)}",
        )


@router.delete("/cache/clear")
async def clear_cache(
    pattern: Optional[str] = Query(None, description="Cache key pattern to clear (e.g., 'price:*')"),
):
    """
    æ¸…é™¤å¿«å–

    Args:
        pattern: è¦æ¸…é™¤çš„å¿«å–éµæ¨¡å¼ï¼ˆå¯é¸ï¼‰

    Returns:
        æ¸…é™¤çš„å¿«å–æ•¸é‡
    """
    try:
        if pattern:
            count = cache.clear_pattern(pattern)
        else:
            # Clear all stock data cache
            count = cache.clear_pattern("stock:*")
            count += cache.clear_pattern("price:*")
            count += cache.clear_pattern("ohlcv:*")
            count += cache.clear_pattern("latest_price:*")
            count += cache.clear_pattern("fundamental:*")

        return {
            "message": f"Cleared {count} cache entries",
            "count": count,
        }

    except Exception as e:
        logger.error(f"Failed to clear cache: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to clear cache: {str(e)}",
        )


# ============ Fundamental Analysis Endpoints ============


@router.get("/fundamental/indicators", response_model=FundamentalIndicatorListResponse)
async def list_fundamental_indicators():
    """
    åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„è²¡å‹™æŒ‡æ¨™

    Returns:
        è²¡å‹™æŒ‡æ¨™åˆ—è¡¨ï¼ŒæŒ‰é¡åˆ¥åˆ†çµ„
    """
    try:
        # Get indicators by category
        categories = FinLabClient.get_fundamental_indicator_categories()

        # Create indicator info list
        indicator_infos = []
        category_map = {
            "profitability": "ç²åˆ©èƒ½åŠ›",
            "growth": "æˆé•·æ€§",
            "efficiency": "ç¶“ç‡Ÿæ•ˆç‡",
            "financial_structure": "è²¡å‹™çµæ§‹",
            "per_share": "æ¯è‚¡æŒ‡æ¨™",
        }

        name_en_map = {
            "ROEç¨…å¾Œ": "ROE (After Tax)",
            "ROAç¨…å¾Œæ¯å‰": "ROA (After Tax, Before Interest)",
            "ç‡Ÿæ¥­æ¯›åˆ©ç‡": "Operating Gross Margin",
            "ç‡Ÿæ¥­åˆ©ç›Šç‡": "Operating Profit Margin",
            "ç¨…å‰æ·¨åˆ©ç‡": "Pre-tax Net Profit Margin",
            "ç¨…å¾Œæ·¨åˆ©ç‡": "After-tax Net Profit Margin",
            "ç‡Ÿæ”¶æˆé•·ç‡": "Revenue Growth Rate",
            "ç¨…å‰æ·¨åˆ©æˆé•·ç‡": "Pre-tax Net Profit Growth Rate",
            "ç¨…å¾Œæ·¨åˆ©æˆé•·ç‡": "After-tax Net Profit Growth Rate",
            "æ‡‰æ”¶å¸³æ¬¾é€±è½‰ç‡": "Accounts Receivable Turnover",
            "å­˜è²¨é€±è½‰ç‡": "Inventory Turnover",
            "ç¸½è³‡ç”¢é€±è½‰ç‡": "Total Asset Turnover",
            "è² å‚µæ¯”ç‡": "Debt Ratio",
            "æµå‹•æ¯”ç‡": "Current Ratio",
            "é€Ÿå‹•æ¯”ç‡": "Quick Ratio",
            "æ¯è‚¡æ·¨å€¼": "Book Value Per Share",
            "æ¯è‚¡ç›ˆé¤˜": "Earnings Per Share (EPS)",
            "æ¯è‚¡ç‡Ÿæ¥­é¡": "Revenue Per Share",
        }

        for category, indicators in categories.items():
            for indicator in indicators:
                indicator_infos.append(
                    FundamentalIndicatorInfo(
                        name=indicator,
                        name_en=name_en_map.get(indicator, indicator),
                        category=category_map.get(category, category),
                    )
                )

        # Calculate category counts
        category_counts = {
            category_map.get(cat, cat): len(indicators)
            for cat, indicators in categories.items()
        }

        logger.info(f"Returned {len(indicator_infos)} fundamental indicators")

        return FundamentalIndicatorListResponse(
            indicators=indicator_infos,
            count=len(indicator_infos),
            categories=category_counts,
        )

    except Exception as e:
        logger.error(f"Failed to list fundamental indicators: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to list indicators: {str(e)}",
        )


@router.get("/fundamental/indicators/categories", response_model=FundamentalIndicatorCategoryResponse)
async def get_fundamental_indicator_categories():
    """
    å–å¾—æŒ‰é¡åˆ¥åˆ†çµ„çš„è²¡å‹™æŒ‡æ¨™

    Returns:
        åˆ†é¡èˆ‡æŒ‡æ¨™çš„æ˜ å°„
    """
    try:
        categories = FinLabClient.get_fundamental_indicator_categories()
        total_count = sum(len(indicators) for indicators in categories.values())

        logger.info(f"Returned {total_count} indicators in {len(categories)} categories")

        return FundamentalIndicatorCategoryResponse(
            categories=categories,
            total_count=total_count,
        )

    except Exception as e:
        logger.error(f"Failed to get indicator categories: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get categories: {str(e)}",
        )


@router.get("/fundamental/{stock_id}/{indicator}", response_model=FundamentalIndicatorResponse)
async def get_fundamental_indicator(
    stock_id: str,
    indicator: str,
    start_date: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    db: Session = Depends(get_db),
):
    """
    å–å¾—è‚¡ç¥¨çš„ç‰¹å®šè²¡å‹™æŒ‡æ¨™æ•¸æ“š

    ä½¿ç”¨ä¸‰å±¤ç·©å­˜æ¶æ§‹:
    L1: Redis (24å°æ™‚) - æœ€å¿«
    L2: PostgreSQL (æ°¸ä¹…) - æŒä¹…åŒ–
    L3: FinLab API - æ•¸æ“šæº

    Args:
        stock_id: è‚¡ç¥¨ä»£è™Ÿ
        indicator: è²¡å‹™æŒ‡æ¨™åç¨±ï¼ˆå¦‚ 'ROEç¨…å¾Œ'ï¼‰
        start_date: é–‹å§‹æ—¥æœŸ
        end_date: çµæŸæ—¥æœŸ

    Returns:
        è²¡å‹™æŒ‡æ¨™æ™‚é–“åºåˆ—æ•¸æ“š
    """
    try:
        # L1 Cache: Try Redis first
        cache_key = f"fundamental:{stock_id}:{indicator}:{start_date}:{end_date}"
        cached_data = cache.get(cache_key)

        if cached_data is not None:
            logger.info(f"âœ… L1 Cache HIT (Redis): {stock_id} - {indicator}")
            return cached_data

        # L2 Cache + L3 Source: Use FundamentalService
        from app.services.fundamental_service import FundamentalService
        service = FundamentalService(db)

        data_points = service.get_indicator_data(
            stock_id=stock_id,
            indicator=indicator,
            start_date=start_date,
            end_date=end_date,
            force_refresh=False  # Allow DB cache
        )

        response = FundamentalIndicatorResponse(
            indicator=indicator,
            stock_id=stock_id,
            data=data_points,
            count=len(data_points),
            start_date=data_points[0].date if data_points else None,
            end_date=data_points[-1].date if data_points else None,
        )

        # Cache in Redis for 24 hours (financial data changes quarterly)
        cache.set(cache_key, response, expiry=86400)
        logger.info(f"ğŸ’¾ Cached in Redis: {stock_id} - {indicator} ({len(data_points)} points)")

        return response

    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=str(e),
        )
    except Exception as e:
        logger.error(f"Failed to get fundamental indicator: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch fundamental data: {str(e)}",
        )


@router.post("/fundamental/{stock_id}/batch", response_model=FundamentalIndicatorBatchResponse)
async def get_fundamental_indicators_batch(
    stock_id: str,
    request: FundamentalIndicatorBatchRequest,
    db: Session = Depends(get_db),
):
    """
    æ‰¹é‡å–å¾—è‚¡ç¥¨çš„å¤šå€‹è²¡å‹™æŒ‡æ¨™

    ä½¿ç”¨ä¸‰å±¤ç·©å­˜æ¶æ§‹:
    L1: Redis (24å°æ™‚) - æœ€å¿«
    L2: PostgreSQL (æ°¸ä¹…) - æŒä¹…åŒ–
    L3: FinLab API - æ•¸æ“šæº

    Args:
        stock_id: è‚¡ç¥¨ä»£è™Ÿ
        request: æ‰¹é‡è«‹æ±‚åƒæ•¸ï¼ˆåŒ…å«æŒ‡æ¨™åˆ—è¡¨ã€æ—¥æœŸç¯„åœï¼‰

    Returns:
        å¤šå€‹è²¡å‹™æŒ‡æ¨™çš„æ•¸æ“š
    """
    try:
        # L1 Cache: Try Redis first
        indicators_str = ",".join(sorted(request.indicators)) if request.indicators else "default"
        cache_key = f"fundamental_batch:{stock_id}:{indicators_str}:{request.start_date}:{request.end_date}"
        cached_data = cache.get(cache_key)

        if cached_data is not None:
            logger.info(f"âœ… L1 Cache HIT (Redis) for batch: {stock_id}")
            return cached_data

        # L2 Cache + L3 Source: Use FundamentalService
        from app.services.fundamental_service import FundamentalService
        service = FundamentalService(db)

        # Determine which indicators to fetch
        indicators_to_fetch = request.indicators if request.indicators else FinLabClient.get_common_fundamental_indicators()

        indicators_dict = service.get_indicators_batch(
            stock_id=stock_id,
            indicators=indicators_to_fetch,
            start_date=request.start_date,
            end_date=request.end_date,
            force_refresh=False  # Allow DB cache
        )

        requested_count = len(indicators_to_fetch)

        response = FundamentalIndicatorBatchResponse(
            stock_id=stock_id,
            indicators=indicators_dict,
            count=len(indicators_dict),
            requested_count=requested_count,
            start_date=request.start_date,
            end_date=request.end_date,
        )

        # Cache in Redis for 24 hours
        cache.set(cache_key, response, expiry=86400)
        logger.info(f"ğŸ’¾ Cached batch in Redis: {stock_id} ({len(indicators_dict)}/{requested_count} indicators)")

        return response

    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=str(e),
        )
    except Exception as e:
        logger.error(f"Failed to get batch fundamental indicators: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch batch fundamental data: {str(e)}",
        )


@router.get("/fundamental/{stock_id}/summary", response_model=FundamentalSummary)
async def get_fundamental_summary(
    stock_id: str,
    finlab_client: FinLabClient = Depends(get_finlab_client),
):
    """
    å–å¾—è‚¡ç¥¨çš„è²¡å‹™æŒ‡æ¨™æ‘˜è¦ï¼ˆæœ€æ–°æ•¸æ“šï¼‰

    Args:
        stock_id: è‚¡ç¥¨ä»£è™Ÿ

    Returns:
        è²¡å‹™æŒ‡æ¨™æ‘˜è¦
    """
    try:
        # Try cache first
        cache_key = f"fundamental_summary:{stock_id}"
        cached_data = cache.get(cache_key)

        if cached_data is not None:
            logger.info(f"Returning cached fundamental summary for {stock_id}")
            return cached_data

        # Fetch common indicators
        indicators_data = finlab_client.get_fundamental_indicators_batch(
            stock_id=stock_id,
            indicators=None,  # Use default common indicators
        )

        # Extract latest values
        latest_date = None
        summary_data = {
            "stock_id": stock_id,
        }

        indicator_mapping = {
            "ROEç¨…å¾Œ": "roe",
            "ROAç¨…å¾Œæ¯å‰": "roa",
            "ç‡Ÿæ¥­æ¯›åˆ©ç‡": "gross_margin",
            "ç‡Ÿæ¥­åˆ©ç›Šç‡": "operating_margin",
            "ç¨…å¾Œæ·¨åˆ©ç‡": "net_margin",
            "ç‡Ÿæ”¶æˆé•·ç‡": "revenue_growth",
            "ç¨…å¾Œæ·¨åˆ©æˆé•·ç‡": "profit_growth",
            "æ‡‰æ”¶å¸³æ¬¾é€±è½‰ç‡": "receivable_turnover",
            "å­˜è²¨é€±è½‰ç‡": "inventory_turnover",
            "ç¸½è³‡ç”¢é€±è½‰ç‡": "asset_turnover",
            "è² å‚µæ¯”ç‡": "debt_ratio",
            "æµå‹•æ¯”ç‡": "current_ratio",
            "é€Ÿå‹•æ¯”ç‡": "quick_ratio",
            "æ¯è‚¡æ·¨å€¼": "book_value_per_share",
            "æ¯è‚¡ç›ˆé¤˜": "eps",
            "æ¯è‚¡ç‡Ÿæ¥­é¡": "revenue_per_share",
        }

        for indicator_name, data_df in indicators_data.items():
            if stock_id in data_df.columns:
                # Get latest non-null value
                latest_value = data_df[stock_id].dropna().iloc[-1] if len(data_df[stock_id].dropna()) > 0 else None
                latest_idx = data_df[stock_id].dropna().index[-1] if len(data_df[stock_id].dropna()) > 0 else None

                if latest_idx and (latest_date is None or latest_idx > latest_date):
                    latest_date = latest_idx

                # Map to summary field
                field_name = indicator_mapping.get(indicator_name)
                if field_name and latest_value is not None:
                    summary_data[field_name] = float(latest_value)

        summary_data["latest_date"] = str(latest_date) if latest_date else None

        response = FundamentalSummary(**summary_data)

        # Cache for 1 hour
        cache.set(cache_key, response, expiry=3600)

        logger.info(f"Returned fundamental summary for {stock_id}")
        return response

    except Exception as e:
        logger.error(f"Failed to get fundamental summary: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch fundamental summary: {str(e)}",
        )


@router.post("/fundamental/compare", response_model=FundamentalComparisonResponse)
async def compare_fundamental_indicators(
    request: FundamentalComparisonRequest,
    finlab_client: FinLabClient = Depends(get_finlab_client),
):
    """
    æ¯”è¼ƒå¤šå€‹è‚¡ç¥¨çš„ç‰¹å®šè²¡å‹™æŒ‡æ¨™

    Args:
        request: æ¯”è¼ƒè«‹æ±‚åƒæ•¸ï¼ˆè‚¡ç¥¨åˆ—è¡¨ã€æŒ‡æ¨™ã€æ—¥æœŸç¯„åœï¼‰

    Returns:
        å„è‚¡ç¥¨çš„æŒ‡æ¨™æ•¸æ“šå°æ¯”
    """
    try:
        # Try cache first
        stock_ids_str = ",".join(sorted(request.stock_ids))
        cache_key = f"fundamental_compare:{stock_ids_str}:{request.indicator}:{request.start_date}:{request.end_date}"
        cached_data = cache.get(cache_key)

        if cached_data is not None:
            logger.info(f"Returning cached fundamental comparison")
            return cached_data

        # Fetch data for each stock
        stocks_data = {}
        for stock_id in request.stock_ids:
            try:
                data_df = finlab_client.get_fundamental_indicator(
                    indicator=request.indicator,
                    stock_id=stock_id,
                    start_date=request.start_date,
                    end_date=request.end_date,
                )

                data_points = [
                    FundamentalDataPoint(
                        date=str(date),
                        value=float(value) if pd.notna(value) else None
                    )
                    for date, value in data_df[stock_id].items()
                ]

                stocks_data[stock_id] = data_points

            except Exception as e:
                logger.warning(f"Failed to get {request.indicator} for {stock_id}: {str(e)}")
                continue

        response = FundamentalComparisonResponse(
            indicator=request.indicator,
            stocks=stocks_data,
            count=len(stocks_data),
            start_date=request.start_date,
            end_date=request.end_date,
        )

        # Cache for 1 hour
        cache.set(cache_key, response, expiry=3600)

        logger.info(f"Returned comparison for {len(stocks_data)}/{len(request.stock_ids)} stocks")
        return response

    except Exception as e:
        logger.error(f"Failed to compare fundamental indicators: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to compare fundamental data: {str(e)}",
        )
