#!/usr/bin/env python3
"""
å°‡ PostgreSQL åˆ†é˜ç·šæ•¸æ“šè½‰æ›ç‚º Qlib æ ¼å¼

åŠŸèƒ½ï¼š
1. å¾ stock_minute_prices è¡¨è®€å–åˆ†é˜ç·šæ•¸æ“š
2. è½‰æ›ç‚º Qlib å®˜æ–¹æ ¼å¼ï¼ˆFileFeatureStorage APIï¼‰
3. æ”¯æ´æ™ºæ…§å¢é‡åŒæ­¥
4. æ¯”å¾ Shioaji API ä¸‹è¼‰å¿« 10-100 å€

ä½¿ç”¨ç¯„ä¾‹ï¼š
    # ğŸ§  æ™ºæ…§å¢é‡è½‰æ›ï¼ˆæ¨è–¦ï¼‰
    python export_minute_to_qlib.py --output-dir /data/qlib/tw_stock_minute --smart

    # å®Œæ•´è½‰æ›æ‰€æœ‰æ•¸æ“š
    python export_minute_to_qlib.py --output-dir /data/qlib/tw_stock_minute --stocks all

    # æ¸¬è©¦æ¨¡å¼ï¼ˆåƒ…è½‰æ› 10 æª”ï¼‰
    python export_minute_to_qlib.py --output-dir /data/qlib/tw_stock_minute --test
"""

import sys
import os
from pathlib import Path
from datetime import datetime, date, timedelta, time as dt_time
from typing import List, Optional, Tuple
import argparse

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd
import numpy as np
from loguru import logger
from tqdm import tqdm
from sqlalchemy import create_engine, text

from app.core.config import settings

# Qlib æ¨¡çµ„
import qlib
from qlib.config import REG_CN
from qlib.data.storage.file_storage import FileFeatureStorage
from qlib.data import D

# Qlib ç‰¹å¾µåˆ—è¡¨ï¼ˆåˆ†é˜ç·šï¼‰
QLIB_MINUTE_FEATURES = ['open', 'high', 'low', 'close', 'volume']

# æ—¥èªŒé…ç½®
logger.remove()
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
    level="INFO"
)
logger.add(
    "/tmp/export_minute_to_qlib_{time}.log",
    rotation="1 day",
    retention="7 days",
    level="DEBUG"
)


def get_db_engine():
    """å»ºç«‹è³‡æ–™åº«é€£æ¥"""
    return create_engine(settings.DATABASE_URL)


def get_all_stock_ids(engine) -> List[str]:
    """ç²å–æ‰€æœ‰æœ‰åˆ†é˜ç·šæ•¸æ“šçš„è‚¡ç¥¨ä»£ç¢¼"""
    with engine.connect() as conn:
        result = conn.execute(text("""
            SELECT DISTINCT stock_id
            FROM stock_minute_prices
            ORDER BY stock_id
        """))
        return [row[0] for row in result.fetchall()]


def get_all_trading_minutes(engine) -> pd.DatetimeIndex:
    """ç²å–æ‰€æœ‰äº¤æ˜“åˆ†é˜"""
    with engine.connect() as conn:
        result = conn.execute(text("""
            SELECT DISTINCT datetime
            FROM stock_minute_prices
            ORDER BY datetime ASC
        """))
        datetimes = [row[0] for row in result.fetchall()]
        return pd.DatetimeIndex(datetimes)


def get_db_date_range(engine, stock_id: str) -> Tuple[Optional[date], Optional[date]]:
    """ç²å–è³‡æ–™åº«ä¸­è©²è‚¡ç¥¨çš„æ—¥æœŸç¯„åœ"""
    with engine.connect() as conn:
        result = conn.execute(text("""
            SELECT MIN(datetime::date) as min_date, MAX(datetime::date) as max_date
            FROM stock_minute_prices
            WHERE stock_id = :stock_id
        """), {"stock_id": stock_id})
        row = result.fetchone()
        if row and row[0] and row[1]:
            return (row[0], row[1])
        return (None, None)


def get_qlib_last_date(stock_id: str) -> Optional[date]:
    """ç²å– Qlib ä¸­è©²è‚¡ç¥¨çš„æœ€å¾Œæ—¥æœŸ"""
    try:
        df = D.features([stock_id], ['$close'], freq='1min')
        if df is None or df.empty:
            return None
        last_datetime = df.index.get_level_values('datetime').max()
        return last_datetime.date()
    except Exception:
        return None


def determine_sync_range(
    engine,
    stock_id: str,
    smart_mode: bool = False
) -> Tuple[Optional[date], Optional[date], str]:
    """
    æ™ºæ…§åˆ¤æ–·éœ€è¦åŒæ­¥çš„æ—¥æœŸç¯„åœ

    Returns:
        (é–‹å§‹æ—¥æœŸ, çµæŸæ—¥æœŸ, åŒæ­¥é¡å‹)
        åŒæ­¥é¡å‹: 'full', 'incremental', 'skip'
    """
    # ç²å–è³‡æ–™åº«æ—¥æœŸç¯„åœ
    db_min_date, db_max_date = get_db_date_range(engine, stock_id)

    if not db_min_date or not db_max_date:
        return (None, None, 'skip')

    # éæ™ºæ…§æ¨¡å¼ï¼Œå®Œæ•´åŒæ­¥
    if not smart_mode:
        return (db_min_date, db_max_date, 'full')

    # æª¢æŸ¥ Qlib å·²æœ‰æ•¸æ“š
    qlib_last_date = get_qlib_last_date(stock_id)

    if not qlib_last_date:
        # é¦–æ¬¡è½‰æ›ï¼Œå®Œæ•´åŒæ­¥
        return (db_min_date, db_max_date, 'full')

    # æª¢æŸ¥æ˜¯å¦æœ‰æ–°æ•¸æ“š
    if qlib_last_date >= db_max_date:
        # å·²æ˜¯æœ€æ–°ï¼Œè·³é
        return (None, None, 'skip')

    # å¢é‡åŒæ­¥ï¼ˆå¾ Qlib æœ€å¾Œæ—¥æœŸçš„ä¸‹ä¸€å¤©é–‹å§‹ï¼‰
    incremental_start = qlib_last_date + timedelta(days=1)
    return (incremental_start, db_max_date, 'incremental')


def create_calendar_file(output_dir: Path, trading_minutes: pd.DatetimeIndex):
    """å»ºç«‹ Qlib äº¤æ˜“åˆ†é˜æ—¥æ›†æª”æ¡ˆ"""
    cal_file = output_dir / 'calendars' / '1min.txt'
    cal_file.parent.mkdir(parents=True, exist_ok=True)

    with open(cal_file, 'w') as f:
        for dt in trading_minutes:
            f.write(dt.strftime('%Y-%m-%d %H:%M:%S') + '\n')

    logger.info(f"âœ… äº¤æ˜“åˆ†é˜æ—¥æ›†: {len(trading_minutes)} å€‹äº¤æ˜“åˆ†é˜")
    logger.info(f"   ç¯„åœ: {trading_minutes[0]} è‡³ {trading_minutes[-1]}")


def fetch_stock_minute_data(
    engine,
    stock_id: str,
    start_date: Optional[date] = None,
    end_date: Optional[date] = None
) -> pd.DataFrame:
    """å¾è³‡æ–™åº«ç²å–è‚¡ç¥¨åˆ†é˜æ•¸æ“š"""
    query = """
        SELECT
            datetime,
            open,
            high,
            low,
            close,
            volume
        FROM stock_minute_prices
        WHERE stock_id = :stock_id
    """

    params = {'stock_id': stock_id}

    if start_date:
        query += " AND datetime::date >= :start_date"
        params['start_date'] = start_date

    if end_date:
        query += " AND datetime::date <= :end_date"
        params['end_date'] = end_date

    query += " ORDER BY datetime ASC"

    df = pd.read_sql(text(query), engine, params=params)

    # è™•ç†ç¼ºå¤±å€¼
    if not df.empty:
        df = df.ffill().bfill()

    return df


def export_stock_to_qlib(
    stock_id: str,
    df: pd.DataFrame,
    trading_minutes: pd.DatetimeIndex
):
    """ä½¿ç”¨ Qlib FileFeatureStorage å°å‡ºè‚¡ç¥¨æ•¸æ“š"""
    instrument = stock_id.lower()

    # å°‡ DataFrame å°é½Šåˆ°å®Œæ•´äº¤æ˜“åˆ†é˜ç´¢å¼•
    df = df.set_index('datetime')
    df = df.reindex(trading_minutes)

    # ç‚ºæ¯å€‹ç‰¹å¾µå¯«å…¥æ•¸æ“š
    for field in QLIB_MINUTE_FEATURES:
        if field not in df.columns:
            continue

        # æå–ç‰¹å¾µæ•¸æ“š
        data = df[field].values.astype(np.float32)

        # ä½¿ç”¨ FileFeatureStorage å¯«å…¥
        storage = FileFeatureStorage(
            instrument=instrument,
            field=field,
            freq="1min"
        )

        try:
            storage.write(data)
        except Exception as e:
            logger.warning(f"  âš ï¸  {field}: å¯«å…¥å¤±æ•— - {e}")
            continue


def main():
    parser = argparse.ArgumentParser(
        description='å°‡ PostgreSQL åˆ†é˜ç·šæ•¸æ“šè½‰æ›ç‚º Qlib æ ¼å¼',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹ç”¨æ³•:
  # ğŸ§  æ™ºæ…§å¢é‡è½‰æ›ï¼ˆæ¨è–¦ï¼‰
  python export_minute_to_qlib.py --output-dir /data/qlib/tw_stock_minute --smart

  # å®Œæ•´è½‰æ›æ‰€æœ‰æ•¸æ“š
  python export_minute_to_qlib.py --output-dir /data/qlib/tw_stock_minute --stocks all

  # æ¸¬è©¦æ¨¡å¼ï¼ˆåƒ…è½‰æ› 10 æª”ï¼‰
  python export_minute_to_qlib.py --output-dir /data/qlib/tw_stock_minute --test
        """
    )

    parser.add_argument('--output-dir', type=str, required=True, help='Qlib æ•¸æ“šè¼¸å‡ºç›®éŒ„')
    parser.add_argument('--stocks', type=str, default='all', help='è‚¡ç¥¨ä»£ç¢¼ï¼ˆé€—è™Ÿåˆ†éš”ï¼‰æˆ– "all"')
    parser.add_argument('--smart', action='store_true', help='ğŸ§  æ™ºæ…§æ¨¡å¼ï¼šè‡ªå‹•å¢é‡åŒæ­¥')
    parser.add_argument('--test', action='store_true', help='æ¸¬è©¦æ¨¡å¼ï¼ˆåƒ…è™•ç†å‰ 10 æª”ï¼‰')
    parser.add_argument('--limit', type=int, help='é™åˆ¶è™•ç†æ•¸é‡')

    args = parser.parse_args()

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # å»ºç«‹è³‡æ–™åº«é€£æ¥
    logger.info("=== é€£æ¥è³‡æ–™åº« ===")
    engine = get_db_engine()

    # åˆå§‹åŒ– Qlib
    logger.info("\n=== åˆå§‹åŒ– Qlib ===")
    qlib.init(provider_uri=str(output_dir), region=REG_CN)
    logger.info(f"âœ… Qlib å·²åˆå§‹åŒ–: {output_dir}")

    # ç²å–äº¤æ˜“åˆ†é˜æ—¥æ›†
    logger.info("\n=== å»ºç«‹äº¤æ˜“åˆ†é˜æ—¥æ›† ===")
    trading_minutes = get_all_trading_minutes(engine)
    create_calendar_file(output_dir, trading_minutes)

    # ç²å–è‚¡ç¥¨åˆ—è¡¨
    logger.info("\n=== æº–å‚™è‚¡ç¥¨åˆ—è¡¨ ===")
    if args.stocks == 'all':
        stock_ids = get_all_stock_ids(engine)
    else:
        stock_ids = [s.strip() for s in args.stocks.split(',')]

    if args.test:
        stock_ids = stock_ids[:10]
        logger.warning(f"âš ï¸  æ¸¬è©¦æ¨¡å¼ï¼šåƒ…è™•ç†å‰ {len(stock_ids)} æª”")

    if args.limit:
        stock_ids = stock_ids[:args.limit]
        logger.warning(f"âš ï¸  é™åˆ¶è™•ç†ï¼š{args.limit} æª”")

    logger.info(f"å…± {len(stock_ids)} æª”è‚¡ç¥¨")

    if args.smart:
        logger.info(f"ğŸ§  æ™ºæ…§æ¨¡å¼ï¼šå•Ÿç”¨å¢é‡åŒæ­¥")

    # å»ºç«‹ features ç›®éŒ„çµæ§‹
    logger.info("\n=== å»ºç«‹ç›®éŒ„çµæ§‹ ===")
    for stock_id in stock_ids:
        instrument = stock_id.lower()
        features_dir = output_dir / 'features' / instrument
        features_dir.mkdir(parents=True, exist_ok=True)

    logger.info(f"âœ… å·²å»ºç«‹ {len(stock_ids)} å€‹è‚¡ç¥¨ç›®éŒ„")

    # å°å‡ºæ¯æª”è‚¡ç¥¨
    logger.info("\n=== é–‹å§‹è½‰æ›æ•¸æ“š ===")
    full_count = 0
    incremental_count = 0
    skip_count = 0
    error_count = 0

    progress_bar = tqdm(stock_ids, desc="è½‰æ›é€²åº¦", unit="æª”")

    for stock_id in progress_bar:
        progress_bar.set_description(f"è½‰æ› {stock_id}")

        try:
            # åˆ¤æ–·åŒæ­¥ç¯„åœ
            start_date, end_date, sync_type = determine_sync_range(
                engine, stock_id, smart_mode=args.smart
            )

            # è·³éå·²æ˜¯æœ€æ–°çš„è‚¡ç¥¨
            if sync_type == 'skip':
                skip_count += 1
                continue

            # ç²å–æ•¸æ“š
            df = fetch_stock_minute_data(engine, stock_id, start_date, end_date)

            if df.empty:
                logger.warning(f"  âš ï¸  {stock_id}: ç„¡æ•¸æ“š")
                skip_count += 1
                continue

            # é¡¯ç¤ºåŒæ­¥è³‡è¨Š
            if sync_type == 'full':
                logger.info(f"  ğŸ“¦ {stock_id}: å®Œæ•´è½‰æ› {len(df)} ç­† ({start_date} ~ {end_date})")
                full_count += 1
            elif sync_type == 'incremental':
                logger.info(f"  â• {stock_id}: å¢é‡è½‰æ› {len(df)} ç­† ({start_date} ~ {end_date})")
                incremental_count += 1

            # å°å‡ºåˆ° Qlib
            export_stock_to_qlib(stock_id, df, trading_minutes)

        except Exception as e:
            logger.error(f"  âŒ {stock_id}: å¤±æ•— - {str(e)}")
            error_count += 1
            continue

    # ç¸½çµ
    logger.info(f"\n{'='*60}")
    logger.info("=== è½‰æ›å®Œæˆ ===")
    logger.info(f"ğŸ“¦ å®Œæ•´è½‰æ›: {full_count} æª”")
    logger.info(f"â• å¢é‡è½‰æ›: {incremental_count} æª”")
    logger.info(f"â­ï¸  è·³é: {skip_count} æª”")
    logger.info(f"âœ… æˆåŠŸ: {full_count + incremental_count} æª”")
    if error_count > 0:
        logger.info(f"âŒ å¤±æ•—: {error_count} æª”")
    logger.info(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_dir}")

    # é©—è­‰æ•¸æ“š
    logger.info("\n=== é©—è­‰æ•¸æ“š ===")
    test_stock = stock_ids[0] if stock_ids else None
    if test_stock:
        try:
            df_test = D.features(
                [test_stock],
                ['$close', '$volume'],
                freq='1min'
            )
            logger.info(f"âœ… é©—è­‰æˆåŠŸ: {test_stock}")
            logger.info(f"   Shape: {df_test.shape}")
            if not df_test.empty:
                logger.info(f"   ç¯„åœ: {df_test.index.get_level_values('datetime').min()} è‡³ {df_test.index.get_level_values('datetime').max()}")
        except Exception as e:
            logger.error(f"âŒ é©—è­‰å¤±æ•—: {e}")


if __name__ == '__main__':
    main()
