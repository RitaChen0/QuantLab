#!/usr/bin/env python3
"""
Shioaji CSV è³‡æ–™åŒ¯å…¥è…³æœ¬

å°‡ ShioajiData/shioaji-stock/ ä¸‹çš„ CSV æª”æ¡ˆæ‰¹æ¬¡åŒ¯å…¥åˆ° PostgreSQL + TimescaleDB

ä½¿ç”¨ç¯„ä¾‹ï¼š
    # æ¸¬è©¦åŒ¯å…¥ 10 æª”è‚¡ç¥¨
    python scripts/import_shioaji_csv.py --limit 10

    # åŒ¯å…¥æŒ‡å®šè‚¡ç¥¨
    python scripts/import_shioaji_csv.py --stocks 2330,2317,2454

    # åŒ¯å…¥æœ€è¿‘ 1 å¹´è³‡æ–™
    python scripts/import_shioaji_csv.py --start-date 2024-01-01

    # å®Œæ•´åŒ¯å…¥æ‰€æœ‰è³‡æ–™
    python scripts/import_shioaji_csv.py --batch-size 50000
"""
import sys
import os
from pathlib import Path

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

import argparse
import pandas as pd
from datetime import datetime
from typing import List, Optional
from loguru import logger
from tqdm import tqdm

from app.db.session import SessionLocal
from app.db.base import import_models
from app.repositories.stock_minute_price import StockMinutePriceRepository
from app.schemas.stock_minute_price import StockMinutePriceCreate

# å°Žå…¥æ‰€æœ‰æ¨¡åž‹ä»¥é¿å… ORM mapper éŒ¯èª¤
import_models()


# é è¨­è³‡æ–™è·¯å¾‘ï¼ˆå®¹å™¨å…§æŽ›è¼‰é»žï¼‰
DEFAULT_DATA_DIR = "/data/shioaji/shioaji-stock"

# ç†±é–€è‚¡ç¥¨æ¸…å–®ï¼ˆå¸‚å€¼å‰ 50 å¤§ï¼‰
TOP_50_STOCKS = [
    '2330', '2317', '2454', '2412', '3008',  # å°ç©é›»ã€é´»æµ·ã€è¯ç™¼ç§‘ã€ä¸­è¯é›»ã€å¤§ç«‹å…‰
    '2308', '2882', '1301', '1303', '2002',  # å°é”é›»ã€åœ‹æ³°é‡‘ã€å°å¡‘ã€å—äºžã€ä¸­é‹¼
    '2886', '2881', '2891', '2892', '2885',  # å…†è±é‡‘ã€å¯Œé‚¦é‡‘ã€ä¸­ä¿¡é‡‘ã€ç¬¬ä¸€é‡‘ã€å…ƒå¤§é‡‘
    '2884', '2887', '2883', '5880', '2912',  # çŽ‰å±±é‡‘ã€å°æ–°é‡‘ã€é–‹ç™¼é‡‘ã€åˆåº«é‡‘ã€çµ±ä¸€è¶…
    '2880', '2382', '2395', '6505', '3045',  # è¯å—é‡‘ã€å»£é”ã€ç ”è¯ã€å°å¡‘åŒ–ã€å°ç£å¤§
    '1216', '2357', '1326', '2303', '2379',  # çµ±ä¸€ã€è¯ç¢©ã€å°åŒ–ã€è¯é›»ã€ç‘žæ˜±
    '2408', '2207', '2327', '3711', '2474',  # å—äºžç§‘ã€å’Œæ³°è»Šã€åœ‹å·¨ã€æ—¥æœˆå…‰æŠ•æŽ§ã€å¯æˆ
    '2801', '2609', '2615', '2603', '4904',  # å½°éŠ€ã€é™½æ˜Žã€è¬æµ·ã€é•·æ¦®ã€é å‚³
    '9910', '2888', '2345', '6669', '2409',  # è±æ³°ã€æ–°å…‰é‡‘ã€æ™ºé‚¦ã€ç·¯ç©Žã€å‹é”
    '3037', '2377', '2353', '5871', '2324',  # æ¬£èˆˆã€å¾®æ˜Ÿã€å®ç¢ã€ä¸­ç§Ÿ-KYã€ä»å¯¶
]


def import_csv_file(
    csv_path: Path,
    batch_size: int = 10000,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    incremental: bool = False
) -> dict:
    """
    åŒ¯å…¥å–®ä¸€ CSV æª”æ¡ˆ

    Args:
        csv_path: CSV æª”æ¡ˆè·¯å¾‘
        batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆé è¨­ 10000ï¼‰
        start_date: èµ·å§‹æ—¥æœŸï¼ˆåƒ…åŒ¯å…¥æ­¤æ—¥æœŸä¹‹å¾Œçš„è³‡æ–™ï¼‰
        end_date: çµæŸæ—¥æœŸï¼ˆåƒ…åŒ¯å…¥æ­¤æ—¥æœŸä¹‹å‰çš„è³‡æ–™ï¼‰
        incremental: æ˜¯å¦å¢žé‡åŒ¯å…¥ï¼ˆæª¢æŸ¥è³‡æ–™åº«å·²æœ‰è³‡æ–™ï¼‰

    Returns:
        dict: {
            "stock_id": str,
            "total_rows": int,
            "inserted": int,
            "skipped": int,
            "errors": int,
            "status": "success" | "failed"
        }
    """
    stock_id = csv_path.stem  # æª”åå³ç‚ºè‚¡ç¥¨ä»£ç¢¼
    db = SessionLocal()
    repo = StockMinutePriceRepository

    result = {
        "stock_id": stock_id,
        "total_rows": 0,
        "inserted": 0,
        "skipped": 0,
        "errors": 0,
        "status": "success"
    }

    try:
        # 1. æª¢æŸ¥å¢žé‡åŒ¯å…¥çš„èµ·å§‹æ—¥æœŸ
        if incremental:
            latest = repo.get_latest(db, stock_id, '1min')
            if latest:
                start_date = latest.datetime.strftime('%Y-%m-%d %H:%M:%S')
                logger.info(f"{stock_id}: Incremental import from {start_date}")

        # 2. è®€å– CSV
        logger.debug(f"{stock_id}: Reading CSV file...")
        try:
            df = pd.read_csv(csv_path)
        except Exception as e:
            logger.error(f"{stock_id}: Failed to read CSV - {str(e)}")
            result["status"] = "failed"
            result["errors"] = 1
            return result

        result["total_rows"] = len(df)

        # 3. è³‡æ–™æ¸…ç†èˆ‡é©—è­‰
        # é‡å‘½åæ¬„ä½
        df = df.rename(columns={
            'ts': 'datetime',
            'Open': 'open',
            'High': 'high',
            'Low': 'low',
            'Close': 'close',
            'Volume': 'volume',
            'Amount': 'amount'
        })

        # è½‰æ›æ™‚é–“æ ¼å¼
        df['datetime'] = pd.to_datetime(df['datetime'])

        # 4. æ™‚é–“ç¯„åœéŽæ¿¾
        if start_date:
            start_dt = pd.to_datetime(start_date)
            df = df[df['datetime'] > start_dt]

        if end_date:
            end_dt = pd.to_datetime(end_date)
            df = df[df['datetime'] <= end_dt]

        # 5. éŽæ¿¾ç„¡æ•ˆè³‡æ–™
        # ç§»é™¤ OHLC å…¨ç‚º 0 çš„è¨˜éŒ„
        df = df[~((df['open'] == 0) & (df['high'] == 0) & (df['low'] == 0) & (df['close'] == 0))]

        # ç§»é™¤è² æ•¸åƒ¹æ ¼
        df = df[(df['open'] > 0) & (df['high'] > 0) & (df['low'] > 0) & (df['close'] > 0)]

        # ç§»é™¤ OHLC é‚è¼¯éŒ¯èª¤çš„è¨˜éŒ„
        df = df[
            (df['high'] >= df['low']) &
            (df['high'] >= df['open']) &
            (df['high'] >= df['close']) &
            (df['low'] <= df['open']) &
            (df['low'] <= df['close'])
        ]

        if df.empty:
            logger.warning(f"{stock_id}: No valid data after filtering")
            result["status"] = "success"
            result["skipped"] = result["total_rows"]
            return result

        # 6. æ–°å¢žæ¬„ä½
        df['stock_id'] = stock_id
        df['timeframe'] = '1min'

        # 7. æ‰¹æ¬¡æ’å…¥
        logger.debug(f"{stock_id}: Inserting {len(df):,} records...")

        for i in range(0, len(df), batch_size):
            batch = df.iloc[i:i+batch_size]

            # è½‰æ›ç‚º Pydantic Schema
            records = []
            for _, row in batch.iterrows():
                try:
                    record = StockMinutePriceCreate(
                        stock_id=row['stock_id'],
                        datetime=row['datetime'],
                        timeframe=row['timeframe'],
                        open=float(row['open']),
                        high=float(row['high']),
                        low=float(row['low']),
                        close=float(row['close']),
                        volume=int(row['volume']) if row['volume'] > 0 else 0
                    )
                    records.append(record)
                except Exception as e:
                    logger.debug(f"{stock_id}: Failed to parse row at {row['datetime']}: {str(e)}")
                    result["errors"] += 1
                    continue

            # æ‰¹æ¬¡æ’å…¥ï¼ˆä½¿ç”¨ upsert é¿å…é‡è¤‡ï¼‰
            if records:
                try:
                    # ä½¿ç”¨ bulk insertï¼ˆé€Ÿåº¦è¼ƒå¿«ï¼‰
                    inserted = repo.create_bulk(db, records)
                    result["inserted"] += inserted
                except Exception as e:
                    # å¦‚æžœæ‰¹æ¬¡æ’å…¥å¤±æ•—ï¼Œå˜—è©¦é€ç­† upsert
                    logger.warning(f"{stock_id}: Bulk insert failed, trying upsert - {str(e)}")
                    for record in records:
                        try:
                            repo.upsert(
                                db,
                                record.stock_id,
                                record.datetime,
                                record.timeframe,
                                record
                            )
                            result["inserted"] += 1
                        except Exception as e2:
                            logger.debug(f"{stock_id}: Upsert failed at {record.datetime}: {str(e2)}")
                            result["errors"] += 1

        result["skipped"] = result["total_rows"] - result["inserted"] - result["errors"]

        logger.info(
            f"âœ… {stock_id}: Inserted {result['inserted']:,}/{result['total_rows']:,} records "
            f"(errors: {result['errors']}, skipped: {result['skipped']})"
        )

    except Exception as e:
        logger.error(f"âŒ {stock_id}: Import failed - {str(e)}")
        result["status"] = "failed"
        result["errors"] += 1

    finally:
        db.close()

    return result


def main():
    parser = argparse.ArgumentParser(
        description='Import Shioaji CSV data to PostgreSQL + TimescaleDB',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # æ¸¬è©¦åŒ¯å…¥ 10 æª”è‚¡ç¥¨
  python scripts/import_shioaji_csv.py --limit 10

  # åŒ¯å…¥å¸‚å€¼å‰ 50 å¤§è‚¡ç¥¨
  python scripts/import_shioaji_csv.py --top50

  # åŒ¯å…¥æŒ‡å®šè‚¡ç¥¨
  python scripts/import_shioaji_csv.py --stocks 2330,2317,2454

  # åŒ¯å…¥æœ€è¿‘ 1 å¹´è³‡æ–™
  python scripts/import_shioaji_csv.py --start-date 2024-01-01

  # å¢žé‡åŒ¯å…¥ï¼ˆåƒ…åŒ¯å…¥æ–°è³‡æ–™ï¼‰
  python scripts/import_shioaji_csv.py --incremental

  # å®Œæ•´åŒ¯å…¥æ‰€æœ‰è³‡æ–™ï¼ˆé«˜æ•ˆèƒ½ï¼‰
  python scripts/import_shioaji_csv.py --batch-size 50000
        """
    )

    parser.add_argument(
        '--data-dir',
        default=DEFAULT_DATA_DIR,
        help=f'Path to shioaji-stock directory (default: {DEFAULT_DATA_DIR})'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=10000,
        help='Batch size for insert (default: 10000, recommended: 50000 for full import)'
    )
    parser.add_argument(
        '--limit',
        type=int,
        help='Limit number of stocks to import (for testing)'
    )
    parser.add_argument(
        '--stocks',
        help='Comma-separated stock IDs to import (e.g., 2330,2317,2454)'
    )
    parser.add_argument(
        '--top50',
        action='store_true',
        help='Import top 50 stocks by market cap'
    )
    parser.add_argument(
        '--start-date',
        help='Start date (YYYY-MM-DD or YYYY-MM-DD HH:MM:SS), only import data after this date'
    )
    parser.add_argument(
        '--end-date',
        help='End date (YYYY-MM-DD or YYYY-MM-DD HH:MM:SS), only import data before this date'
    )
    parser.add_argument(
        '--incremental',
        action='store_true',
        help='Incremental import (only import new data after existing records)'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Enable verbose logging (debug level)'
    )

    args = parser.parse_args()

    # è¨­å®šæ—¥èªŒç´šåˆ¥
    if args.verbose:
        logger.remove()
        logger.add(sys.stderr, level="DEBUG")
    else:
        logger.remove()
        logger.add(sys.stderr, level="INFO")

    # æª¢æŸ¥è³‡æ–™ç›®éŒ„
    data_dir = Path(args.data_dir)
    if not data_dir.exists():
        logger.error(f"âŒ Data directory not found: {data_dir}")
        logger.error(f"Please check the path or create symbolic link:")
        logger.error(f"  ln -s /path/to/ShioajiData /home/ubuntu/QuantLab/ShioajiData")
        sys.exit(1)

    # ç²å–æ‰€æœ‰ CSV æª”æ¡ˆ
    csv_files = sorted(data_dir.glob('*.csv'))

    if not csv_files:
        logger.error(f"âŒ No CSV files found in {data_dir}")
        sys.exit(1)

    logger.info(f"ðŸ“ Found {len(csv_files)} CSV files in {data_dir}")

    # éŽæ¿¾æŒ‡å®šè‚¡ç¥¨
    if args.top50:
        logger.info(f"ðŸ”¥ Filtering top 50 stocks by market cap...")
        csv_files = [f for f in csv_files if f.stem in TOP_50_STOCKS]
        logger.info(f"ðŸ“Š Selected {len(csv_files)} stocks")

    elif args.stocks:
        stock_ids = [s.strip() for s in args.stocks.split(',')]
        logger.info(f"ðŸŽ¯ Filtering specified stocks: {stock_ids}")
        csv_files = [f for f in csv_files if f.stem in stock_ids]
        logger.info(f"ðŸ“Š Selected {len(csv_files)} stocks")

    # é™åˆ¶æ•¸é‡ï¼ˆæ¸¬è©¦ç”¨ï¼‰
    if args.limit:
        csv_files = csv_files[:args.limit]
        logger.info(f"ðŸ§ª Test mode: Limited to first {args.limit} stocks")

    # çµ±è¨ˆè³‡è¨Š
    total_stocks = len(csv_files)
    total_rows = 0
    total_inserted = 0
    total_skipped = 0
    total_errors = 0
    failed_stocks = []
    success_stocks = []

    # é¡¯ç¤ºåŒ¯å…¥è¨­å®š
    logger.info(f"\n{'='*60}")
    logger.info(f"ðŸš€ Import Configuration:")
    logger.info(f"  Total stocks: {total_stocks}")
    logger.info(f"  Batch size: {args.batch_size:,}")
    logger.info(f"  Start date: {args.start_date or 'All'}")
    logger.info(f"  End date: {args.end_date or 'All'}")
    logger.info(f"  Incremental: {args.incremental}")
    logger.info(f"{'='*60}\n")

    # é–‹å§‹åŒ¯å…¥
    start_time = datetime.now()

    for csv_file in tqdm(csv_files, desc="Importing stocks", unit="stock"):
        try:
            result = import_csv_file(
                csv_file,
                args.batch_size,
                args.start_date,
                args.end_date,
                args.incremental
            )

            total_rows += result["total_rows"]
            total_inserted += result["inserted"]
            total_skipped += result["skipped"]
            total_errors += result["errors"]

            if result["status"] == "success":
                success_stocks.append(result["stock_id"])
            else:
                failed_stocks.append(result["stock_id"])

        except Exception as e:
            logger.error(f"âŒ Failed to import {csv_file.stem}: {str(e)}")
            failed_stocks.append(csv_file.stem)
            continue

    # è¨ˆç®—åŸ·è¡Œæ™‚é–“
    end_time = datetime.now()
    elapsed = end_time - start_time
    elapsed_minutes = elapsed.total_seconds() / 60

    # ç¸½çµå ±å‘Š
    logger.info(f"\n{'='*60}")
    logger.info(f"âœ… Import Completed!")
    logger.info(f"{'='*60}")
    logger.info(f"ðŸ“Š Statistics:")
    logger.info(f"  Total stocks processed: {total_stocks}")
    logger.info(f"  Successful: {len(success_stocks)}")
    logger.info(f"  Failed: {len(failed_stocks)}")
    logger.info(f"\nðŸ“ˆ Data:")
    logger.info(f"  Total rows read: {total_rows:,}")
    logger.info(f"  Records inserted: {total_inserted:,}")
    logger.info(f"  Records skipped: {total_skipped:,}")
    logger.info(f"  Errors: {total_errors:,}")
    logger.info(f"\nâ±ï¸  Performance:")
    logger.info(f"  Elapsed time: {elapsed_minutes:.1f} minutes")
    logger.info(f"  Average speed: {total_inserted / elapsed.total_seconds():.0f} records/second")

    if failed_stocks:
        logger.warning(f"\nâš ï¸  Failed stocks ({len(failed_stocks)}):")
        logger.warning(f"  {', '.join(failed_stocks[:10])}")
        if len(failed_stocks) > 10:
            logger.warning(f"  ... and {len(failed_stocks) - 10} more")

    logger.info(f"{'='*60}\n")

    # é©—è­‰å»ºè­°
    logger.info(f"ðŸ’¡ Next steps:")
    logger.info(f"  1. Verify data:")
    logger.info(f"     docker compose exec postgres psql -U quantlab quantlab -c \"SELECT COUNT(*) FROM stock_minute_prices;\"")
    logger.info(f"\n  2. Check specific stock:")
    logger.info(f"     docker compose exec postgres psql -U quantlab quantlab -c \"SELECT COUNT(*), MIN(datetime), MAX(datetime) FROM stock_minute_prices WHERE stock_id = '2330';\"")
    logger.info(f"\n  3. Test API:")
    logger.info(f"     curl http://localhost:8000/api/v1/intraday/coverage/2330?timeframe=1min")


if __name__ == '__main__':
    main()
