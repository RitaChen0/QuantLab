#!/usr/bin/env python3
"""
Shioaji CSV è³‡æ–™åŒ¯å…¥è…³æœ¬

å°‡ ShioajiData/shioaji-stock/ ä¸‹çš„ CSV æª”æ¡ˆæ‰¹æ¬¡åŒ¯å…¥åˆ° PostgreSQL + TimescaleDB

ä½¿ç”¨ç¯„ä¾‹ï¼š
    # æ¸¬è©¦åŒ¯å…¥ 10 æª”è‚¡ç¥¨
    python scripts/import_shioaji_csv.py --limit 10

    # åŒ¯å…¥æŒ‡å®šè‚¡ç¥¨
    python scripts/import_shioaji_csv.py --stocks 2330,2317,2454

    # åŒ¯å…¥æœ€è¿‘ 1 å¹´è³‡æ–™
    python scripts/import_shioaji_csv.py --start-date 2024-01-01

    # å®Œæ•´åŒ¯å…¥æ‰€æœ‰è³‡æ–™
    python scripts/import_shioaji_csv.py --batch-size 50000
"""
import sys
import os
from pathlib import Path

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

import argparse
import pandas as pd
from datetime import datetime, timezone
from typing import List, Optional
from loguru import logger
from tqdm import tqdm
from sqlalchemy import create_engine
from sqlalchemy.orm import Session, sessionmaker

from app.core.config import settings
from app.db.base import import_models
from app.repositories.stock_minute_price import StockMinutePriceRepository
from app.schemas.stock_minute_price import StockMinutePriceCreate

# å°Žå…¥æ‰€æœ‰æ¨¡åž‹ä»¥é¿å… ORM mapper éŒ¯èª¤
import_models()

# å‰µå»ºå°ˆç”¨æ–¼å°Žå…¥çš„ Sessionï¼ˆé—œé–‰ SQL echo é¿å…æ—¥èªŒè†¨è„¹ï¼‰
engine_silent = create_engine(
    settings.DATABASE_URL,
    pool_pre_ping=True,
    pool_size=10,
    max_overflow=20,
    echo=False,  # é—œé–‰ SQL æ—¥èªŒè¨˜éŒ„
)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine_silent)


# é è¨­è³‡æ–™è·¯å¾‘ï¼ˆå®¹å™¨å…§æŽ›è¼‰é»žï¼‰
# Docker volume: ./ShioajiData:/data/shioaji
DEFAULT_DATA_DIR = "/data/shioaji/shioaji-stock"

# ç†±é–€è‚¡ç¥¨æ¸…å–®ï¼ˆå¸‚å€¼å‰ 50 å¤§ï¼‰
TOP_50_STOCKS = [
    '2330', '2317', '2454', '2412', '3008',  # å°ç©é›»ã€é´»æµ·ã€è¯ç™¼ç§‘ã€ä¸­è¯é›»ã€å¤§ç«‹å…‰
    '2308', '2882', '1301', '1303', '2002',  # å°é”é›»ã€åœ‹æ³°é‡‘ã€å°å¡‘ã€å—äºžã€ä¸­é‹¼
    '2886', '2881', '2891', '2892', '2885',  # å…†è±é‡‘ã€å¯Œé‚¦é‡‘ã€ä¸­ä¿¡é‡‘ã€ç¬¬ä¸€é‡‘ã€å…ƒå¤§é‡‘
    '2884', '2887', '2883', '5880', '2912',  # çŽ‰å±±é‡‘ã€å°æ–°é‡‘ã€é–‹ç™¼é‡‘ã€åˆåº«é‡‘ã€çµ±ä¸€è¶…
    '2880', '2382', '2395', '6505', '3045',  # è¯å—é‡‘ã€å»£é”ã€ç ”è¯ã€å°å¡‘åŒ–ã€å°ç£å¤§
    '1216', '2357', '1326', '2303', '2379',  # çµ±ä¸€ã€è¯ç¢©ã€å°åŒ–ã€è¯é›»ã€ç‘žæ˜±
    '2408', '2207', '2327', '3711', '2474',  # å—äºžç§‘ã€å’Œæ³°è»Šã€åœ‹å·¨ã€æ—¥æœˆå…‰æŠ•æŽ§ã€å¯æˆ
    '2801', '2609', '2615', '2603', '4904',  # å½°éŠ€ã€é™½æ˜Žã€è¬æµ·ã€é•·æ¦®ã€é å‚³
    '9910', '2888', '2345', '6669', '2409',  # è±æ³°ã€æ–°å…‰é‡‘ã€æ™ºé‚¦ã€ç·¯ç©Žã€å‹é”
    '3037', '2377', '2353', '5871', '2324',  # æ¬£èˆˆã€å¾®æ˜Ÿã€å®ç¢ã€ä¸­ç§Ÿ-KYã€ä»å¯¶
]


def _process_dataframe(
    df: pd.DataFrame,
    stock_id: str,
    start_date: Optional[str],
    end_date: Optional[str]
) -> pd.DataFrame:
    """
    è™•ç† DataFrameï¼šæ¸…ç†ã€é©—è­‰ã€éŽæ¿¾

    Args:
        df: åŽŸå§‹ DataFrame
        stock_id: è‚¡ç¥¨ä»£ç¢¼
        start_date: èµ·å§‹æ—¥æœŸ
        end_date: çµæŸæ—¥æœŸ

    Returns:
        è™•ç†å¾Œçš„ DataFrame
    """
    # 1. é‡å‘½åæ¬„ä½
    df = df.rename(columns={
        'ts': 'datetime',
        'Open': 'open',
        'High': 'high',
        'Low': 'low',
        'Close': 'close',
        'Volume': 'volume',
        'Amount': 'amount'
    })

    # 2. è½‰æ›æ™‚é–“æ ¼å¼
    df['datetime'] = pd.to_datetime(df['datetime'])

    # 3. æ™‚é–“ç¯„åœéŽæ¿¾
    if start_date:
        start_dt = pd.to_datetime(start_date)
        # å¢žé‡æ¨¡å¼ï¼šä½¿ç”¨ > é¿å…é‡è¤‡æœ€å¾Œä¸€ç­†è¨˜éŒ„
        df = df[df['datetime'] > start_dt]

    if end_date:
        end_dt = pd.to_datetime(end_date)
        df = df[df['datetime'] <= end_dt]

    # 4. éŽæ¿¾ç„¡æ•ˆè³‡æ–™
    # ç§»é™¤ OHLC å…¨ç‚º 0 çš„è¨˜éŒ„
    df = df[~((df['open'] == 0) & (df['high'] == 0) & (df['low'] == 0) & (df['close'] == 0))]

    # ç§»é™¤è² æ•¸åƒ¹æ ¼
    df = df[(df['open'] > 0) & (df['high'] > 0) & (df['low'] > 0) & (df['close'] > 0)]

    # ç§»é™¤ OHLC é‚è¼¯éŒ¯èª¤çš„è¨˜éŒ„
    df = df[
        (df['high'] >= df['low']) &
        (df['high'] >= df['open']) &
        (df['high'] >= df['close']) &
        (df['low'] <= df['open']) &
        (df['low'] <= df['close'])
    ]

    # 5. æ–°å¢žæ¬„ä½
    df['stock_id'] = stock_id
    df['timeframe'] = '1min'

    return df


def _import_csv_chunked(
    csv_path: Path,
    db: Session,
    stock_id: str,
    repo,
    batch_size: int,
    start_date: Optional[str],
    end_date: Optional[str],
    chunk_size: int,
    result: dict
) -> dict:
    """
    ä½¿ç”¨åˆ†å¡Šè®€å–åŒ¯å…¥ CSVï¼ˆè¨˜æ†¶é«”å‹å–„ï¼‰

    Args:
        csv_path: CSV æª”æ¡ˆè·¯å¾‘
        db: è³‡æ–™åº«æœƒè©±
        stock_id: è‚¡ç¥¨ä»£ç¢¼
        repo: Repository
        batch_size: æ’å…¥æ‰¹æ¬¡å¤§å°
        start_date: èµ·å§‹æ—¥æœŸ
        end_date: çµæŸæ—¥æœŸ
        chunk_size: CSV è®€å–åˆ†å¡Šå¤§å°
        result: çµæžœå­—å…¸

    Returns:
        æ›´æ–°å¾Œçš„çµæžœå­—å…¸
    """
    try:
        logger.debug(f"{stock_id}: Using chunked reading (chunk_size={chunk_size})")

        for chunk in pd.read_csv(csv_path, chunksize=chunk_size):
            result["total_rows"] += len(chunk)

            # è™•ç†åˆ†å¡Šè³‡æ–™
            df = _process_dataframe(chunk, stock_id, start_date, end_date)

            if df.empty:
                result["skipped"] += len(chunk)
                continue

            # æ‰¹æ¬¡æ’å…¥
            for i in range(0, len(df), batch_size):
                batch = df.iloc[i:i+batch_size]

                # è½‰æ›ç‚º Pydantic Schema
                records = []
                for _, row in batch.iterrows():
                    try:
                        record = StockMinutePriceCreate(
                            stock_id=row['stock_id'],
                            datetime=row['datetime'],
                            timeframe=row['timeframe'],
                            open=float(row['open']),
                            high=float(row['high']),
                            low=float(row['low']),
                            close=float(row['close']),
                            volume=int(row['volume']) if row['volume'] > 0 else 0
                        )
                        records.append(record)
                    except Exception as e:
                        logger.debug(f"{stock_id}: Failed to parse row: {str(e)}")
                        result["errors"] += 1
                        continue

                # æ‰¹æ¬¡æ’å…¥
                if records:
                    try:
                        inserted = repo.create_bulk(db, records)
                        result["inserted"] += inserted
                    except Exception as e:
                        logger.warning(f"{stock_id}: Bulk insert failed, trying upsert - {str(e)}")
                        for record in records:
                            try:
                                repo.upsert(db, record.stock_id, record.datetime, record.timeframe, record)
                                result["inserted"] += 1
                            except Exception as e2:
                                logger.debug(f"{stock_id}: Upsert failed: {str(e2)}")
                                result["errors"] += 1

        result["skipped"] = result["total_rows"] - result["inserted"] - result["errors"]

        logger.info(
            f"âœ… {stock_id}: Inserted {result['inserted']:,}/{result['total_rows']:,} records "
            f"(errors: {result['errors']}, skipped: {result['skipped']})"
        )

    except Exception as e:
        logger.error(f"âŒ {stock_id}: Chunked import failed - {str(e)}")
        result["status"] = "failed"
        result["errors"] += 1

    return result


def import_csv_file(
    csv_path: Path,
    db: Session,
    batch_size: int = 10000,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    incremental: bool = False,
    use_chunks: bool = False,
    chunk_size: int = 50000
) -> dict:
    """
    åŒ¯å…¥å–®ä¸€ CSV æª”æ¡ˆ

    Args:
        csv_path: CSV æª”æ¡ˆè·¯å¾‘
        db: è³‡æ–™åº«æœƒè©±ï¼ˆç”±å‘¼å«è€…ç®¡ç†ï¼‰
        batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆé è¨­ 10000ï¼‰
        start_date: èµ·å§‹æ—¥æœŸï¼ˆåƒ…åŒ¯å…¥æ­¤æ—¥æœŸä¹‹å¾Œçš„è³‡æ–™ï¼‰
        end_date: çµæŸæ—¥æœŸï¼ˆåƒ…åŒ¯å…¥æ­¤æ—¥æœŸä¹‹å‰çš„è³‡æ–™ï¼‰
        incremental: æ˜¯å¦å¢žé‡åŒ¯å…¥ï¼ˆæª¢æŸ¥è³‡æ–™åº«å·²æœ‰è³‡æ–™ï¼‰
        use_chunks: æ˜¯å¦ä½¿ç”¨åˆ†å¡Šè®€å–ï¼ˆé™ä½Žè¨˜æ†¶é«”ä½¿ç”¨ï¼‰
        chunk_size: åˆ†å¡Šå¤§å°ï¼ˆé è¨­ 50000ï¼‰

    Returns:
        dict: {
            "stock_id": str,
            "total_rows": int,
            "inserted": int,
            "skipped": int,
            "errors": int,
            "status": "success" | "failed"
        }
    """
    stock_id = csv_path.stem  # æª”åå³ç‚ºè‚¡ç¥¨ä»£ç¢¼
    repo = StockMinutePriceRepository

    result = {
        "stock_id": stock_id,
        "total_rows": 0,
        "inserted": 0,
        "skipped": 0,
        "errors": 0,
        "status": "success"
    }

    try:
        # 1. æª¢æŸ¥å¢žé‡åŒ¯å…¥çš„èµ·å§‹æ—¥æœŸ
        if incremental:
            latest = repo.get_latest(db, stock_id, '1min')
            if latest:
                start_date = latest.datetime.strftime('%Y-%m-%d %H:%M:%S')
                logger.info(f"{stock_id}: Incremental import from {start_date}")

        # 2. è®€å– CSV
        logger.debug(f"{stock_id}: Reading CSV file...")

        if use_chunks:
            # ä½¿ç”¨åˆ†å¡Šè®€å–ï¼ˆè¨˜æ†¶é«”å‹å–„ï¼‰
            return _import_csv_chunked(
                csv_path, db, stock_id, repo, batch_size,
                start_date, end_date, chunk_size, result
            )

        # æ¨™æº–è®€å–ï¼ˆä¸€æ¬¡æ€§è¼‰å…¥ï¼‰
        try:
            df = pd.read_csv(csv_path)
        except Exception as e:
            logger.error(f"{stock_id}: Failed to read CSV - {str(e)}")
            result["status"] = "failed"
            result["errors"] = 1
            return result

        result["total_rows"] = len(df)

        # 3. è™•ç†è³‡æ–™ï¼ˆæ¸…ç†ã€é©—è­‰ã€éŽæ¿¾ï¼‰
        df = _process_dataframe(df, stock_id, start_date, end_date)

        if df.empty:
            if incremental:
                logger.info(f"{stock_id}: âœ… Already up-to-date, no new data")
            else:
                logger.warning(f"{stock_id}: No valid data after filtering")
            result["status"] = "success"
            result["skipped"] = result["total_rows"]
            return result

        # 4. æ‰¹æ¬¡æ’å…¥
        logger.debug(f"{stock_id}: Inserting {len(df):,} records...")

        for i in range(0, len(df), batch_size):
            batch = df.iloc[i:i+batch_size]

            # è½‰æ›ç‚º Pydantic Schema
            records = []
            for _, row in batch.iterrows():
                try:
                    record = StockMinutePriceCreate(
                        stock_id=row['stock_id'],
                        datetime=row['datetime'],
                        timeframe=row['timeframe'],
                        open=float(row['open']),
                        high=float(row['high']),
                        low=float(row['low']),
                        close=float(row['close']),
                        volume=int(row['volume']) if row['volume'] > 0 else 0
                    )
                    records.append(record)
                except Exception as e:
                    logger.debug(f"{stock_id}: Failed to parse row at {row['datetime']}: {str(e)}")
                    result["errors"] += 1
                    continue

            # æ‰¹æ¬¡æ’å…¥ï¼ˆä½¿ç”¨ upsert é¿å…é‡è¤‡ï¼‰
            if records:
                try:
                    # ä½¿ç”¨ bulk insertï¼ˆé€Ÿåº¦è¼ƒå¿«ï¼‰
                    inserted = repo.create_bulk(db, records)
                    result["inserted"] += inserted
                except Exception as e:
                    # å¦‚æžœæ‰¹æ¬¡æ’å…¥å¤±æ•—ï¼Œå˜—è©¦é€ç­† upsert
                    logger.warning(f"{stock_id}: Bulk insert failed, trying upsert - {str(e)}")
                    # ðŸ”§ Rollback before trying individual upserts
                    db.rollback()
                    for record in records:
                        try:
                            repo.upsert(
                                db,
                                record.stock_id,
                                record.datetime,
                                record.timeframe,
                                record
                            )
                            result["inserted"] += 1
                        except Exception as e2:
                            logger.debug(f"{stock_id}: Upsert failed at {record.datetime}: {str(e2)}")
                            result["errors"] += 1

        result["skipped"] = result["total_rows"] - result["inserted"] - result["errors"]

        logger.info(
            f"âœ… {stock_id}: Inserted {result['inserted']:,}/{result['total_rows']:,} records "
            f"(errors: {result['errors']}, skipped: {result['skipped']})"
        )

    except Exception as e:
        logger.error(f"âŒ {stock_id}: Import failed - {str(e)}")
        result["status"] = "failed"
        result["errors"] += 1
        # ðŸ”§ Rollback session to allow subsequent imports to continue
        db.rollback()

    return result


def main():
    parser = argparse.ArgumentParser(
        description='Import Shioaji CSV data to PostgreSQL + TimescaleDB',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # æ¸¬è©¦åŒ¯å…¥ 10 æª”è‚¡ç¥¨
  python scripts/import_shioaji_csv.py --limit 10

  # åŒ¯å…¥å¸‚å€¼å‰ 50 å¤§è‚¡ç¥¨
  python scripts/import_shioaji_csv.py --top50

  # åŒ¯å…¥æŒ‡å®šè‚¡ç¥¨
  python scripts/import_shioaji_csv.py --stocks 2330,2317,2454

  # åŒ¯å…¥æœ€è¿‘ 1 å¹´è³‡æ–™
  python scripts/import_shioaji_csv.py --start-date 2024-01-01

  # å¢žé‡åŒ¯å…¥ï¼ˆåƒ…åŒ¯å…¥æ–°è³‡æ–™ï¼‰
  python scripts/import_shioaji_csv.py --incremental

  # å®Œæ•´åŒ¯å…¥æ‰€æœ‰è³‡æ–™ï¼ˆé«˜æ•ˆèƒ½ï¼‰
  python scripts/import_shioaji_csv.py --batch-size 50000
        """
    )

    parser.add_argument(
        '--data-dir',
        default=DEFAULT_DATA_DIR,
        help=f'Path to shioaji-stock directory (default: {DEFAULT_DATA_DIR})'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=10000,
        help='Batch size for insert (default: 10000, recommended: 50000 for full import)'
    )
    parser.add_argument(
        '--limit',
        type=int,
        help='Limit number of stocks to import (for testing)'
    )
    parser.add_argument(
        '--stocks',
        help='Comma-separated stock IDs to import (e.g., 2330,2317,2454)'
    )
    parser.add_argument(
        '--top50',
        action='store_true',
        help='Import top 50 stocks by market cap'
    )
    parser.add_argument(
        '--start-date',
        help='Start date (YYYY-MM-DD or YYYY-MM-DD HH:MM:SS), only import data after this date'
    )
    parser.add_argument(
        '--end-date',
        help='End date (YYYY-MM-DD or YYYY-MM-DD HH:MM:SS), only import data before this date'
    )
    parser.add_argument(
        '--incremental',
        action='store_true',
        help='Incremental import (only import new data after existing records)'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Enable verbose logging (debug level)'
    )
    parser.add_argument(
        '--use-chunks',
        action='store_true',
        help='Use chunked reading for large CSV files (reduces memory usage)'
    )
    parser.add_argument(
        '--chunk-size',
        type=int,
        default=50000,
        help='Chunk size for chunked reading (default: 50000)'
    )

    args = parser.parse_args()

    # è¨­å®šæ—¥èªŒç´šåˆ¥
    if args.verbose:
        logger.remove()
        logger.add(sys.stderr, level="DEBUG")
    else:
        logger.remove()
        logger.add(sys.stderr, level="INFO")

    # æª¢æŸ¥è³‡æ–™ç›®éŒ„
    data_dir = Path(args.data_dir)
    if not data_dir.exists():
        logger.error(f"âŒ Data directory not found: {data_dir}")
        logger.error(f"Please check the path or create symbolic link:")
        logger.error(f"  ln -s /path/to/ShioajiData /home/ubuntu/QuantLab/ShioajiData")
        sys.exit(1)

    # ç²å–æ‰€æœ‰ CSV æª”æ¡ˆ
    csv_files = sorted(data_dir.glob('*.csv'))

    if not csv_files:
        logger.error(f"âŒ No CSV files found in {data_dir}")
        sys.exit(1)

    logger.info(f"ðŸ“ Found {len(csv_files)} CSV files in {data_dir}")

    # éŽæ¿¾æŒ‡å®šè‚¡ç¥¨
    if args.top50:
        logger.info(f"ðŸ”¥ Filtering top 50 stocks by market cap...")
        csv_files = [f for f in csv_files if f.stem in TOP_50_STOCKS]
        logger.info(f"ðŸ“Š Selected {len(csv_files)} stocks")

    elif args.stocks:
        stock_ids = [s.strip() for s in args.stocks.split(',')]
        logger.info(f"ðŸŽ¯ Filtering specified stocks: {stock_ids}")
        csv_files = [f for f in csv_files if f.stem in stock_ids]
        logger.info(f"ðŸ“Š Selected {len(csv_files)} stocks")

    # é™åˆ¶æ•¸é‡ï¼ˆæ¸¬è©¦ç”¨ï¼‰
    if args.limit:
        csv_files = csv_files[:args.limit]
        logger.info(f"ðŸ§ª Test mode: Limited to first {args.limit} stocks")

    # çµ±è¨ˆè³‡è¨Š
    total_stocks = len(csv_files)
    total_rows = 0
    total_inserted = 0
    total_skipped = 0
    total_errors = 0
    failed_stocks = []
    success_stocks = []

    # é¡¯ç¤ºåŒ¯å…¥è¨­å®š
    logger.info(f"\n{'='*60}")
    logger.info(f"ðŸš€ Import Configuration:")
    logger.info(f"  Total stocks: {total_stocks}")
    logger.info(f"  Batch size: {args.batch_size:,}")
    logger.info(f"  Start date: {args.start_date or 'All'}")
    logger.info(f"  End date: {args.end_date or 'All'}")
    logger.info(f"  Incremental: {args.incremental}")
    logger.info(f"{'='*60}\n")

    # é–‹å§‹åŒ¯å…¥
    start_time = datetime.now(timezone.utc)

    # å»ºç«‹å…±ç”¨è³‡æ–™åº«é€£ç·š
    db = SessionLocal()

    try:
        for csv_file in tqdm(csv_files, desc="Importing stocks", unit="stock"):
            try:
                result = import_csv_file(
                    csv_file,
                    db,
                    args.batch_size,
                    args.start_date,
                    args.end_date,
                    args.incremental,
                    args.use_chunks,
                    args.chunk_size
                )

                total_rows += result["total_rows"]
                total_inserted += result["inserted"]
                total_skipped += result["skipped"]
                total_errors += result["errors"]

                if result["status"] == "success":
                    success_stocks.append(result["stock_id"])
                else:
                    failed_stocks.append(result["stock_id"])

            except Exception as e:
                logger.error(f"âŒ Failed to import {csv_file.stem}: {str(e)}")
                failed_stocks.append(csv_file.stem)
                continue

    finally:
        # ç¢ºä¿è³‡æ–™åº«é€£ç·šæ­£ç¢ºé—œé–‰
        db.close()

    # è¨ˆç®—åŸ·è¡Œæ™‚é–“
    end_time = datetime.now(timezone.utc)
    elapsed = end_time - start_time
    elapsed_minutes = elapsed.total_seconds() / 60

    # ç¸½çµå ±å‘Š
    logger.info(f"\n{'='*60}")
    logger.info(f"âœ… Import Completed!")
    logger.info(f"{'='*60}")
    logger.info(f"ðŸ“Š Statistics:")
    logger.info(f"  Total stocks processed: {total_stocks}")
    logger.info(f"  Successful: {len(success_stocks)}")
    logger.info(f"  Failed: {len(failed_stocks)}")
    logger.info(f"\nðŸ“ˆ Data:")
    logger.info(f"  Total rows read: {total_rows:,}")
    logger.info(f"  Records inserted: {total_inserted:,}")
    logger.info(f"  Records skipped: {total_skipped:,}")
    logger.info(f"  Errors: {total_errors:,}")
    logger.info(f"\nâ±ï¸  Performance:")
    logger.info(f"  Elapsed time: {elapsed_minutes:.1f} minutes")
    logger.info(f"  Average speed: {total_inserted / elapsed.total_seconds():.0f} records/second")

    if failed_stocks:
        logger.warning(f"\nâš ï¸  Failed stocks ({len(failed_stocks)}):")
        logger.warning(f"  {', '.join(failed_stocks[:10])}")
        if len(failed_stocks) > 10:
            logger.warning(f"  ... and {len(failed_stocks) - 10} more")

    logger.info(f"{'='*60}\n")

    # é©—è­‰å»ºè­°
    logger.info(f"ðŸ’¡ Next steps:")
    logger.info(f"  1. Verify data:")
    logger.info(f"     docker compose exec postgres psql -U quantlab quantlab -c \"SELECT COUNT(*) FROM stock_minute_prices;\"")
    logger.info(f"\n  2. Check specific stock:")
    logger.info(f"     docker compose exec postgres psql -U quantlab quantlab -c \"SELECT COUNT(*), MIN(datetime), MAX(datetime) FROM stock_minute_prices WHERE stock_id = '2330';\"")
    logger.info(f"\n  3. Test API:")
    logger.info(f"     curl http://localhost:8000/api/v1/intraday/coverage/2330?timeframe=1min")


if __name__ == '__main__':
    main()
