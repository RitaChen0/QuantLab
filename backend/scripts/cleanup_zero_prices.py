"""
æ¸…ç†é›¶åƒ¹æ ¼è¨˜éŒ„

æ­¤è…³æœ¬åªåˆªé™¤ open=0 çš„åƒ¹æ ¼è¨˜éŒ„ï¼ˆä¿ç•™æœ‰æ•ˆæ•¸æ“šï¼‰
é©ç”¨æ–¼è‚¡ç¥¨æœ‰éƒ¨åˆ†æœ‰æ•ˆæ•¸æ“šå’Œéƒ¨åˆ†ç„¡æ•ˆæ•¸æ“šçš„æƒ…æ³
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from sqlalchemy import text
from app.db.session import SessionLocal
from loguru import logger
import argparse


def get_zero_price_stats(db) -> dict:
    """
    ç²å–é›¶åƒ¹æ ¼è¨˜éŒ„çµ±è¨ˆ

    Returns:
        Statistics dictionary
    """
    query = text("""
    SELECT
        COUNT(*) as total_zero_records,
        COUNT(DISTINCT stock_id) as affected_stocks,
        MIN(date) as earliest_date,
        MAX(date) as latest_date
    FROM stock_prices
    WHERE open <= 0
    """)

    result = db.execute(query).fetchone()
    return {
        "total_zero_records": result.total_zero_records,
        "affected_stocks": result.affected_stocks,
        "earliest_date": result.earliest_date,
        "latest_date": result.latest_date
    }


def cleanup_zero_prices(db, dry_run: bool = False, batch_size: int = 10000) -> dict:
    """
    æ¸…ç†æ‰€æœ‰é›¶åƒ¹æ ¼è¨˜éŒ„

    Args:
        db: Database session
        dry_run: If True, don't actually delete
        batch_size: Delete records in batches

    Returns:
        Cleanup statistics
    """
    if dry_run:
        logger.info("ğŸ” [DRY RUN] é è¦½å°‡è¦åˆªé™¤çš„è¨˜éŒ„...")
        stats = get_zero_price_stats(db)
        logger.info(f"   ç¸½è¨˜éŒ„æ•¸: {stats['total_zero_records']:,}")
        logger.info(f"   å½±éŸ¿è‚¡ç¥¨: {stats['affected_stocks']}")
        logger.info(f"   æ—¥æœŸç¯„åœ: {stats['earliest_date']} ~ {stats['latest_date']}")
        return stats

    # å¯¦éš›åˆªé™¤
    logger.info("ğŸ—‘ï¸  é–‹å§‹åˆªé™¤é›¶åƒ¹æ ¼è¨˜éŒ„...")

    # ç²å–åˆå§‹çµ±è¨ˆ
    initial_stats = get_zero_price_stats(db)
    total_to_delete = initial_stats["total_zero_records"]

    logger.info(f"   æº–å‚™åˆªé™¤ {total_to_delete:,} ç­†è¨˜éŒ„")
    logger.info(f"   å½±éŸ¿ {initial_stats['affected_stocks']} å€‹è‚¡ç¥¨")

    # æ‰¹æ¬¡åˆªé™¤ï¼ˆé¿å…ä¸€æ¬¡æ€§åˆªé™¤å¤ªå¤šé€ æˆé–å®šå•é¡Œï¼‰
    deleted_count = 0
    batch_num = 0

    while True:
        batch_num += 1
        logger.info(f"   åŸ·è¡Œæ‰¹æ¬¡ #{batch_num} (æ¯æ‰¹ {batch_size:,} ç­†)...")

        # åˆªé™¤ä¸€æ‰¹è¨˜éŒ„ (ä½¿ç”¨ stock_id + date è€Œä¸æ˜¯ ctidï¼Œå…¼å®¹ TimescaleDB å£“ç¸®)
        delete_query = text(f"""
        DELETE FROM stock_prices
        WHERE (stock_id, date) IN (
            SELECT stock_id, date FROM stock_prices
            WHERE open <= 0
            LIMIT {batch_size}
        )
        """)

        result = db.execute(delete_query)
        batch_deleted = result.rowcount
        deleted_count += batch_deleted

        db.commit()

        logger.info(f"      âœ… æ‰¹æ¬¡ #{batch_num} å®Œæˆ: åˆªé™¤ {batch_deleted:,} ç­† (ç¸½è¨ˆ: {deleted_count:,}/{total_to_delete:,})")

        # å¦‚æœé€™æ‰¹æ²’åˆªé™¤ä»»ä½•è¨˜éŒ„ï¼Œè¡¨ç¤ºå®Œæˆäº†
        if batch_deleted == 0:
            logger.info("   âœ… æ‰€æœ‰é›¶åƒ¹æ ¼è¨˜éŒ„å·²æ¸…ç†å®Œç•¢")
            break

        # æ¯10æ‰¹é¡¯ç¤ºé€²åº¦
        if batch_num % 10 == 0:
            progress = (deleted_count / total_to_delete * 100) if total_to_delete > 0 else 100
            logger.info(f"   ğŸ“Š é€²åº¦: {progress:.1f}% ({deleted_count:,}/{total_to_delete:,})")

    return {
        "total_deleted": deleted_count,
        "batches": batch_num,
        **initial_stats
    }


def main(dry_run: bool = False, batch_size: int = 10000):
    """
    ä¸»æ¸…ç†å‡½æ•¸

    Args:
        dry_run: If True, only show what would be cleaned
        batch_size: Number of records to delete per batch
    """
    db = SessionLocal()

    try:
        logger.info("=" * 60)
        logger.info("ğŸ”§ é–‹å§‹æ¸…ç†é›¶åƒ¹æ ¼è¨˜éŒ„")
        if dry_run:
            logger.warning("âš ï¸  DRY RUN MODE - ä¸æœƒå¯¦éš›ä¿®æ”¹è³‡æ–™åº«")
        logger.info("=" * 60)

        # åŸ·è¡Œæ¸…ç†
        result = cleanup_zero_prices(db, dry_run=dry_run, batch_size=batch_size)

        # ç¸½çµ
        logger.info("\n" + "=" * 60)
        logger.info("âœ… æ¸…ç†å®Œæˆï¼")
        logger.info("=" * 60)

        if not dry_run:
            logger.info(f"åˆªé™¤è¨˜éŒ„æ•¸: {result['total_deleted']:,}")
            logger.info(f"åŸ·è¡Œæ‰¹æ¬¡æ•¸: {result['batches']}")
            logger.info(f"å½±éŸ¿è‚¡ç¥¨æ•¸: {result['affected_stocks']}")
        else:
            logger.info(f"å°‡åˆªé™¤è¨˜éŒ„æ•¸: {result['total_zero_records']:,}")
            logger.info(f"å½±éŸ¿è‚¡ç¥¨æ•¸: {result['affected_stocks']}")
            logger.info(f"æ—¥æœŸç¯„åœ: {result['earliest_date']} ~ {result['latest_date']}")
            logger.warning("âš ï¸  é€™æ˜¯ DRY RUNï¼Œå¯¦éš›è³‡æ–™åº«æœªè¢«ä¿®æ”¹")
            logger.info("ğŸ’¡ åŸ·è¡Œ --no-dry-run ä»¥å¯¦éš›æ¸…ç†è³‡æ–™")

        # é©—è­‰æ¸…ç†å¾Œçš„ç‹€æ…‹
        if not dry_run:
            logger.info("\nğŸ“Š é©—è­‰æ¸…ç†çµæœ...")
            remaining_stats = get_zero_price_stats(db)
            logger.info(f"   å‰©é¤˜é›¶åƒ¹æ ¼è¨˜éŒ„: {remaining_stats['total_zero_records']:,}")

            if remaining_stats['total_zero_records'] == 0:
                logger.info("   âœ… ç¢ºèªï¼šæ‰€æœ‰é›¶åƒ¹æ ¼è¨˜éŒ„å·²æ¸…é™¤ï¼")
            else:
                logger.warning(f"   âš ï¸  ä»æœ‰ {remaining_stats['total_zero_records']:,} ç­†é›¶åƒ¹æ ¼è¨˜éŒ„")

    except Exception as e:
        logger.error(f"âŒ æ¸…ç†å¤±æ•—: {e}")
        db.rollback()
        raise

    finally:
        db.close()


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='æ¸…ç†é›¶åƒ¹æ ¼è¨˜éŒ„')
    parser.add_argument(
        '--dry-run',
        action='store_true',
        default=True,
        help='æ¼”ç·´æ¨¡å¼ï¼ˆä¸ä¿®æ”¹è³‡æ–™åº«ï¼Œé è¨­å•Ÿç”¨ï¼‰'
    )
    parser.add_argument(
        '--no-dry-run',
        action='store_false',
        dest='dry_run',
        help='å¯¦éš›åŸ·è¡Œæ¸…ç†ï¼ˆæœƒä¿®æ”¹è³‡æ–™åº«ï¼‰'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=10000,
        help='æ¯æ‰¹åˆªé™¤çš„è¨˜éŒ„æ•¸ï¼ˆé è¨­ï¼š10000ï¼‰'
    )

    args = parser.parse_args()

    main(dry_run=args.dry_run, batch_size=args.batch_size)
