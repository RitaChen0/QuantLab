#!/usr/bin/env python3
"""
Shioaji åˆ° Qlib ç¨ç«‹åŒæ­¥å·¥å…·ï¼ˆæ™ºæ…§å¢é‡åŒæ­¥ç‰ˆï¼‰

åŠŸèƒ½ï¼š
1. å¾ Shioaji API ç²å–è‚¡ç¥¨ 1 åˆ†é˜ K ç·šæ•¸æ“š
2. åŒæ™‚å­˜å„²åˆ° PostgreSQL å’Œ Qlib äºŒé€²åˆ¶æ ¼å¼
3. ğŸ§  æ™ºæ…§å¢é‡åŒæ­¥ï¼šè‡ªå‹•æª¢æ¸¬ç¾æœ‰æ•¸æ“šçš„æœ€å¾Œæ—¥æœŸï¼Œåƒ…åŒæ­¥ç¼ºå¤±éƒ¨åˆ†
4. å°ˆé–€åœ¨æ”¶ç›¤å¾Œé‹è¡Œï¼Œæˆªå–ç•¶å¤©æ‰€æœ‰è‚¡ç¥¨çš„åˆ†é˜ç·šè³‡æ–™

ä½¿ç”¨ç¯„ä¾‹ï¼š
    # ğŸ§  æ™ºæ…§å¢é‡åŒæ­¥ï¼ˆæ¨è–¦ï¼Œæ”¶ç›¤å¾Œé‹è¡Œï¼‰
    python sync_shioaji_to_qlib.py --smart

    # æ™ºæ…§åŒæ­¥åˆ°æŒ‡å®šæ—¥æœŸ
    python sync_shioaji_to_qlib.py --smart --end-date 2025-12-13

    # å‚³çµ±æ¨¡å¼ï¼šåŒæ­¥ä»Šå¤©çš„æ•¸æ“š
    python sync_shioaji_to_qlib.py --today

    # åŒæ­¥æŒ‡å®šæ—¥æœŸç¯„åœ
    python sync_shioaji_to_qlib.py --start-date 2025-12-01 --end-date 2025-12-13

    # æ¸¬è©¦æ¨¡å¼ï¼ˆåƒ…åŒæ­¥ 5 æª”è‚¡ç¥¨ï¼‰
    python sync_shioaji_to_qlib.py --smart --test

å®šæ™‚ä»»å‹™ï¼ˆCronï¼‰ï¼š
    # æ¯å€‹äº¤æ˜“æ—¥ 15:00 è‡ªå‹•æ™ºæ…§å¢é‡åŒæ­¥
    0 15 * * 1-5 cd /home/ubuntu/QuantLab/backend && python scripts/sync_shioaji_to_qlib.py --smart
"""

import sys
import os
from pathlib import Path
from datetime import datetime, date, timedelta
from typing import List, Optional, Tuple
import argparse
import time
import struct

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd
import numpy as np
from loguru import logger
from tqdm import tqdm
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

# QuantLab æ¨¡çµ„
from app.core.config import settings
from app.db.base import import_models
from app.services.shioaji_client import ShioajiClient
from app.repositories.stock_minute_price import StockMinutePriceRepository
from app.schemas.stock_minute_price import StockMinutePriceCreate

# Qlib æ¨¡çµ„
import qlib
from qlib.config import REG_CN
from qlib.data.storage.file_storage import FileFeatureStorage
from qlib.data import D

# å°å…¥æ‰€æœ‰æ¨¡å‹
import_models()

# Qlib ç‰¹å¾µåˆ—è¡¨ï¼ˆåˆ†é˜ç·šï¼‰
QLIB_MINUTE_FEATURES = ['open', 'high', 'low', 'close', 'volume']

# æ—¥èªŒé…ç½®
logger.remove()
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
    level="INFO"
)
logger.add(
    "/tmp/shioaji_to_qlib_{time}.log",
    rotation="1 day",
    retention="7 days",
    level="DEBUG"
)


class ShioajiToQlibSyncer:
    """Shioaji åˆ° Qlib åŒæ­¥å™¨ï¼ˆæ”¯æ´æ™ºæ…§å¢é‡åŒæ­¥ï¼‰

    å¯ä»¥ä½œç‚ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ä½¿ç”¨ä»¥ç¢ºä¿è³‡æºæ­£ç¢ºé‡‹æ”¾ï¼š
        with ShioajiToQlibSyncer() as syncer:
            syncer.sync_all(...)
    """

    def __init__(
        self,
        qlib_data_dir: str = "/data/qlib/tw_stock_minute",
        db_url: Optional[str] = None,
        skip_db: bool = False,
        verbose: bool = False
    ):
        """
        åˆå§‹åŒ–åŒæ­¥å™¨

        Args:
            qlib_data_dir: Qlib æ•¸æ“šç›®éŒ„
            db_url: è³‡æ–™åº«é€£æ¥å­—ä¸²ï¼ˆNone å‰‡ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ï¼‰
            skip_db: æ˜¯å¦è·³éè³‡æ–™åº«å­˜å„²ï¼ˆåƒ…æ›´æ–° Qlibï¼‰
            verbose: æ˜¯å¦è¼¸å‡ºè©³ç´°æ—¥èªŒï¼ˆé»˜èª Falseï¼Œé©åˆå¤§é‡è‚¡ç¥¨åŒæ­¥ï¼‰
        """
        self.verbose = verbose

        logger.info("=" * 60)
        logger.info("ğŸ”§ åˆå§‹åŒ– Shioaji â†’ Qlib åŒæ­¥å™¨...")
        logger.info("=" * 60)

        self.qlib_data_dir = Path(qlib_data_dir)
        self.skip_db = skip_db
        logger.info(f"ğŸ“ Qlib æ•¸æ“šç›®éŒ„: {qlib_data_dir}")
        if verbose:
            logger.info(f"ğŸ“ è©³ç´°æ—¥èªŒæ¨¡å¼: å•Ÿç”¨")

        # åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥
        if not skip_db:
            logger.info("ğŸ—„ï¸  æ­£åœ¨é€£æ¥ PostgreSQL...")
            try:
                self.db_url = db_url or settings.DATABASE_URL
                # è¨­ç½®é€£æ¥è¶…æ™‚å’Œé€£æ¥æ± åƒæ•¸
                self.engine = create_engine(
                    self.db_url,
                    pool_pre_ping=True,
                    pool_size=5,
                    max_overflow=10,
                    pool_recycle=3600,  # 1å°æ™‚å›æ”¶é€£æ¥
                    connect_args={
                        'connect_timeout': 10,  # é€£æ¥è¶…æ™‚ 10 ç§’
                        'options': '-c statement_timeout=120000'  # SQL èªå¥è¶…æ™‚ 120 ç§’ï¼ˆå¤§è¡¨æŸ¥è©¢å¯èƒ½è¼ƒæ…¢ï¼‰
                    }
                )
                SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=self.engine)
                self.db_session = SessionLocal()
                self.repo = StockMinutePriceRepository  # éœæ…‹æ–¹æ³•ï¼Œä¸éœ€å¯¦ä¾‹åŒ–
                logger.info("âœ… PostgreSQL é€£æ¥æˆåŠŸ (è¶…æ™‚è¨­ç½®: é€£æ¥ 10s, æŸ¥è©¢ 120s)")
            except Exception as e:
                logger.error(f"âŒ PostgreSQL é€£æ¥å¤±æ•—: {e}")
                raise
        else:
            logger.info("âš ï¸  è·³éè³‡æ–™åº«å­˜å„²ï¼Œåƒ…æ›´æ–° Qlib")
            self.engine = None
            self.db_session = None
            self.repo = None

        # åˆå§‹åŒ– Qlib
        self._init_qlib()

        # Shioaji å®¢æˆ¶ç«¯ï¼ˆå»¶é²åˆå§‹åŒ–ï¼‰
        self.shioaji_client = None
        logger.info("â³ Shioaji å®¢æˆ¶ç«¯å°‡åœ¨é¦–æ¬¡ä½¿ç”¨æ™‚åˆå§‹åŒ–")

    def _init_qlib(self):
        """åˆå§‹åŒ– Qlib ç’°å¢ƒ"""
        logger.info("ğŸ“Š æ­£åœ¨åˆå§‹åŒ– Qlib...")
        try:
            # æª¢æŸ¥ç›®éŒ„æ˜¯å¦å­˜åœ¨
            if not self.qlib_data_dir.exists():
                logger.info(f"   å‰µå»º Qlib ç›®éŒ„: {self.qlib_data_dir}")
                self.qlib_data_dir.mkdir(parents=True, exist_ok=True)
            else:
                logger.info(f"   Qlib ç›®éŒ„å·²å­˜åœ¨: {self.qlib_data_dir}")

            # åˆå§‹åŒ– Qlib
            qlib.init(provider_uri=str(self.qlib_data_dir), region=REG_CN)
            logger.info(f"âœ… Qlib åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ Qlib åˆå§‹åŒ–å¤±æ•—: {e}")
            logger.exception("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
            raise

    def get_stock_list(self) -> List[str]:
        """
        å¾è³‡æ–™åº«ç²å–è‚¡ç¥¨æ¸…å–®

        Returns:
            è‚¡ç¥¨ä»£ç¢¼åˆ—è¡¨
        """
        logger.info("ğŸ“‹ æ­£åœ¨ç²å–è‚¡ç¥¨æ¸…å–®...")

        if self.skip_db or not self.engine:
            logger.warning("âš ï¸  ç„¡æ³•å¾è³‡æ–™åº«ç²å–è‚¡ç¥¨æ¸…å–®ï¼Œè¿”å› Top 50")
            # Fallback: è¿”å›ç†±é–€è‚¡ç¥¨
            return [
                '2330', '2317', '2454', '2412', '3008',
                '2308', '2882', '1301', '1303', '2002',
                '2886', '2881', '2891', '2892', '2885',
                '2884', '2887', '2883', '5880', '2912',
                '2880', '2382', '2395', '6505', '3045',
                '1216', '2357', '1326', '2303', '2379',
                '2408', '2207', '2327', '3711', '2474',
                '2801', '2609', '2615', '2603', '4904',
                '9910', '2888', '2345', '6669', '2409',
                '3037', '2377', '2353', '5871', '2324',
            ]

        try:
            logger.info("   æŸ¥è©¢è³‡æ–™åº« stock_prices è¡¨...")
            with self.engine.connect() as conn:
                result = conn.execute(text("""
                    SELECT DISTINCT stock_id
                    FROM stock_prices
                    ORDER BY stock_id
                """))
                stock_ids = [row[0] for row in result.fetchall()]
                logger.info(f"âœ… å¾è³‡æ–™åº«ç²å– {len(stock_ids)} æª”è‚¡ç¥¨")
                if stock_ids:
                    logger.info(f"   ç¯„åœ: {stock_ids[0]} ~ {stock_ids[-1]}")
                return stock_ids
        except Exception as e:
            logger.error(f"âŒ ç²å–è‚¡ç¥¨æ¸…å–®å¤±æ•—: {e}")
            logger.exception("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
            return []

    def get_db_last_date(self, stock_id: str) -> Optional[date]:
        """
        ç²å– PostgreSQL ä¸­è©²è‚¡ç¥¨çš„æœ€å¾Œæ—¥æœŸ

        Args:
            stock_id: è‚¡ç¥¨ä»£ç¢¼

        Returns:
            æœ€å¾Œæ—¥æœŸæˆ– None
        """
        if self.skip_db or not self.engine:
            return None

        try:
            with self.engine.connect() as conn:
                result = conn.execute(text("""
                    SELECT MAX(datetime::date) as last_date
                    FROM stock_minute_prices
                    WHERE stock_id = :stock_id
                """), {"stock_id": stock_id})
                row = result.fetchone()
                if row and row[0]:
                    return row[0]
                return None
        except Exception as e:
            logger.debug(f"ç„¡æ³•ç²å– {stock_id} çš„ PostgreSQL æœ€å¾Œæ—¥æœŸ: {e}")
            return None

    def get_qlib_last_date(self, stock_id: str) -> Optional[date]:
        """
        ç²å– Qlib ä¸­è©²è‚¡ç¥¨çš„æœ€å¾Œæ—¥æœŸ

        Args:
            stock_id: è‚¡ç¥¨ä»£ç¢¼

        Returns:
            æœ€å¾Œæ—¥æœŸæˆ– None
        """
        try:
            # å˜—è©¦è®€å–è©²è‚¡ç¥¨çš„æ”¶ç›¤åƒ¹æ•¸æ“š
            df = D.features([stock_id], ['$close'], freq='1min')

            if df is None or df.empty:
                return None

            # ç²å–æœ€å¾Œä¸€å€‹æ—¥æœŸæ™‚é–“
            last_datetime = df.index.get_level_values('datetime').max()
            return last_datetime.date()
        except Exception:
            # å¦‚æœè®€å–å¤±æ•—ï¼Œè¡¨ç¤ºæ•¸æ“šä¸å­˜åœ¨
            return None

    def determine_sync_range(
        self,
        stock_id: str,
        user_end_date: date,
        smart_mode: bool = False
    ) -> Tuple[Optional[date], Optional[date], str]:
        """
        æ™ºæ…§åˆ¤æ–·éœ€è¦åŒæ­¥çš„æ—¥æœŸç¯„åœ

        Args:
            stock_id: è‚¡ç¥¨ä»£ç¢¼
            user_end_date: ç”¨æˆ¶æŒ‡å®šçš„çµæŸæ—¥æœŸï¼ˆé€šå¸¸æ˜¯ä»Šå¤©ï¼‰
            smart_mode: æ˜¯å¦ä½¿ç”¨æ™ºæ…§æ¨¡å¼

        Returns:
            (é–‹å§‹æ—¥æœŸ, çµæŸæ—¥æœŸ, åŒæ­¥é¡å‹)
            åŒæ­¥é¡å‹: 'full', 'incremental', 'skip'
        """
        if not smart_mode:
            # éæ™ºæ…§æ¨¡å¼ï¼Œè¿”å› None è¡¨ç¤ºä½¿ç”¨ç”¨æˆ¶æŒ‡å®šçš„æ—¥æœŸç¯„åœ
            return (None, None, 'user_specified')

        # æª¢æŸ¥ PostgreSQL æœ€å¾Œæ—¥æœŸ
        db_last_date = self.get_db_last_date(stock_id)

        # æª¢æŸ¥ Qlib æœ€å¾Œæ—¥æœŸ
        qlib_last_date = self.get_qlib_last_date(stock_id)

        # å–å…©è€…ä¸­è¼ƒæ—©çš„æ—¥æœŸä½œç‚ºåƒè€ƒé»
        if db_last_date and qlib_last_date:
            last_date = min(db_last_date, qlib_last_date)
        elif db_last_date:
            last_date = db_last_date
        elif qlib_last_date:
            last_date = qlib_last_date
        else:
            # å®Œå…¨æ²’æœ‰æ•¸æ“šï¼Œé¦–æ¬¡åŒæ­¥ï¼ˆé è¨­å¾ 6 å€‹æœˆå‰é–‹å§‹ï¼‰
            start_date = user_end_date - timedelta(days=180)
            return (start_date, user_end_date, 'full')

        # æª¢æŸ¥æ˜¯å¦å·²æ˜¯æœ€æ–°ï¼ˆä½¿ç”¨åš´æ ¼çš„ > è€Œé >=ï¼‰
        if last_date > user_end_date:
            return (None, None, 'skip')

        # å¢é‡åŒæ­¥ï¼ˆå¾æœ€å¾Œæ—¥æœŸé–‹å§‹ï¼Œå…è¨±è¦†è“‹æœ€å¾Œä¸€å¤©ï¼Œç¢ºä¿å¹‚ç­‰æ€§ï¼‰
        # æ³¨æ„ï¼šä½¿ç”¨ ON CONFLICT DO NOTHINGï¼Œé‡è¤‡æ•¸æ“šæœƒè¢«è‡ªå‹•è·³é
        start_date = last_date
        return (start_date, user_end_date, 'incremental')

    def _is_futures(self, stock_id: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºæœŸè²¨"""
        return stock_id in ['TX', 'MTX']

    def _get_contract_type(self, stock_id: str) -> str:
        """ç²å–å¥‘ç´„é¡å‹"""
        return 'futures' if self._is_futures(stock_id) else 'stock'

    def fetch_minute_data(
        self,
        stock_id: str,
        start_date: date,
        end_date: date,
        max_retries: int = 3,
        retry_delay: float = 2.0
    ) -> Optional[Tuple[pd.DataFrame, str]]:
        """
        å¾ Shioaji API ç²å–åˆ†é˜ K ç·šæ•¸æ“šï¼ˆæ”¯æŒè‚¡ç¥¨å’ŒæœŸè²¨ï¼‰

        Args:
            stock_id: æ¨™çš„ä»£ç¢¼ï¼ˆè‚¡ç¥¨æˆ–æœŸè²¨ï¼‰
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ
            max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼ˆé»˜èª 3 æ¬¡ï¼‰
            retry_delay: é‡è©¦å»¶é²ç§’æ•¸ï¼ˆé»˜èª 2 ç§’ï¼‰

        Returns:
            (DataFrame, actual_stock_id): æ•¸æ“šå’Œå¯¦éš›æ¨™çš„ä»£ç¢¼
            - è‚¡ç¥¨: ("2330", "2330")
            - æœŸè²¨: ("TX", "TX202512")  â† è¿”å›å¯¦éš›æœˆä»½åˆç´„ä»£ç¢¼
        """
        if self.verbose:
            logger.info(f"  ğŸ“¡ [API] æ­£åœ¨ç²å– {stock_id} æ•¸æ“š ({start_date} ~ {end_date})...")

        # åˆå§‹åŒ– Shioaji å®¢æˆ¶ç«¯
        if not self.shioaji_client:
            logger.info("  ğŸ”Œ é¦–æ¬¡èª¿ç”¨ï¼Œåˆå§‹åŒ– Shioaji å®¢æˆ¶ç«¯...")
            try:
                start_init = time.time()
                self.shioaji_client = ShioajiClient()
                init_elapsed = time.time() - start_init
                logger.info(f"  âœ… Shioaji å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸ ({init_elapsed:.1f}s)")
            except Exception as e:
                logger.error(f"  âŒ Shioaji å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")
                logger.exception("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
                return None

        if not self.shioaji_client.is_available():
            logger.error("  âŒ Shioaji å®¢æˆ¶ç«¯æœªå°±ç·’")
            return None

        # åˆ¤æ–·å¥‘ç´„é¡å‹
        contract_type = self._get_contract_type(stock_id)
        is_futures = self._is_futures(stock_id)
        if self.verbose:
            logger.debug(f"  ğŸ“ å¥‘ç´„é¡å‹: {'æœŸè²¨' if is_futures else 'è‚¡ç¥¨'}")

        # å°æ–¼æœŸè²¨ï¼Œç²å–å¯¦éš›æœˆä»½åˆç´„ä»£ç¢¼
        actual_stock_id = stock_id
        if is_futures:
            logger.info(f"  ğŸ” æŸ¥è©¢æœŸè²¨åˆç´„ä»£ç¢¼...")
            try:
                contract_id = self.shioaji_client.get_futures_contract_id(stock_id)
                if contract_id:
                    actual_stock_id = contract_id
                    logger.info(f"  âœ… [CONTRACT] {stock_id} â†’ {actual_stock_id}")
                else:
                    logger.error(f"  âŒ [CONTRACT] ç„¡æ³•å–å¾— {stock_id} çš„åˆç´„ä»£ç¢¼")
                    return None
            except Exception as e:
                logger.error(f"  âŒ [CONTRACT] æŸ¥è©¢åˆç´„å¤±æ•—: {e}")
                logger.exception("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
                return None

        try:
            # è¨­å®šæ™‚é–“ç¯„åœ
            # æœŸè²¨ï¼š08:45-æ¬¡æ—¥05:00ï¼ˆå®Œæ•´æ—¥ç›¤ + å¤œç›¤ï¼‰
            # è‚¡ç¥¨ï¼š09:00-13:30ï¼ˆåƒ…æ—¥ç›¤ï¼‰
            if is_futures:
                start_datetime = datetime.combine(start_date, datetime.min.time().replace(hour=8, minute=45))
                # æœŸè´§å¤œç›˜å»¶ç»­åˆ°æ¬¡æ—¥ 05:00ï¼Œå› æ­¤ end_datetime éœ€è¦ +1 å¤©
                end_datetime = datetime.combine(end_date + timedelta(days=1), datetime.min.time().replace(hour=5, minute=0))
                logger.debug(f"  ğŸ“… æœŸè²¨æ™‚é–“ç¯„åœ: {start_datetime} ~ {end_datetime}ï¼ˆå«å®Œæ•´å¤œç›¤ï¼‰")
            else:
                start_datetime = datetime.combine(start_date, datetime.min.time().replace(hour=9, minute=0))
                end_datetime = datetime.combine(end_date, datetime.min.time().replace(hour=13, minute=30))
                logger.debug(f"  ğŸ“… è‚¡ç¥¨æ™‚é–“ç¯„åœ: {start_datetime} ~ {end_datetime}")

            # èª¿ç”¨ Shioaji APIï¼ˆå¸¶é‡è©¦æ©Ÿåˆ¶ï¼‰
            df = None
            last_error = None

            for attempt in range(max_retries):
                try:
                    if self.verbose and attempt > 0:
                        logger.info(f"  ğŸ”„ é‡è©¦ {attempt}/{max_retries}...")

                    if self.verbose:
                        logger.info(f"  â³ æ­£åœ¨èª¿ç”¨ Shioaji API...")
                    api_start = time.time()

                    # èª¿ç”¨ API
                    df = self.shioaji_client.get_kbars(
                        stock_id=stock_id,
                        start_datetime=start_datetime,
                        end_datetime=end_datetime,
                        timeframe='1min',
                        contract_type=contract_type
                    )

                    api_elapsed = time.time() - api_start
                    if self.verbose:
                        logger.info(f"  â±ï¸  API éŸ¿æ‡‰æ™‚é–“: {api_elapsed:.1f}s")

                    # æˆåŠŸç²å–æ•¸æ“šï¼Œè·³å‡ºé‡è©¦å¾ªç’°
                    if df is not None and not df.empty:
                        if self.verbose:
                            logger.info(f"  âœ… {stock_id}: æˆåŠŸç²å– {len(df)} ç­†åˆ†é˜æ•¸æ“š")
                        return df, actual_stock_id
                    else:
                        # API è¿”å›ç©ºæ•¸æ“šï¼Œä¸éœ€è¦é‡è©¦
                        if self.verbose:
                            logger.warning(f"  âš ï¸  {stock_id}: API è¿”å›ç„¡æ•¸æ“š")
                        return None

                except (TimeoutError, ConnectionError) as e:
                    last_error = e
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (2 ** attempt)  # æŒ‡æ•¸é€€é¿
                        logger.warning(f"  âš ï¸  {stock_id}: {type(e).__name__} - ç­‰å¾… {wait_time:.1f}s å¾Œé‡è©¦...")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"  âŒ {stock_id}: {type(e).__name__} (å·²é‡è©¦ {max_retries} æ¬¡) - {e}")
                        return None

                except Exception as e:
                    # å…¶ä»–éŒ¯èª¤ä¸é‡è©¦
                    logger.error(f"  âŒ {stock_id}: ç²å–æ•¸æ“šå¤±æ•— - {e}")
                    logger.exception("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
                    return None

            # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—
            if last_error:
                logger.error(f"  âŒ {stock_id}: API èª¿ç”¨å¤±æ•— (å·²é‡è©¦ {max_retries} æ¬¡)")
            return None

        except Exception as e:
            logger.error(f"  âŒ {stock_id}: è™•ç†å¤±æ•— - {e}")
            logger.exception("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
            return None

    def save_to_postgresql(self, stock_id: str, df: pd.DataFrame) -> int:
        """
        ä¿å­˜æ•¸æ“šåˆ° PostgreSQLï¼ˆä½¿ç”¨ ON CONFLICT å¿½ç•¥é‡è¤‡ï¼‰

        ä½¿ç”¨ç­–ç•¥ï¼š
        1. ä½¿ç”¨ SQLAlchemy Core çš„ INSERT ... ON CONFLICT DO NOTHING
        2. å‘é‡åŒ–æ•¸æ“šæº–å‚™ï¼ˆé¿å… iterrowsï¼Œæ€§èƒ½æå‡ 100 å€ï¼‰
        3. è™•ç† NaN å€¼ï¼ˆé˜²æ­¢é‹è¡Œæ™‚éŒ¯èª¤ï¼‰

        Args:
            stock_id: è‚¡ç¥¨ä»£ç¢¼
            df: æ•¸æ“š DataFrame

        Returns:
            æˆåŠŸæ’å…¥çš„è¨˜éŒ„æ•¸
        """
        if self.skip_db or not self.repo:
            logger.debug(f"  â­ï¸  è·³éè³‡æ–™åº«å­˜å„²")
            return 0

        logger.info(f"  ğŸ’¾ [DB] æ­£åœ¨ä¿å­˜åˆ° PostgreSQL...")

        try:
            if df.empty:
                logger.warning(f"  âš ï¸  DataFrame ç‚ºç©ºï¼Œç„¡æ³•ä¿å­˜")
                return 0

            from sqlalchemy.dialects.postgresql import insert
            from app.models.stock_minute_price import StockMinutePrice

            # å‘é‡åŒ–æº–å‚™æ•¸æ“šï¼ˆæ¯” iterrows å¿« 100 å€ï¼‰
            logger.debug(f"  ğŸ“ æº–å‚™æ•¸æ“šï¼ˆ{len(df)} ç­†ï¼‰...")
            df_copy = df.copy()
            df_copy['stock_id'] = stock_id
            df_copy['timeframe'] = '1min'

            # ç¢ºä¿æ•¸æ“šé¡å‹æ­£ç¢º
            df_copy['open'] = df_copy['open'].astype(float)
            df_copy['high'] = df_copy['high'].astype(float)
            df_copy['low'] = df_copy['low'].astype(float)
            df_copy['close'] = df_copy['close'].astype(float)

            # è™•ç† NaN å€¼ï¼ˆvolume å¿…é ˆæ˜¯æ•´æ•¸ï¼‰
            df_copy['volume'] = df_copy['volume'].fillna(0).astype(int)

            # é¸æ“‡éœ€è¦çš„æ¬„ä½ä¸¦è½‰æ›ç‚ºå­—å…¸åˆ—è¡¨
            records = df_copy[['stock_id', 'datetime', 'timeframe', 'open', 'high', 'low', 'close', 'volume']].to_dict('records')

            # åˆ†æ‰¹æ’å…¥ï¼ˆæ¯æ‰¹ 1,000 ç­†ï¼‰
            batch_size = 1000
            total_inserted = 0
            num_batches = (len(records) + batch_size - 1) // batch_size

            logger.info(f"  ğŸ“¦ åˆ†æ‰¹æ’å…¥ï¼ˆ{num_batches} æ‰¹ï¼Œæ¯æ‰¹ {batch_size} ç­†ï¼‰...")
            db_start = time.time()

            for i in range(0, len(records), batch_size):
                batch = records[i:i + batch_size]
                batch_num = i // batch_size + 1

                try:
                    # ä½¿ç”¨ SQLAlchemy Core çš„ ON CONFLICT DO UPDATE
                    # å…è¨±æ›´æ–°æ•¸æ“šæºä¿®æ­£çš„æ­·å²æ•¸æ“š
                    stmt = insert(StockMinutePrice).values(batch)
                    stmt = stmt.on_conflict_do_update(
                        index_elements=['stock_id', 'datetime', 'timeframe'],
                        set_={
                            'open': stmt.excluded.open,
                            'high': stmt.excluded.high,
                            'low': stmt.excluded.low,
                            'close': stmt.excluded.close,
                            'volume': stmt.excluded.volume,
                        }
                    )

                    result = self.db_session.execute(stmt)
                    total_inserted += result.rowcount

                    # æ¯æ‰¹æäº¤ä¸€æ¬¡ï¼ˆé¿å…å¤§äº‹å‹™å°è‡´å…§å­˜æº¢å‡ºï¼‰
                    self.db_session.commit()
                    logger.debug(f"  âœ“ æ‰¹æ¬¡ {batch_num}/{num_batches} å®Œæˆ")

                except Exception as batch_error:
                    self.db_session.rollback()
                    logger.error(f"  âŒ æ‰¹æ¬¡ {batch_num} å¤±æ•—: {batch_error}")
                    continue

            db_elapsed = time.time() - db_start
            skipped = len(records) - total_inserted

            if skipped > 0:
                logger.info(f"  âœ… PostgreSQL: æ’å…¥ {total_inserted} ç­†ï¼ˆè·³é {skipped} ç­†é‡è¤‡ï¼‰({db_elapsed:.1f}s)")
            else:
                logger.info(f"  âœ… PostgreSQL: æ’å…¥ {total_inserted} ç­† ({db_elapsed:.1f}s)")
            return total_inserted

        except Exception as e:
            self.db_session.rollback()
            logger.error(f"  âŒ PostgreSQL: ä¿å­˜å¤±æ•— - {e}")
            logger.exception("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
            return 0

    def save_to_qlib(
        self,
        stock_id: str,
        df: pd.DataFrame,
        trading_minutes: pd.DatetimeIndex
    ) -> bool:
        """
        ä¿å­˜æ•¸æ“šåˆ° Qlib æ ¼å¼

        Args:
            stock_id: è‚¡ç¥¨ä»£ç¢¼
            df: æ•¸æ“š DataFrame
            trading_minutes: å®Œæ•´çš„äº¤æ˜“åˆ†é˜ç´¢å¼•

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        logger.info(f"  ğŸ“Š [QLIB] æ­£åœ¨ä¿å­˜åˆ° Qlib...")

        try:
            instrument = stock_id.lower()

            # å‰µå»ºè‚¡ç¥¨ç›®éŒ„
            features_dir = self.qlib_data_dir / 'features' / instrument
            logger.debug(f"  ğŸ“ ç›®æ¨™ç›®éŒ„: {features_dir}")

            if not features_dir.exists():
                logger.info(f"  â• å‰µå»ºç›®éŒ„: {instrument}")
                features_dir.mkdir(parents=True, exist_ok=True)

            # å°‡ DataFrame å°é½Šåˆ°å®Œæ•´äº¤æ˜“åˆ†é˜ç´¢å¼•
            logger.debug(f"  ğŸ”§ å°é½Šæ™‚é–“ç´¢å¼•ï¼ˆ{len(trading_minutes)} å€‹æ™‚é–“é»ï¼‰...")
            df = df.set_index('datetime')
            df = df.reindex(trading_minutes)

            # ç‚ºæ¯å€‹ç‰¹å¾µå¯«å…¥æ•¸æ“š
            qlib_start = time.time()
            successful_features = 0

            for field in QLIB_MINUTE_FEATURES:
                if field not in df.columns:
                    logger.warning(f"  âš ï¸  æ¬„ä½ {field} ä¸å­˜åœ¨ï¼Œè·³é")
                    continue

                try:
                    # æå–ç‰¹å¾µæ•¸æ“š
                    data = df[field].values.astype(np.float32)

                    # ä½¿ç”¨ FileFeatureStorage å¯«å…¥
                    storage = FileFeatureStorage(
                        instrument=instrument,
                        field=field,
                        freq="1min"
                    )

                    storage.write(data)
                    successful_features += 1
                    logger.debug(f"  âœ“ {field}: å¯«å…¥æˆåŠŸ")

                except Exception as e:
                    logger.error(f"  âŒ Qlib {field}: å¯«å…¥å¤±æ•— - {e}")
                    continue

            qlib_elapsed = time.time() - qlib_start

            if successful_features == len(QLIB_MINUTE_FEATURES):
                logger.info(f"  âœ… Qlib: {len(df)} å€‹æ™‚é–“é»ï¼Œ{successful_features} å€‹ç‰¹å¾µ ({qlib_elapsed:.1f}s)")
                return True
            else:
                logger.warning(f"  âš ï¸  Qlib: éƒ¨åˆ†æˆåŠŸï¼ˆ{successful_features}/{len(QLIB_MINUTE_FEATURES)} å€‹ç‰¹å¾µï¼‰")
                return False

        except Exception as e:
            logger.error(f"  âŒ Qlib: ä¿å­˜å¤±æ•— - {e}")
            logger.exception("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
            return False

    def generate_trading_minutes(
        self,
        start_date: date,
        end_date: date,
        is_futures: bool = False
    ) -> pd.DatetimeIndex:
        """
        ç”Ÿæˆäº¤æ˜“åˆ†é˜ç´¢å¼•ï¼ˆæ”¯æŒè‚¡ç¥¨å’ŒæœŸè´§ï¼‰

        Args:
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ
            is_futures: æ˜¯å¦ç‚ºæœŸè²¨ï¼ˆTrue=æœŸè²¨ï¼ŒFalse=è‚¡ç¥¨ï¼‰

        Returns:
            äº¤æ˜“åˆ†é˜ç´¢å¼•

        äº¤æ˜“æ™‚é–“ï¼š
        - è‚¡ç¥¨ï¼š09:00-13:30
        - æœŸè²¨ï¼šå®Œæ•´äº¤æ˜“æ™‚æ®µï¼ˆæ—¥ç›¤ + å¤œç›¤ï¼‰
          - å¤œç›¤å¾Œæ®µï¼š00:00-05:00
          - æ—¥ç›¤ï¼š08:45-13:45
          - å¤œç›¤å‰æ®µï¼š15:00-23:59
        """
        minutes = []
        current_date = start_date

        if is_futures:
            # æœŸè²¨ï¼šå®Œæ•´äº¤æ˜“æ™‚æ®µï¼ˆæ—¥ç›¤ + å¤œç›¤ï¼‰
            while current_date <= end_date:
                # 1. å¤œç›¤å¾Œæ®µï¼š00:00-05:00ï¼ˆå±¬æ–¼å‰ä¸€äº¤æ˜“æ—¥çš„å¤œç›¤ï¼‰
                for hour in range(0, 5):
                    for minute in range(60):
                        dt = datetime.combine(current_date, datetime.min.time().replace(hour=hour, minute=minute))
                        minutes.append(dt)

                # 05:00 è¨˜éŒ„æœ€å¾Œä¸€åˆ†é˜
                dt = datetime.combine(current_date, datetime.min.time().replace(hour=5, minute=0))
                minutes.append(dt)

                # 2. æ—¥ç›¤ï¼š08:45-13:45
                # 08:45-08:59 (15 åˆ†é’Ÿ)
                for minute in range(45, 60):
                    dt = datetime.combine(current_date, datetime.min.time().replace(hour=8, minute=minute))
                    minutes.append(dt)

                # 09:00-12:00
                for hour in range(9, 12):
                    for minute in range(60):
                        dt = datetime.combine(current_date, datetime.min.time().replace(hour=hour, minute=minute))
                        minutes.append(dt)

                # 12:00-13:45
                for hour in range(12, 14):
                    for minute in range(60):
                        if hour == 13 and minute > 45:
                            break
                        dt = datetime.combine(current_date, datetime.min.time().replace(hour=hour, minute=minute))
                        minutes.append(dt)

                # 3. å¤œç›¤å‰æ®µï¼š15:00-23:59ï¼ˆå±¬æ–¼ç•¶æ—¥äº¤æ˜“ï¼‰
                for hour in range(15, 24):
                    for minute in range(60):
                        dt = datetime.combine(current_date, datetime.min.time().replace(hour=hour, minute=minute))
                        minutes.append(dt)

                current_date += timedelta(days=1)
        else:
            # è‚¡ç¥¨ï¼š09:00-13:30
            while current_date <= end_date:
                # ä¸Šåˆç›¤ï¼š09:00-12:00
                for hour in range(9, 12):
                    for minute in range(60):
                        dt = datetime.combine(current_date, datetime.min.time().replace(hour=hour, minute=minute))
                        minutes.append(dt)

                # ä¸‹åˆç›¤ï¼š12:00-13:30
                for hour in range(12, 14):
                    for minute in range(60):
                        if hour == 13 and minute > 30:
                            break
                        dt = datetime.combine(current_date, datetime.min.time().replace(hour=hour, minute=minute))
                        minutes.append(dt)

                current_date += timedelta(days=1)

        return pd.DatetimeIndex(minutes)

    def sync_stock(
        self,
        stock_id: str,
        start_date: date,
        end_date: date,
        trading_minutes: pd.DatetimeIndex
    ) -> Tuple[int, int]:
        """
        åŒæ­¥å–®ä¸€æ¨™çš„çš„æ•¸æ“šï¼ˆæ”¯æŒè‚¡ç¥¨å’ŒæœŸè´§ï¼‰

        Args:
            stock_id: æ¨™çš„ä»£ç¢¼ï¼ˆè‚¡ç¥¨æˆ–æœŸè´§ï¼‰
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ
            trading_minutes: äº¤æ˜“åˆ†é˜ç´¢å¼•

        Returns:
            (PostgreSQL æ’å…¥æ•¸, Qlib æ˜¯å¦æˆåŠŸ)
        """
        # 1. å¾ Shioaji ç²å–æ•¸æ“š
        result = self.fetch_minute_data(stock_id, start_date, end_date)

        if result is None:
            return (0, 0)

        # ğŸ†• è§£åŒ…è¿”å›å€¼ï¼šDataFrame å’Œå¯¦éš›æ¨™çš„ä»£ç¢¼
        df, actual_stock_id = result

        if df.empty:
            return (0, 0)

        # 2. ä¿å­˜åˆ° PostgreSQLï¼ˆä½¿ç”¨å¯¦éš›åˆç´„ä»£ç¢¼ï¼‰
        db_count = self.save_to_postgresql(actual_stock_id, df) if not self.skip_db else 0

        # 3. ä¿å­˜åˆ° Qlibï¼ˆä½¿ç”¨å¯¦éš›åˆç´„ä»£ç¢¼ï¼‰
        qlib_success = self.save_to_qlib(actual_stock_id, df, trading_minutes)

        return (db_count, 1 if qlib_success else 0)

    def sync_all(
        self,
        stock_ids: List[str],
        user_start_date: Optional[date],
        user_end_date: date,
        smart_mode: bool = False
    ):
        """
        åŒæ­¥æ‰€æœ‰æ¨™çš„çš„æ•¸æ“šï¼ˆæ”¯æŒè‚¡ç¥¨å’ŒæœŸè´§ï¼Œæ”¯æ´æ™ºæ…§æ¨¡å¼ï¼‰

        Args:
            stock_ids: æ¨™çš„ä»£ç¢¼åˆ—è¡¨ï¼ˆè‚¡ç¥¨æˆ–æœŸè´§ï¼‰
            user_start_date: ç”¨æˆ¶æŒ‡å®šçš„é–‹å§‹æ—¥æœŸï¼ˆæ™ºæ…§æ¨¡å¼ä¸‹å¯ç‚º Noneï¼‰
            user_end_date: ç”¨æˆ¶æŒ‡å®šçš„çµæŸæ—¥æœŸ
            smart_mode: æ˜¯å¦ä½¿ç”¨æ™ºæ…§å¢é‡åŒæ­¥
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸš€ é–‹å§‹åŒæ­¥: {len(stock_ids)} æª”æ¨™çš„ï¼ˆè‚¡ç¥¨/æœŸè´§ï¼‰")
        if smart_mode:
            logger.info(f"ğŸ§  æ™ºæ…§æ¨¡å¼: è‡ªå‹•æª¢æ¸¬æ¯æª”æ¨™çš„çš„æœ€å¾Œæ—¥æœŸ")
            logger.info(f"ğŸ“… ç›®æ¨™æ—¥æœŸ: {user_end_date}")
        else:
            logger.info(f"ğŸ“… æ—¥æœŸç¯„åœ: {user_start_date} ~ {user_end_date}")
        logger.info(f"{'='*60}\n")

        # çµ±è¨ˆè®Šé‡
        total_db_count = 0
        total_qlib_count = 0
        error_count = 0
        skipped_count = 0
        full_sync_count = 0
        incremental_sync_count = 0

        # é€²åº¦è¿½è¹¤
        start_time = time.time()
        processed_count = 0
        total_count = len(stock_ids)

        # é€²åº¦æ¢
        progress_bar = tqdm(stock_ids, desc="åŒæ­¥é€²åº¦", unit="æª”")

        for stock_id in progress_bar:
            processed_count += 1

            # è¨ˆç®—é€²åº¦ç™¾åˆ†æ¯”å’Œé ä¼°æ™‚é–“
            progress_pct = (processed_count / total_count) * 100
            elapsed = time.time() - start_time
            if processed_count > 1:
                avg_time_per_stock = elapsed / processed_count
                remaining_stocks = total_count - processed_count
                eta_seconds = avg_time_per_stock * remaining_stocks
                eta_minutes = int(eta_seconds / 60)
                eta_text = f"é ä¼°å‰©é¤˜ {eta_minutes} åˆ†é˜" if eta_minutes > 0 else f"é ä¼°å‰©é¤˜ {int(eta_seconds)} ç§’"
            else:
                eta_text = "è¨ˆç®—ä¸­..."

            # æ›´æ–°é€²åº¦æ¢æè¿°
            progress_bar.set_description(f"[{processed_count}/{total_count}] {stock_id} ({progress_pct:.1f}%)")

            # è¼¸å‡ºè©³ç´°é€²åº¦æ—¥èªŒ
            logger.info(f"\n{'â”€'*60}")
            logger.info(f"ğŸ“Š é€²åº¦: {processed_count}/{total_count} ({progress_pct:.1f}%) | {eta_text}")
            logger.info(f"ğŸ¯ ç•¶å‰æ¨™çš„: {stock_id}")
            logger.info(f"{'â”€'*60}")

            try:
                # åˆ¤æ–·åŒæ­¥ç¯„åœ
                if smart_mode:
                    logger.info(f"  ğŸ” æª¢æŸ¥ {stock_id} çš„ç¾æœ‰æ•¸æ“š...")
                    sync_start, sync_end, sync_type = self.determine_sync_range(
                        stock_id, user_end_date, smart_mode=True
                    )

                    if sync_type == 'skip':
                        skipped_count += 1
                        logger.info(f"  â­ï¸  {stock_id}: å·²æ˜¯æœ€æ–°ï¼Œè·³é")
                        continue

                    if sync_type == 'full':
                        full_sync_count += 1
                        logger.info(f"  ğŸ“¦ {stock_id}: å®Œæ•´åŒæ­¥ ({sync_start} ~ {sync_end})")
                    elif sync_type == 'incremental':
                        incremental_sync_count += 1
                        logger.info(f"  â• {stock_id}: å¢é‡åŒæ­¥ ({sync_start} ~ {sync_end})")
                else:
                    # éæ™ºæ…§æ¨¡å¼ï¼Œä½¿ç”¨ç”¨æˆ¶æŒ‡å®šçš„æ—¥æœŸ
                    sync_start = user_start_date
                    sync_end = user_end_date
                    logger.info(f"  ğŸ“… åŒæ­¥ç¯„åœ: {sync_start} ~ {sync_end}")

                # ç”Ÿæˆäº¤æ˜“åˆ†é˜ç´¢å¼•ï¼ˆæ ¹æ“šæ¨™çš„é¡å‹ï¼‰
                is_futures = self._is_futures(stock_id)
                logger.debug(f"  ğŸ”§ ç”Ÿæˆäº¤æ˜“åˆ†é˜ç´¢å¼•ï¼ˆ{'æœŸè²¨' if is_futures else 'è‚¡ç¥¨'}ï¼‰...")
                trading_minutes = self.generate_trading_minutes(sync_start, sync_end, is_futures=is_futures)
                logger.debug(f"  âœ“ ç”Ÿæˆ {len(trading_minutes)} å€‹æ™‚é–“é»")

                # åŸ·è¡ŒåŒæ­¥
                stock_start = time.time()
                db_count, qlib_count = self.sync_stock(
                    stock_id, sync_start, sync_end, trading_minutes
                )
                stock_elapsed = time.time() - stock_start

                if db_count == 0 and qlib_count == 0:
                    logger.warning(f"  âš ï¸  {stock_id}: ç„¡æ–°æ•¸æ“š ({stock_elapsed:.1f}s)")
                else:
                    total_db_count += db_count
                    total_qlib_count += qlib_count
                    logger.info(f"  âœ… {stock_id}: DB +{db_count}, Qlib {'âœ“' if qlib_count else 'âœ—'} ({stock_elapsed:.1f}s)")

            except Exception as e:
                error_count += 1
                logger.error(f"  âŒ {stock_id}: åŒæ­¥å¤±æ•— - {e}")
                logger.exception("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
                logger.info(f"  â© ç¹¼çºŒè™•ç†ä¸‹ä¸€æª”...")
                continue

        # ç¸½çµ
        total_elapsed = time.time() - start_time
        total_minutes = int(total_elapsed / 60)
        total_seconds = int(total_elapsed % 60)

        logger.info(f"\n{'='*60}")
        logger.info("ğŸ‰ åŒæ­¥å®Œæˆï¼")
        logger.info(f"{'='*60}")
        logger.info(f"â±ï¸  ç¸½åŸ·è¡Œæ™‚é–“: {total_minutes} åˆ† {total_seconds} ç§’")
        logger.info(f"ğŸ“Š è™•ç†çµ±è¨ˆ:")
        if smart_mode:
            logger.info(f"   ğŸ“¦ å®Œæ•´åŒæ­¥: {full_sync_count} æª”")
            logger.info(f"   â• å¢é‡åŒæ­¥: {incremental_sync_count} æª”")
            logger.info(f"   â­ï¸  å·²æœ€æ–°è·³é: {skipped_count} æª”")
        else:
            logger.info(f"   âœ… æˆåŠŸ: {len(stock_ids) - error_count - skipped_count} æª”")
        logger.info(f"   âŒ å¤±æ•—: {error_count} æª”")
        logger.info(f"ğŸ“Š æ•¸æ“šçµ±è¨ˆ:")
        logger.info(f"   ğŸ’¾ PostgreSQL: æ’å…¥ {total_db_count:,} ç­†")
        logger.info(f"   ğŸ“ˆ Qlib: æ›´æ–° {total_qlib_count} æª”")
        if processed_count > 0:
            avg_time = total_elapsed / processed_count
            logger.info(f"ğŸ“ˆ æ•ˆç‡: å¹³å‡æ¯æª” {avg_time:.1f} ç§’")
        logger.info(f"{'='*60}")

    def close(self):
        """é—œé–‰è³‡æº"""
        logger.info("ğŸ”§ æ­£åœ¨é‡‹æ”¾è³‡æº...")

        try:
            if self.shioaji_client and self.shioaji_client.is_available():
                logger.info("  ğŸ“¡ é—œé–‰ Shioaji å®¢æˆ¶ç«¯...")
                self.shioaji_client.__exit__(None, None, None)
                logger.info("  âœ… Shioaji å®¢æˆ¶ç«¯å·²é—œé–‰")
        except Exception as e:
            logger.warning(f"  âš ï¸  é—œé–‰ Shioaji å®¢æˆ¶ç«¯æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        try:
            if self.db_session:
                logger.info("  ğŸ—„ï¸  é—œé–‰è³‡æ–™åº«é€£æ¥...")
                self.db_session.close()
                logger.info("  âœ… è³‡æ–™åº«é€£æ¥å·²é—œé–‰")
        except Exception as e:
            logger.warning(f"  âš ï¸  é—œé–‰è³‡æ–™åº«é€£æ¥æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        try:
            if self.engine:
                logger.info("  ğŸ”Œ é—œé–‰è³‡æ–™åº«å¼•æ“...")
                self.engine.dispose()
                logger.info("  âœ… è³‡æ–™åº«å¼•æ“å·²é—œé–‰")
        except Exception as e:
            logger.warning(f"  âš ï¸  é—œé–‰è³‡æ–™åº«å¼•æ“æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        logger.info("âœ… æ‰€æœ‰è³‡æºå·²é‡‹æ”¾")

    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€²å…¥"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡ºï¼Œç¢ºä¿è³‡æºé‡‹æ”¾"""
        self.close()
        return False  # ä¸æŠ‘åˆ¶ç•°å¸¸


def main():
    parser = argparse.ArgumentParser(
        description='Shioaji åˆ° Qlib ç¨ç«‹åŒæ­¥å·¥å…·ï¼ˆæ™ºæ…§å¢é‡åŒæ­¥ç‰ˆï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹ç”¨æ³•:
  # ğŸ§  æ™ºæ…§å¢é‡åŒæ­¥ï¼ˆæ¨è–¦ï¼‰
  python sync_shioaji_to_qlib.py --smart

  # æ™ºæ…§åŒæ­¥åˆ°æŒ‡å®šæ—¥æœŸ
  python sync_shioaji_to_qlib.py --smart --end-date 2025-12-13

  # å‚³çµ±æ¨¡å¼ï¼šåŒæ­¥ä»Šå¤©çš„æ•¸æ“š
  python sync_shioaji_to_qlib.py --today

  # åŒæ­¥æŒ‡å®šæ—¥æœŸç¯„åœ
  python sync_shioaji_to_qlib.py --start-date 2025-12-01 --end-date 2025-12-13

  # æ¸¬è©¦æ¨¡å¼ï¼ˆåƒ…åŒæ­¥ 5 æª”è‚¡ç¥¨ï¼‰
  python sync_shioaji_to_qlib.py --smart --test
        """
    )

    # æ—¥æœŸç¯„åœåƒæ•¸
    date_group = parser.add_mutually_exclusive_group(required=True)
    date_group.add_argument('--smart', action='store_true',
                           help='ğŸ§  æ™ºæ…§æ¨¡å¼ï¼šè‡ªå‹•æª¢æ¸¬æœ€å¾Œæ—¥æœŸï¼Œåƒ…åŒæ­¥ç¼ºå¤±éƒ¨åˆ†ï¼ˆæ¨è–¦ï¼‰')
    date_group.add_argument('--today', action='store_true', help='åŒæ­¥ä»Šå¤©çš„æ•¸æ“š')
    date_group.add_argument('--yesterday', action='store_true', help='åŒæ­¥æ˜¨å¤©çš„æ•¸æ“š')
    date_group.add_argument('--start-date', type=str, help='é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)')

    parser.add_argument('--end-date', type=str, help='çµæŸæ—¥æœŸ (YYYY-MM-DDï¼Œé è¨­ç‚ºä»Šå¤©)')

    # è‚¡ç¥¨ç¯„åœåƒæ•¸
    parser.add_argument('--stocks', type=str, help='è‚¡ç¥¨ä»£ç¢¼ï¼ˆé€—è™Ÿåˆ†éš”ï¼‰ï¼Œç•™ç©ºå‰‡åŒæ­¥æ‰€æœ‰')
    parser.add_argument('--test', action='store_true', help='æ¸¬è©¦æ¨¡å¼ï¼ˆåƒ…åŒæ­¥å‰ 5 æª”ï¼‰')
    parser.add_argument('--limit', type=int, help='é™åˆ¶åŒæ­¥æ•¸é‡')

    # å­˜å„²é¸é …
    parser.add_argument('--qlib-only', action='store_true', help='åƒ…æ›´æ–° Qlibï¼Œè·³é PostgreSQL')
    parser.add_argument('--qlib-data-dir', type=str, default='/data/qlib/tw_stock_minute',
                        help='Qlib æ•¸æ“šç›®éŒ„ï¼ˆé è¨­: /data/qlib/tw_stock_minuteï¼‰')

    # æ—¥èªŒé¸é …
    parser.add_argument('--verbose', '-v', action='store_true',
                        help='è¼¸å‡ºè©³ç´°æ—¥èªŒï¼ˆé©åˆå°é‡è‚¡ç¥¨åŒæ­¥ï¼Œå¤§é‡åŒæ­¥æ™‚å»ºè­°é—œé–‰ï¼‰')

    args = parser.parse_args()

    # è§£ææ—¥æœŸç¯„åœ
    logger.info("=" * 60)
    logger.info("ğŸ”§ è§£æåŸ·è¡Œåƒæ•¸...")
    logger.info("=" * 60)

    smart_mode = False
    if args.smart:
        smart_mode = True
        start_date = None  # æ™ºæ…§æ¨¡å¼ä¸éœ€è¦ start_date
        end_date = datetime.strptime(args.end_date, '%Y-%m-%d').date() if args.end_date else date.today()
        logger.info(f"âœ… æ¨¡å¼: æ™ºæ…§å¢é‡åŒæ­¥")
        logger.info(f"   ç›®æ¨™æ—¥æœŸ: {end_date}")
    elif args.today:
        start_date = end_date = date.today()
        logger.info(f"âœ… æ¨¡å¼: åŒæ­¥ä»Šå¤© ({date.today()})")
    elif args.yesterday:
        start_date = end_date = date.today() - timedelta(days=1)
        logger.info(f"âœ… æ¨¡å¼: åŒæ­¥æ˜¨å¤© ({start_date})")
    else:
        start_date = datetime.strptime(args.start_date, '%Y-%m-%d').date()
        end_date = datetime.strptime(args.end_date, '%Y-%m-%d').date() if args.end_date else start_date
        logger.info(f"âœ… æ¨¡å¼: æŒ‡å®šæ—¥æœŸç¯„åœ")
        logger.info(f"   é–‹å§‹: {start_date}")
        logger.info(f"   çµæŸ: {end_date}")

    logger.info(f"ğŸ“ Qlib ç›®éŒ„: {args.qlib_data_dir}")
    logger.info(f"ğŸ’¾ è³‡æ–™åº«: {'è·³é' if args.qlib_only else 'å•Ÿç”¨'}")
    logger.info("")

    # åˆå§‹åŒ–åŒæ­¥å™¨
    try:
        syncer = ShioajiToQlibSyncer(
            qlib_data_dir=args.qlib_data_dir,
            skip_db=args.qlib_only,
            verbose=args.verbose
        )
    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–åŒæ­¥å™¨å¤±æ•—: {e}")
        logger.exception("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
        return 1

    # ç²å–è‚¡ç¥¨æ¸…å–®
    try:
        if args.stocks:
            stock_ids = [s.strip() for s in args.stocks.split(',')]
            logger.info(f"âœ… ä½¿ç”¨æŒ‡å®šè‚¡ç¥¨æ¸…å–®: {len(stock_ids)} æª”")
            if len(stock_ids) <= 10:
                logger.info(f"   è‚¡ç¥¨: {', '.join(stock_ids)}")
        else:
            stock_ids = syncer.get_stock_list()

        if not stock_ids:
            logger.error("âŒ è‚¡ç¥¨æ¸…å–®ç‚ºç©ºï¼Œç„¡æ³•ç¹¼çºŒ")
            return 1

        # æ¸¬è©¦æ¨¡å¼
        if args.test:
            stock_ids = stock_ids[:5]
            logger.warning(f"âš ï¸  æ¸¬è©¦æ¨¡å¼: åƒ…åŒæ­¥å‰ {len(stock_ids)} æª”")
            logger.info(f"   è‚¡ç¥¨: {', '.join(stock_ids)}")

        # é™åˆ¶æ•¸é‡
        if args.limit:
            stock_ids = stock_ids[:args.limit]
            logger.warning(f"âš ï¸  é™åˆ¶åŒæ­¥: {args.limit} æª”")

    except Exception as e:
        logger.error(f"âŒ ç²å–è‚¡ç¥¨æ¸…å–®å¤±æ•—: {e}")
        logger.exception("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
        syncer.close()
        return 1

    # é–‹å§‹åŒæ­¥
    logger.info("")
    logger.info("ğŸš€ æº–å‚™é–‹å§‹åŒæ­¥...")
    logger.info("")

    exit_code = 0
    try:
        syncer.sync_all(stock_ids, start_date, end_date, smart_mode=smart_mode)
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ç”¨æˆ¶ä¸­æ–·åŸ·è¡Œ (Ctrl+C)")
        exit_code = 130
    except Exception as e:
        logger.error(f"\nâŒ åŒæ­¥éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logger.exception("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
        exit_code = 1
    finally:
        syncer.close()

    return exit_code


if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)
