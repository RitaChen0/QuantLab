#!/usr/bin/env python3
"""
å¾ TX æœŸè²¨åˆ†é˜ç·šèšåˆç”Ÿæˆæ—¥ç·šæ•¸æ“šä¸¦å¯«å…¥ Qlib æ ¼å¼

ç”¨é€”ï¼šç‚º RD-Agent æä¾›å®Œæ•´çš„ TX æœŸè²¨ OHLCV æ—¥ç·šæ•¸æ“š

æ•¸æ“šä¾†æºï¼šPostgreSQL stock_minute_prices è¡¨ï¼ˆTX202512 æœˆä»½åˆç´„ï¼‰
è¼¸å‡ºæ ¼å¼ï¼šQlib v2 binary format
è¼¸å‡ºä½ç½®ï¼š/data/qlib/tw_stock_v2/features/tx/

ä½¿ç”¨æ–¹å¼ï¼š
    # èšåˆä¸¦å¯«å…¥ï¼ˆé è¨­ä½¿ç”¨ TX202512ï¼‰
    python scripts/generate_tx_daily_from_minute.py

    # ä½¿ç”¨é€£çºŒåˆç´„ï¼ˆTXCONTï¼‰
    python scripts/generate_tx_daily_from_minute.py --contract TXCONT

    # åƒ…é è¦½ï¼ˆä¸å¯«å…¥ï¼‰
    python scripts/generate_tx_daily_from_minute.py --dry-run

    # å¾ Docker åŸ·è¡Œ
    docker compose exec backend python /app/scripts/generate_tx_daily_from_minute.py
"""

import sys
from pathlib import Path

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent.parent))

import argparse
import numpy as np
import pandas as pd
from datetime import datetime
from sqlalchemy import create_engine, text
from loguru import logger

from app.core.config import settings


def aggregate_tx_daily(contract: str = 'TX202512', dry_run: bool = False):
    """
    å¾åˆ†é˜ç·šèšåˆ TX æœŸè²¨æ—¥ç·šæ•¸æ“š

    Args:
        contract: æœŸè²¨åˆç´„ä»£ç¢¼ï¼ˆTX202512, TX202601, TXCONT ç­‰ï¼‰
        dry_run: åƒ…é è¦½ï¼Œä¸å¯«å…¥æª”æ¡ˆ

    Returns:
        DataFrame: èšåˆçš„æ—¥ç·šæ•¸æ“š
    """
    logger.info("=" * 80)
    logger.info("ğŸš€ TX æœŸè²¨æ—¥ç·šæ•¸æ“šèšåˆ")
    logger.info("=" * 80)
    logger.info(f"ğŸ“Š æœŸè²¨åˆç´„ï¼š{contract}")
    logger.info(f"ğŸ”§ æ¨¡å¼ï¼š{'é è¦½æ¨¡å¼ï¼ˆä¸å¯«å…¥ï¼‰' if dry_run else 'æ­£å¼æ¨¡å¼ï¼ˆå¯«å…¥ Qlibï¼‰'}")

    # é€£æ¥è³‡æ–™åº«
    db_url = settings.DATABASE_URL
    engine = create_engine(db_url)

    # æŸ¥è©¢åˆç´„æ•¸æ“šç¯„åœ
    range_query = text("""
        SELECT
            MIN(datetime::date) as earliest,
            MAX(datetime::date) as latest,
            COUNT(DISTINCT datetime::date) as trading_days,
            COUNT(*) as total_bars
        FROM stock_minute_prices
        WHERE stock_id = :contract
    """)

    with engine.connect() as conn:
        range_result = conn.execute(range_query, {"contract": contract}).fetchone()

    if not range_result or not range_result[0]:
        logger.error(f"âŒ æ‰¾ä¸åˆ°åˆç´„ {contract} çš„åˆ†é˜ç·šæ•¸æ“š")
        logger.info("ğŸ’¡ æç¤ºï¼šè«‹æª¢æŸ¥åˆç´„ä»£ç¢¼æ˜¯å¦æ­£ç¢ºï¼Œæˆ–å…ˆåŒæ­¥åˆ†é˜ç·šæ•¸æ“š")
        return None

    earliest, latest, trading_days, total_bars = range_result
    logger.info(f"ğŸ“… æ•¸æ“šç¯„åœï¼š{earliest} ~ {latest}")
    logger.info(f"ğŸ“ˆ äº¤æ˜“æ—¥æ•¸ï¼š{trading_days} å¤©")
    logger.info(f"ğŸ“Š åˆ†é˜ç·šæ•¸ï¼š{total_bars:,} ç­†")

    if trading_days < 60:
        logger.warning(f"âš ï¸  äº¤æ˜“æ—¥æ•¸è¼ƒå°‘ï¼ˆ{trading_days} < 60ï¼‰ï¼Œå¯èƒ½å½±éŸ¿ RD-Agent å› å­æ¸¬è©¦æº–ç¢ºæ€§")

    # èšåˆæ—¥ç·šæ•¸æ“šï¼ˆOHLCVï¼‰
    logger.info("")
    logger.info("ğŸ”„ é–‹å§‹èšåˆæ—¥ç·šæ•¸æ“š...")

    agg_query = text("""
        SELECT
            datetime::date as date,
            (ARRAY_AGG(open ORDER BY datetime ASC))[1] as open,
            MAX(high) as high,
            MIN(low) as low,
            (ARRAY_AGG(close ORDER BY datetime DESC))[1] as close,
            SUM(volume) as volume
        FROM stock_minute_prices
        WHERE stock_id = :contract
        GROUP BY datetime::date
        ORDER BY date
    """)

    logger.info(f"ğŸ“Š åŸ·è¡Œèšåˆ SQL...")
    df = pd.read_sql(agg_query, engine, params={"contract": contract}, index_col='date')

    if df.empty:
        logger.error(f"âŒ èšåˆçµæœç‚ºç©º")
        return None

    logger.info(f"âœ… èšåˆå®Œæˆï¼š{len(df)} å€‹äº¤æ˜“æ—¥")
    logger.info("")
    logger.info("ğŸ“Š èšåˆæ•¸æ“šé è¦½ï¼ˆå‰ 5 å¤©ï¼‰ï¼š")
    logger.info(f"\n{df.head(5).to_string()}")
    logger.info("")
    logger.info("ğŸ“Š èšåˆæ•¸æ“šé è¦½ï¼ˆæœ€å¾Œ 5 å¤©ï¼‰ï¼š")
    logger.info(f"\n{df.tail(5).to_string()}")

    # æ•¸æ“šå“è³ªæª¢æŸ¥
    logger.info("")
    logger.info("ğŸ” æ•¸æ“šå“è³ªæª¢æŸ¥...")

    null_counts = df.isnull().sum()
    if null_counts.any():
        logger.warning("âš ï¸  ç™¼ç¾ç¼ºå¤±å€¼ï¼š")
        for field, count in null_counts.items():
            if count > 0:
                logger.warning(f"   {field}: {count} ç­†ç¼ºå¤±")
    else:
        logger.info("âœ… ç„¡ç¼ºå¤±å€¼")

    # æª¢æŸ¥ç•°å¸¸å€¼ï¼ˆåƒ¹æ ¼ç‚º 0 æˆ–è² æ•¸ï¼‰
    invalid_price = (df[['open', 'high', 'low', 'close']] <= 0).any(axis=1).sum()
    if invalid_price > 0:
        logger.warning(f"âš ï¸  ç™¼ç¾ {invalid_price} å¤©æœ‰ç•°å¸¸åƒ¹æ ¼ï¼ˆ<= 0ï¼‰")
    else:
        logger.info("âœ… åƒ¹æ ¼æ•¸æ“šæ­£å¸¸ï¼ˆ> 0ï¼‰")

    # æª¢æŸ¥ high >= low
    invalid_range = (df['high'] < df['low']).sum()
    if invalid_range > 0:
        logger.warning(f"âš ï¸  ç™¼ç¾ {invalid_range} å¤© high < lowï¼ˆç•°å¸¸ï¼‰")
    else:
        logger.info("âœ… é«˜ä½åƒ¹é—œä¿‚æ­£å¸¸ï¼ˆhigh >= lowï¼‰")

    if dry_run:
        logger.info("")
        logger.info("=" * 80)
        logger.info("ğŸ” é è¦½æ¨¡å¼å®Œæˆï¼ˆæœªå¯«å…¥æª”æ¡ˆï¼‰")
        logger.info("=" * 80)
        logger.info("ğŸ’¡ æç¤ºï¼šç§»é™¤ --dry-run åƒæ•¸ä»¥å¯«å…¥ Qlib æ ¼å¼")
        return df

    # å¯«å…¥ Qlib æ ¼å¼
    logger.info("")
    logger.info("=" * 80)
    logger.info("ğŸ“ å¯«å…¥ Qlib æ ¼å¼...")
    logger.info("=" * 80)

    output_dir = "/data/qlib/tw_stock_v2"
    instrument = "tx"

    logger.info(f"ğŸ“‚ è¼¸å‡ºç›®éŒ„ï¼š{output_dir}")
    logger.info(f"ğŸ“Š æ¨™çš„ä»£ç¢¼ï¼š{instrument}")

    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    features_dir = Path(output_dir) / "features" / instrument
    features_dir.mkdir(parents=True, exist_ok=True)

    # åˆå§‹åŒ– Qlibï¼ˆå¿…é ˆï¼‰
    try:
        import qlib
        qlib.init(provider_uri=output_dir, region='tw')
        logger.info("   âœ… Qlib å·²åˆå§‹åŒ–")
    except Exception as e:
        logger.error(f"   âŒ Qlib åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
        raise

    # è®€å–äº¤æ˜“æ—¥æ›†
    logger.info("   ğŸ“… è®€å–äº¤æ˜“æ—¥æ›†...")
    calendar_file = Path(output_dir) / "calendars" / "day.txt"
    if not calendar_file.exists():
        logger.error(f"   âŒ äº¤æ˜“æ—¥æ›†ä¸å­˜åœ¨ï¼š{calendar_file}")
        raise FileNotFoundError(f"Calendar file not found: {calendar_file}")

    with open(calendar_file, 'r') as f:
        trading_dates = [pd.to_datetime(line.strip()).date() for line in f if line.strip()]

    logger.info(f"   âœ… äº¤æ˜“æ—¥æ›†ï¼š{len(trading_dates)} å¤©ï¼ˆ{trading_dates[0]} ~ {trading_dates[-1]}ï¼‰")

    # å°‡æ•¸æ“šå°é½Šåˆ°å®Œæ•´äº¤æ˜“æ—¥æ›†ï¼ˆé—œéµæ­¥é©Ÿï¼ï¼‰
    logger.info("   ğŸ”„ å°é½Šæ•¸æ“šåˆ°äº¤æ˜“æ—¥æ›†...")
    df_aligned = df.reindex(trading_dates)
    logger.info(f"   âœ… å°é½Šå®Œæˆï¼š{len(df_aligned)} å¤©ï¼ˆå«ç©ºå€¼ï¼‰")

    # å¯«å…¥ Qlib äºŒé€²åˆ¶æ ¼å¼ï¼ˆåŒ…å«é–‹å§‹ç´¢å¼•ï¼‰
    try:
        fields = ['open', 'high', 'low', 'close', 'volume']

        for field in fields:
            logger.info(f"   ğŸ“ å¯«å…¥ {field}.day.bin...")

            # æ§‹å»ºæª”æ¡ˆè·¯å¾‘
            file_path = features_dir / f"{field}.day.bin"

            # å¯«å…¥æ•¸æ“šï¼ˆè½‰ç‚º float32ï¼Œå…è¨± NaNï¼‰
            data = df_aligned[field].values.astype(np.float32)

            # å¯«å…¥ Qlib äºŒé€²åˆ¶æ ¼å¼ï¼ˆåŒ…å«é–‹å§‹ç´¢å¼•ï¼‰
            # æ ¼å¼ï¼š[start_index (float32)] + [data array (float32)]
            with open(file_path, 'wb') as f:
                # å¯«å…¥é–‹å§‹ç´¢å¼•ï¼ˆ0 è¡¨ç¤ºå¾ç¬¬ä¸€å¤©é–‹å§‹ï¼‰
                np.array([0], dtype='<f').tofile(f)
                # å¯«å…¥æ•¸æ“š
                data.astype('<f').tofile(f)

            logger.info(f"      âœ… å®Œæˆï¼ˆ{len(data)} ç­†ï¼Œ{file_path.stat().st_size / 1024:.1f} KBï¼‰")

        # å¯«å…¥ factor.day.binï¼ˆèª¿æ•´å› å­ï¼Œé€šå¸¸ç‚º 1.0ï¼‰
        logger.info(f"   ğŸ“ å¯«å…¥ factor.day.bin...")
        file_path = features_dir / "factor.day.bin"
        factor_data = np.ones(len(df_aligned), dtype=np.float32)
        with open(file_path, 'wb') as f:
            # å¯«å…¥é–‹å§‹ç´¢å¼•
            np.array([0], dtype='<f').tofile(f)
            # å¯«å…¥æ•¸æ“š
            factor_data.astype('<f').tofile(f)
        logger.info(f"      âœ… å®Œæˆï¼ˆ{len(factor_data)} ç­†ï¼Œ{file_path.stat().st_size / 1024:.1f} KBï¼‰")

    except Exception as e:
        logger.error(f"âŒ å¯«å…¥ Qlib æ ¼å¼å¤±æ•—ï¼š{e}")
        logger.error("ğŸ’¡ æç¤ºï¼šç¢ºèªå·²å®‰è£ qlib å¥—ä»¶ä¸¦æœ‰å¯«å…¥æ¬Šé™")
        raise

    logger.info("")
    logger.info("=" * 80)
    logger.info("âœ… TX æœŸè²¨æ—¥ç·šæ•¸æ“šç”Ÿæˆå®Œæˆ")
    logger.info("=" * 80)
    logger.info(f"ğŸ“Š åˆç´„ï¼š{contract}")
    logger.info(f"ğŸ“… æ—¥æœŸç¯„åœï¼š{earliest} ~ {latest}")
    logger.info(f"ğŸ“ˆ äº¤æ˜“æ—¥æ•¸ï¼š{len(df)} å¤©")
    logger.info(f"ğŸ“‚ è¼¸å‡ºä½ç½®ï¼š{features_dir}")
    logger.info("")
    logger.info("ğŸ“‹ ç”Ÿæˆçš„æª”æ¡ˆï¼š")
    for field in fields + ['factor']:
        file_path = features_dir / f"{field}.day.bin"
        if file_path.exists():
            size_kb = file_path.stat().st_size / 1024
            logger.info(f"   âœ… {field}.day.bin ({size_kb:.1f} KB)")
        else:
            logger.warning(f"   âŒ {field}.day.binï¼ˆæœªæ‰¾åˆ°ï¼‰")

    logger.info("")
    logger.info("ğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
    logger.info("   1. é©—è­‰æ•¸æ“šï¼šls -lh /data/qlib/tw_stock_v2/features/tx/")
    logger.info("   2. æ¸¬è©¦ Qlib è®€å–")
    logger.info("   3. åŸ·è¡Œ RD-Agent å› å­æŒ–æ˜")

    return df


def main():
    """ä¸»ç¨‹å¼"""
    parser = argparse.ArgumentParser(
        description="å¾ TX æœŸè²¨åˆ†é˜ç·šèšåˆç”Ÿæˆæ—¥ç·šæ•¸æ“š",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹ï¼š
  # ä½¿ç”¨é è¨­åˆç´„ï¼ˆTX202512ï¼‰
  python scripts/generate_tx_daily_from_minute.py

  # ä½¿ç”¨é€£çºŒåˆç´„
  python scripts/generate_tx_daily_from_minute.py --contract TXCONT

  # åƒ…é è¦½
  python scripts/generate_tx_daily_from_minute.py --dry-run
        """
    )

    parser.add_argument(
        '--contract',
        type=str,
        default='TX202512',
        help='æœŸè²¨åˆç´„ä»£ç¢¼ï¼ˆé è¨­ï¼šTX202512ï¼‰'
    )

    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='åƒ…é è¦½ï¼Œä¸å¯«å…¥æª”æ¡ˆ'
    )

    args = parser.parse_args()

    try:
        df = aggregate_tx_daily(
            contract=args.contract,
            dry_run=args.dry_run
        )

        if df is not None and not args.dry_run:
            logger.info("ğŸ‰ æˆåŠŸï¼TX æœŸè²¨æ—¥ç·šæ•¸æ“šå·²å°±ç·’ï¼Œå¯ä¾› RD-Agent ä½¿ç”¨ã€‚")
            sys.exit(0)
        elif df is not None and args.dry_run:
            logger.info("ğŸ” é è¦½å®Œæˆ")
            sys.exit(0)
        else:
            logger.error("âŒ å¤±æ•—")
            sys.exit(1)

    except Exception as e:
        logger.error(f"âŒ åŸ·è¡Œå¤±æ•—ï¼š{e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    main()
