#!/usr/bin/env python3
"""
æ‰¹æ¬¡åŒæ­¥æ‰€æœ‰è‚¡ç¥¨çš„è²¡å‹™æŒ‡æ¨™æ•¸æ“š

ç‰¹æ€§ï¼š
- åˆ†æ‰¹è™•ç†ï¼ˆæ¯æ‰¹ 100 æª”è‚¡ç¥¨ï¼‰
- é€²åº¦è¿½è¹¤èˆ‡æ–·é»çºŒå‚³
- å¤±æ•—é‡è©¦æ©Ÿåˆ¶
- API é…é¡ç›£æ§
- é ä¼°å‰©é¤˜æ™‚é–“
"""

import sys
import json
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Set

# Add backend to path
sys.path.insert(0, '/app')

from app.db.session import SessionLocal
from app.services.fundamental_service import FundamentalService
from app.services.finlab_client import FinLabClient
from loguru import logger


class BatchSyncProgress:
    """é€²åº¦è¿½è¹¤"""

    def __init__(self, progress_file: str = "/tmp/batch_sync_progress.json"):
        self.progress_file = Path(progress_file)
        self.data = self.load()

    def load(self) -> Dict:
        """è¼‰å…¥é€²åº¦"""
        if self.progress_file.exists():
            with open(self.progress_file, 'r') as f:
                return json.load(f)
        return {
            "completed_stocks": [],
            "failed_stocks": [],
            "current_batch": 0,
            "total_synced": 0,
            "start_time": None,
            "last_update": None
        }

    def save(self):
        """å„²å­˜é€²åº¦"""
        self.data["last_update"] = datetime.now().isoformat()
        with open(self.progress_file, 'w') as f:
            json.dump(self.data, f, indent=2, ensure_ascii=False)

    def mark_completed(self, stock_id: str, synced_count: int):
        """æ¨™è¨˜è‚¡ç¥¨å·²å®Œæˆ"""
        if stock_id not in self.data["completed_stocks"]:
            self.data["completed_stocks"].append(stock_id)
        self.data["total_synced"] += synced_count
        self.save()

    def mark_failed(self, stock_id: str, error: str):
        """æ¨™è¨˜è‚¡ç¥¨å¤±æ•—"""
        self.data["failed_stocks"].append({
            "stock_id": stock_id,
            "error": error,
            "timestamp": datetime.now().isoformat()
        })
        self.save()

    def get_completed_stocks(self) -> Set[str]:
        """å–å¾—å·²å®Œæˆçš„è‚¡ç¥¨"""
        return set(self.data["completed_stocks"])

    def reset(self):
        """é‡ç½®é€²åº¦"""
        self.data = {
            "completed_stocks": [],
            "failed_stocks": [],
            "current_batch": 0,
            "total_synced": 0,
            "start_time": datetime.now().isoformat(),
            "last_update": None
        }
        self.save()


def get_all_stock_ids() -> List[str]:
    """å–å¾—æ‰€æœ‰è‚¡ç¥¨ä»£ç¢¼ï¼ˆéæ¿¾æ‰ ETF å’Œç‰¹åˆ¥è‚¡ï¼‰"""
    logger.info("æ­£åœ¨å–å¾—è‚¡ç¥¨æ¸…å–®...")

    try:
        client = FinLabClient()
        stocks_df = client.get_stock_list()

        # Get all stock IDs
        all_stock_ids = stocks_df.index.tolist()

        # Filter out ETFs and special stocks
        # - ETFs: start with '00' (e.g., 0050, 00721B)
        # - Special stocks: end with 'B', 'C', 'P' (e.g., 00679B)
        normal_stocks = [
            stock_id for stock_id in all_stock_ids
            if not (
                stock_id.startswith('00') or  # ETFs
                stock_id.endswith('B') or      # Special stocks (B shares)
                stock_id.endswith('C') or      # Special stocks (C shares)
                stock_id.endswith('P')         # Special stocks (preferred)
            )
        ]

        excluded_count = len(all_stock_ids) - len(normal_stocks)
        logger.info(f"âœ… å–å¾— {len(all_stock_ids)} æª”æ¨™çš„")
        logger.info(f"ğŸ“ˆ ä¸€èˆ¬è‚¡ç¥¨: {len(normal_stocks)} æª”")
        logger.info(f"âš ï¸  å·²éæ¿¾: {excluded_count} æª” (ETF + ç‰¹åˆ¥è‚¡)")

        return normal_stocks
    except Exception as e:
        logger.error(f"âŒ å–å¾—è‚¡ç¥¨æ¸…å–®å¤±æ•—: {e}")
        raise


def sync_single_stock(
    db,
    stock_id: str,
    indicators: List[str],
    progress: BatchSyncProgress
) -> int:
    """åŒæ­¥å–®ä¸€è‚¡ç¥¨çš„æ‰€æœ‰æŒ‡æ¨™"""
    service = FundamentalService(db)
    synced_count = 0

    for indicator in indicators:
        try:
            count = service.sync_indicator_data(
                stock_id=stock_id,
                indicator=indicator,
                start_date=None,  # å…¨éƒ¨æ­·å²æ•¸æ“š
                end_date=None
            )
            synced_count += count

        except Exception as e:
            logger.warning(f"âš ï¸  {stock_id} - {indicator} å¤±æ•—: {str(e)[:50]}")
            continue

    return synced_count


def batch_sync_all_stocks(
    batch_size: int = 100,
    batch_delay: int = 60,
    resume: bool = True,
    max_stocks: int = None
):
    """
    æ‰¹æ¬¡åŒæ­¥æ‰€æœ‰è‚¡ç¥¨

    Args:
        batch_size: æ¯æ‰¹è™•ç†çš„è‚¡ç¥¨æ•¸é‡
        batch_delay: æ‰¹æ¬¡ä¹‹é–“çš„å»¶é²ï¼ˆç§’ï¼‰
        resume: æ˜¯å¦å¾ä¸Šæ¬¡ä¸­æ–·è™•ç¹¼çºŒ
        max_stocks: æœ€å¤§è™•ç†è‚¡ç¥¨æ•¸ï¼ˆNone = å…¨éƒ¨ï¼‰
    """

    logger.info("=" * 80)
    logger.info("ğŸš€ é–‹å§‹æ‰¹æ¬¡åŒæ­¥è²¡å‹™æŒ‡æ¨™æ•¸æ“š")
    logger.info("=" * 80)

    # åˆå§‹åŒ–
    progress = BatchSyncProgress()

    if not resume:
        logger.warning("âš ï¸  é‡ç½®é€²åº¦ï¼Œå¾é ­é–‹å§‹")
        progress.reset()
    else:
        logger.info(f"ğŸ“‹ è¼‰å…¥é€²åº¦: å·²å®Œæˆ {len(progress.get_completed_stocks())} æª”è‚¡ç¥¨")

    # å–å¾—æ‰€æœ‰è‚¡ç¥¨
    all_stock_ids = get_all_stock_ids()

    # é™åˆ¶æ•¸é‡ï¼ˆæ¸¬è©¦ç”¨ï¼‰
    if max_stocks:
        all_stock_ids = all_stock_ids[:max_stocks]
        logger.info(f"âš ï¸  æ¸¬è©¦æ¨¡å¼ï¼šåƒ…è™•ç†å‰ {max_stocks} æª”è‚¡ç¥¨")

    # éæ¿¾å·²å®Œæˆçš„è‚¡ç¥¨
    completed = progress.get_completed_stocks()
    remaining_stocks = [s for s in all_stock_ids if s not in completed]

    logger.info(f"ğŸ“Š ç¸½è¨ˆ: {len(all_stock_ids)} æª”")
    logger.info(f"âœ… å·²å®Œæˆ: {len(completed)} æª”")
    logger.info(f"â³ å¾…è™•ç†: {len(remaining_stocks)} æª”")

    if len(remaining_stocks) == 0:
        logger.info("ğŸ‰ æ‰€æœ‰è‚¡ç¥¨å·²åŒæ­¥å®Œæˆï¼")
        return

    # å–å¾—æŒ‡æ¨™æ¸…å–®
    indicators = FinLabClient.get_common_fundamental_indicators()
    logger.info(f"ğŸ“ˆ æŒ‡æ¨™æ•¸é‡: {len(indicators)} å€‹")

    # é ä¼°æ™‚é–“
    avg_time_per_stock = 2.5  # ç§’
    total_time = len(remaining_stocks) * avg_time_per_stock
    hours = int(total_time // 3600)
    minutes = int((total_time % 3600) // 60)
    logger.info(f"â±ï¸  é ä¼°æ™‚é–“: {hours} å°æ™‚ {minutes} åˆ†é˜")
    logger.info(f"âš™ï¸  æ‰¹æ¬¡å¤§å°: {batch_size} æª”/æ‰¹")
    logger.info(f"â¸ï¸  æ‰¹æ¬¡å»¶é²: {batch_delay} ç§’")

    # è©¢å•ç¢ºèª
    print("\n" + "=" * 80)
    print("âš ï¸  è­¦å‘Šï¼šæ­¤æ“ä½œå°‡åŸ·è¡Œæ•¸å°æ™‚ï¼Œè«‹ç¢ºä¿ï¼š")
    print("   1. Docker å®¹å™¨ä¿æŒé‹è¡Œ")
    print("   2. ç¶²è·¯é€£ç·šç©©å®š")
    print("   3. FinLab API é…é¡å……è¶³ï¼ˆå»ºè­° > 1000 MBï¼‰")
    print("=" * 80)
    confirm = input("\næ˜¯å¦ç¹¼çºŒï¼Ÿ(yes/no): ")

    if confirm.lower() != 'yes':
        logger.info("âŒ ä½¿ç”¨è€…å–æ¶ˆ")
        return

    # é–‹å§‹è™•ç†
    from app.db.session import ensure_models_imported
    ensure_models_imported()  # Ensure all models are loaded

    db = SessionLocal()
    start_time = time.time()

    try:
        # åˆ†æ‰¹è™•ç†
        for batch_idx, i in enumerate(range(0, len(remaining_stocks), batch_size)):
            batch = remaining_stocks[i:i + batch_size]
            batch_num = batch_idx + 1
            total_batches = (len(remaining_stocks) + batch_size - 1) // batch_size

            logger.info("")
            logger.info("=" * 80)
            logger.info(f"ğŸ“¦ æ‰¹æ¬¡ {batch_num}/{total_batches} - è™•ç† {len(batch)} æª”è‚¡ç¥¨")
            logger.info("=" * 80)

            batch_start = time.time()

            for stock_idx, stock_id in enumerate(batch):
                stock_num = i + stock_idx + 1
                total_stocks = len(remaining_stocks)

                try:
                    logger.info(f"[{stock_num}/{total_stocks}] ğŸ”„ è™•ç† {stock_id}...")

                    synced = sync_single_stock(db, stock_id, indicators, progress)
                    progress.mark_completed(stock_id, synced)

                    # è¨ˆç®—é€²åº¦
                    elapsed = time.time() - start_time
                    avg_time = elapsed / (stock_num)
                    remaining = (total_stocks - stock_num) * avg_time

                    remaining_hours = int(remaining // 3600)
                    remaining_mins = int((remaining % 3600) // 60)

                    logger.info(
                        f"âœ… {stock_id} å®Œæˆ: {synced} ç­† | "
                        f"é€²åº¦: {stock_num}/{total_stocks} ({stock_num*100//total_stocks}%) | "
                        f"é ä¼°å‰©é¤˜: {remaining_hours}h {remaining_mins}m"
                    )

                    # å°å»¶é²é¿å… API éè¼‰
                    time.sleep(0.5)

                except Exception as e:
                    error_msg = str(e)[:200]
                    logger.error(f"âŒ {stock_id} å¤±æ•—: {error_msg}")
                    progress.mark_failed(stock_id, error_msg)
                    continue

            batch_elapsed = time.time() - batch_start
            logger.info(f"âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆï¼Œè€—æ™‚ {batch_elapsed:.1f} ç§’")

            # æ‰¹æ¬¡é–“å»¶é²ï¼ˆæœ€å¾Œä¸€æ‰¹é™¤å¤–ï¼‰
            if batch_num < total_batches:
                logger.info(f"â¸ï¸  ç­‰å¾… {batch_delay} ç§’å¾Œç¹¼çºŒä¸‹ä¸€æ‰¹...")
                time.sleep(batch_delay)

        # å®Œæˆçµ±è¨ˆ
        total_elapsed = time.time() - start_time
        hours = int(total_elapsed // 3600)
        minutes = int((total_elapsed % 3600) // 60)

        logger.info("")
        logger.info("=" * 80)
        logger.info("ğŸ‰ æ‰¹æ¬¡åŒæ­¥å®Œæˆï¼")
        logger.info("=" * 80)
        logger.info(f"âœ… æˆåŠŸ: {len(progress.get_completed_stocks())} æª”")
        logger.info(f"âŒ å¤±æ•—: {len(progress.data['failed_stocks'])} æª”")
        logger.info(f"ğŸ“Š ç¸½æ•¸æ“š: {progress.data['total_synced']} ç­†")
        logger.info(f"â±ï¸  ç¸½è€—æ™‚: {hours} å°æ™‚ {minutes} åˆ†é˜")

        # é¡¯ç¤ºå¤±æ•—æ¸…å–®
        if progress.data['failed_stocks']:
            logger.warning(f"\nå¤±æ•—çš„è‚¡ç¥¨ï¼ˆå‰ 10 ç­†ï¼‰ï¼š")
            for item in progress.data['failed_stocks'][:10]:
                logger.warning(f"  - {item['stock_id']}: {item['error'][:100]}")

    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ä½¿ç”¨è€…ä¸­æ–·ï¼é€²åº¦å·²å„²å­˜")
        logger.info(f"ğŸ“‹ å·²å®Œæˆ: {len(progress.get_completed_stocks())} æª”")
        logger.info(f"ğŸ’¾ é€²åº¦æª”: {progress.progress_file}")
        logger.info("ğŸ’¡ ä¸‹æ¬¡åŸ·è¡Œæ™‚å°‡è‡ªå‹•çºŒå‚³")

    except Exception as e:
        logger.error(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        raise

    finally:
        db.close()


def show_progress():
    """é¡¯ç¤ºç•¶å‰é€²åº¦"""
    progress = BatchSyncProgress()

    print("\n" + "=" * 80)
    print("ğŸ“Š æ‰¹æ¬¡åŒæ­¥é€²åº¦")
    print("=" * 80)
    print(f"âœ… å·²å®Œæˆè‚¡ç¥¨: {len(progress.get_completed_stocks())} æª”")
    print(f"âŒ å¤±æ•—è‚¡ç¥¨: {len(progress.data['failed_stocks'])} æª”")
    print(f"ğŸ“Š ç´¯è¨ˆæ•¸æ“š: {progress.data['total_synced']} ç­†")
    print(f"ğŸ• é–‹å§‹æ™‚é–“: {progress.data.get('start_time', 'N/A')}")
    print(f"ğŸ• æœ€å¾Œæ›´æ–°: {progress.data.get('last_update', 'N/A')}")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='æ‰¹æ¬¡åŒæ­¥è²¡å‹™æŒ‡æ¨™æ•¸æ“š')
    parser.add_argument('--batch-size', type=int, default=100, help='æ¯æ‰¹è™•ç†è‚¡ç¥¨æ•¸')
    parser.add_argument('--batch-delay', type=int, default=60, help='æ‰¹æ¬¡é–“å»¶é²ï¼ˆç§’ï¼‰')
    parser.add_argument('--reset', action='store_true', help='é‡ç½®é€²åº¦å¾é ­é–‹å§‹')
    parser.add_argument('--max-stocks', type=int, help='æœ€å¤§è™•ç†è‚¡ç¥¨æ•¸ï¼ˆæ¸¬è©¦ç”¨ï¼‰')
    parser.add_argument('--status', action='store_true', help='é¡¯ç¤ºé€²åº¦')

    args = parser.parse_args()

    if args.status:
        show_progress()
    else:
        batch_sync_all_stocks(
            batch_size=args.batch_size,
            batch_delay=args.batch_delay,
            resume=not args.reset,
            max_stocks=args.max_stocks
        )
